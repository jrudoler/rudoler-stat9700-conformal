@inproceedings{acerbiPracticalBayesianOptimization2017a,
  title = {Practical {{Bayesian Optimization}} for {{Model Fitting}} with {{Bayesian Adaptive Direct Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Acerbi, Luigi and Ma, Wei Ji},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html},
  urldate = {2023-03-21},
  abstract = {Computational models in fields such as computational neuroscience  are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning.  Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.},
  keywords = {bayes},
  file = {/Users/jrudoler/Zotero/storage/JZEII6JV/Acerbi and Ma - 2017 - Practical Bayesian Optimization for Model Fitting .pdf}
}

@online{angelopoulosGentleIntroductionConformal2022,
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  date = {2022-12-07},
  eprint = {2107.07511},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2107.07511},
  urldate = {2023-11-02},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.},
  langid = {english},
  pubstate = {preprint},
  keywords = {conformal,priority},
  file = {/Users/jrudoler/Zotero/storage/DM3DRW5X/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf}
}

@inproceedings{angelopoulosImagetoImageRegressionDistributionFree2022,
  title = {Image-to-{{Image Regression}} with {{Distribution-Free Uncertainty Quantification}} and {{Applications}} in {{Imaging}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Angelopoulos, Anastasios N. and Kohli, Amit Pal and Bates, Stephen and Jordan, Michael and Malik, Jitendra and Alshaabi, Thayer and Upadhyayula, Srigokul and Romano, Yaniv},
  date = {2022-06-28},
  pages = {717--730},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/angelopoulos22a.html},
  urldate = {2023-11-21},
  abstract = {Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model’s mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees\{—\}regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/N8X5XGWS/Angelopoulos et al. - 2022 - Image-to-Image Regression with Distribution-Free U.pdf}
}

@online{angelopoulosLearnThenTest2022,
  title = {Learn Then {{Test}}: {{Calibrating Predictive Algorithms}} to {{Achieve Risk Control}}},
  shorttitle = {Learn Then {{Test}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen and Candès, Emmanuel J. and Jordan, Michael I. and Lei, Lihua},
  date = {2022-09-29},
  eprint = {2110.01052},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2110.01052},
  url = {http://arxiv.org/abs/2110.01052},
  urldate = {2023-11-26},
  abstract = {We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data.},
  pubstate = {preprint},
  keywords = {priority,unread},
  file = {/Users/jrudoler/Zotero/storage/QNVEHUT9/Angelopoulos et al. - 2022 - Learn then Test Calibrating Predictive Algorithms.pdf;/Users/jrudoler/Zotero/storage/NGMTB48E/2110.html}
}

@article{angelopoulosPredictionpoweredInference2023,
  title = {Prediction-Powered Inference},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fannjiang, Clara and Jordan, Michael I. and Zrnic, Tijana},
  date = {2023-11-10},
  journaltitle = {Science},
  volume = {382},
  number = {6671},
  pages = {669--674},
  doi = {10.1126/science.adi6000},
  url = {https://www.science.org/doi/full/10.1126/science.adi6000},
  urldate = {2023-11-29},
  abstract = {Prediction-powered inference is a framework for performing valid statistical inference when an experimental dataset is supplemented with predictions from a machine-learning system. The framework yields simple algorithms for computing provably valid confidence intervals for quantities such as means, quantiles, and linear and logistic regression coefficients without making any assumptions about the machine-learning algorithm that supplies the predictions. Furthermore, more accurate predictions translate to smaller confidence intervals. Prediction-powered inference could enable researchers to draw valid and more data-efficient conclusions using machine learning. The benefits of prediction-powered inference were demonstrated with datasets from proteomics, astronomy, genomics, remote sensing, census analysis, and ecology.},
  keywords = {unread}
}

@online{angelopoulosUncertaintySetsImage2022,
  title = {Uncertainty {{Sets}} for {{Image Classifiers}} Using {{Conformal Prediction}}},
  author = {Angelopoulos, Anastasios and Bates, Stephen and Malik, Jitendra and Jordan, Michael I.},
  date = {2022-09-03},
  eprint = {2009.14193},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2009.14193},
  urldate = {2023-11-21},
  abstract = {Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network's probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90\%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/N7BXXJPU/Angelopoulos et al. - 2022 - Uncertainty Sets for Image Classifiers using Confo.pdf;/Users/jrudoler/Zotero/storage/SP7KVWLR/2009.html}
}

@article{aroraComparisonLogisticRegression2018,
  title = {Comparison of Logistic Regression, Support Vector Machines, and Deep Learning Classifiers for Predicting Memory Encoding Success Using Human Intracranial {{EEG}} Recordings},
  author = {Arora, Akshay and Lin, Jui-Jui and Gasperian, Alec and Maldjian, Joseph and Stein, Joel and Kahana, Michael and Lega, Bradley},
  date = {2018-10},
  journaltitle = {Journal of Neural Engineering},
  shortjournal = {J. Neural Eng.},
  volume = {15},
  number = {6},
  pages = {066028},
  publisher = {{IOP Publishing}},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/aae131},
  url = {https://doi.org/10.1088/1741-2552/aae131},
  urldate = {2022-09-05},
  abstract = {Objective. We sought to test the performance of three strategies for binary classification (logistic regression, support vector machines, and deep learning) for the problem of predicting successful episodic memory encoding using direct brain recordings obtained from human stereo EEG subjects. We also sought to test the impact of applying t-distributed stochastic neighbor embedding (tSNE) for unsupervised dimensionality reduction, as well as testing the effect of reducing input features to a core set of memory relevant brain areas. This work builds upon published efforts to develop a closed-loop stimulation device to improve memory performance. Approach. We used a unique data set consisting of 30 stereo EEG patients with electrodes implanted into a core set of five common brain regions (along with other areas) who performed the free recall episodic memory task as brain activity was recorded. Using three different machine learning strategies, we trained classifiers to predict successful versus unsuccessful memory encoding and compared the difference in classifier performance (as measured by the AUC) at the subject level and in aggregate across modalities. We report the impact of feature reduction on the classifiers, including reducing the number of input brain regions, frequency bands, and the impact of tSNE. Results. Deep learning classifiers outperformed both support vector machines (SVM) and logistic regression (LR). A priori selection of core brain regions also improved classifier performance for LR and SVM models, especially when combined with tSNE. Significance. We report for the first time a direct comparison among traditional and deep learning methods of binary classification to the problem of predicting successful memory encoding using human brain electrophysiological data. Our findings will inform the design of brain machine interface devices to affect memory processing.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/2ZB73Q6E/Arora et al. - 2018 - Comparison of logistic regression, support vector .pdf}
}

@online{aroraTheoryEmergenceComplex2023,
  title = {A {{Theory}} for {{Emergence}} of {{Complex Skills}} in {{Language Models}}},
  author = {Arora, Sanjeev and Goyal, Anirudh},
  date = {2023-07-29},
  eprint = {2307.15936},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2307.15936},
  url = {http://arxiv.org/abs/2307.15936},
  urldate = {2023-09-11},
  abstract = {A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this \{\textbackslash em slingshot generalization\} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing tasks involving \$k\$-tuples of skills emerges essentially at the same scaling and same rate as competence on the elementary skills themselves.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/FXKLWPLA/Arora and Goyal - 2023 - A Theory for Emergence of Complex Skills in Langua.pdf;/Users/jrudoler/Zotero/storage/8HSY8QVC/2307.html}
}

@online{azabouUnifiedScalableFramework2023,
  title = {A {{Unified}}, {{Scalable Framework}} for {{Neural Population Decoding}}},
  author = {Azabou, Mehdi and Arora, Vinam and Ganesh, Venkataramana and Mao, Ximeng and Nachimuthu, Santosh and Mendelson, Michael J. and Richards, Blake and Perich, Matthew G. and Lajoie, Guillaume and Dyer, Eva L.},
  date = {2023-10-24},
  url = {https://arxiv.org/abs/2310.16046v1},
  urldate = {2023-11-02},
  abstract = {Our ability to use deep learning approaches to decipher neural activity would likely benefit from greater scale, in terms of both model size and datasets. However, the integration of many neural recordings into one unified model is challenging, as each recording contains the activity of different neurons from different individual animals. In this paper, we introduce a training framework and architecture designed to model the population dynamics of neural activity across diverse, large-scale neural recordings. Our method first tokenizes individual spikes within the dataset to build an efficient representation of neural events that captures the fine temporal structure of neural activity. We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities. Utilizing this architecture and training framework, we construct a large-scale multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different sessions of recording from over 27,373 neural units and over 100 hours of recordings. In a number of different tasks, we demonstrate that our pretrained model can be rapidly adapted to new, unseen sessions with unspecified neuron correspondence, enabling few-shot performance with minimal labels. This work presents a powerful new approach for building deep learning tools to analyze neural data and stakes out a clear path to training at scale.},
  langid = {english},
  organization = {{arXiv.org}},
  keywords = {priority},
  file = {/Users/jrudoler/Zotero/storage/D2NXXHHM/Azabou et al. - 2023 - A Unified, Scalable Framework for Neural Populatio.pdf}
}

@online{bafumiFittingMultilevelModels2007,
  type = {SSRN Scholarly Paper},
  title = {Fitting {{Multilevel Models When Predictors}} and {{Group Effects Correlate}}},
  author = {Bafumi, Joseph and Gelman, Andrew},
  date = {2007-09-03},
  number = {1010095},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.1010095},
  url = {https://papers.ssrn.com/abstract=1010095},
  urldate = {2023-03-01},
  abstract = {Random effects models (that is, regressions with varying intercepts that are modeled with error) are avoided by some social scientists because of potential issues with bias and uncertainty estimates. Particularly, when one or more predictors correlate with the group or unit effects, a key Gauss-Markov assumption is violated and estimates are compromised. However, this problem can easily be solved by including the average of each individual-level predictors in the group-level regression. We explain the solution, demonstrate its effectiveness using simulations, show how it can be applied in some commonly-used statistical software, and discuss its potential for substantive modeling.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/EASXW6AD/Bafumi and Gelman - 2007 - Fitting Multilevel Models When Predictors and Grou.pdf}
}

@online{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  date = {2022-12-15},
  eprint = {2212.08073},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.08073},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2023-09-06},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/CYSWCPAI/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf;/Users/jrudoler/Zotero/storage/UN5JSJ4P/2212.html}
}

@online{baiEmpiricalEvaluationGeneric2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  date = {2018-04-19},
  eprint = {1803.01271},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.01271},
  url = {http://arxiv.org/abs/1803.01271},
  urldate = {2023-05-01},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/6AU8AJLC/Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf;/Users/jrudoler/Zotero/storage/RQAH8W5J/1803.html}
}

@online{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  eprint = {2204.05862},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.05862},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2023-09-20},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/Y847WCHD/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/jrudoler/Zotero/storage/NJEECGBT/2204.html}
}

@inproceedings{balestrieroDataAugmentationWorthThousand2022,
  title = {A {{Data-Augmentation Is Worth A Thousand Samples}}: {{Analytical Moments And Sampling-Free Training}}},
  shorttitle = {A {{Data-Augmentation Is Worth A Thousand Samples}}},
  author = {Balestriero, Randall and Misra, Ishan and LeCun, Yann},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=ekQ_xrVWwQp},
  urldate = {2022-11-20},
  abstract = {Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Up to our knowledge, we obtain the first explicit regularizer that corresponds to using DA during training for non-trivial transformations such as affine transformations, color jittering, or Gaussian blur. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, given a loss at hand, we find that common DAs require tens of thousands of samples for the loss to be correctly estimated and for the model training to converge. We then show that for a training loss to have reduced variance under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample's covariance matrix under the considered DA augmentation; this is exactly the quantity estimated and regularized by TangentProp. Those findings also hint at a possible explanation on why models tend to shift their focus from edges to textures when specific DAs are employed.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/TTII3GGC/Balestriero et al. - 2022 - A Data-Augmentation Is Worth A Thousand Samples A.pdf;/Users/jrudoler/Zotero/storage/MR5Q74LF/forum.html}
}

@article{baoTwoLevelDomainAdaptation2021,
  title = {Two-{{Level Domain Adaptation Neural Network}} for {{EEG-Based Emotion Recognition}}},
  author = {Bao, Guangcheng and Zhuang, Ning and Tong, Li and Yan, Bin and Shu, Jun and Wang, Linyuan and Zeng, Ying and Shen, Zhichong},
  date = {2021},
  journaltitle = {Frontiers in Human Neuroscience},
  volume = {14},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2020.605246},
  urldate = {2022-08-12},
  abstract = {Emotion recognition plays an important part in human-computer interaction (HCI). Currently, the main challenge in electroencephalogram (EEG)-based emotion recognition is the non-stationarity of EEG signals, which causes performance of the trained model decreasing over time. In this paper, we propose a two-level domain adaptation neural network (TDANN) to construct a transfer model for EEG-based emotion recognition. Specifically, deep features from the topological graph, which preserve topological information from EEG signals, are extracted using a deep neural network. These features are then passed through TDANN for two-level domain confusion. The first level uses the maximum mean discrepancy (MMD) to reduce the distribution discrepancy of deep features between source domain and target domain, and the second uses the domain adversarial neural network (DANN) to force the deep features closer to their corresponding class centers. We evaluated the domain-transfer performance of the model on both our self-built data set and the public data set SEED. In the cross-day transfer experiment, the ability to accurately discriminate joy from other emotions was high: sadness (84\%), anger (87.04\%), and fear (85.32\%) on the self-built data set. The accuracy reached 74.93\% on the SEED data set. In the cross-subject transfer experiment, the ability to accurately discriminate joy from other emotions was equally high: sadness (83.79\%), anger (84.13\%), and fear (81.72\%) on the self-built data set. The average accuracy reached 87.9\% on the SEED data set, which was higher than WGAN-DA. The experimental results demonstrate that the proposed TDANN can effectively handle the domain transfer problem in EEG-based emotion recognition.},
  keywords = {transfer-learning},
  file = {/Users/jrudoler/Zotero/storage/BA6ZDH2Z/Bao et al. - 2021 - Two-Level Domain Adaptation Neural Network for EEG.pdf}
}

@online{barberConformalPredictionExchangeability2023,
  title = {Conformal Prediction beyond Exchangeability},
  author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  date = {2023-03-16},
  eprint = {2202.13415},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2202.13415},
  url = {http://arxiv.org/abs/2202.13415},
  urldate = {2023-11-02},
  abstract = {Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use a nonsymmetric algorithm that treats recent observations as more relevant. This paper generalizes conformal prediction to deal with both aspects: we employ weighted quantiles to introduce robustness against distribution drift, and design a new randomization technique to allow for algorithms that do not treat data points symmetrically. Our new methods are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. We demonstrate the practical utility of these new tools with simulations and real-data experiments on electricity and election forecasting.},
  pubstate = {preprint},
  keywords = {conformal,priority},
  file = {/Users/jrudoler/Zotero/storage/PHBZVUSS/Barber et al. - 2023 - Conformal prediction beyond exchangeability.pdf;/Users/jrudoler/Zotero/storage/3VCWDIT5/2202.html}
}

@article{barberLimitsDistributionfreeConditional2021,
  title = {The Limits of Distribution-Free Conditional Predictive Inference},
  author = {Barber, Rina Foygel and Candès, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
  date = {2021-06-15},
  journaltitle = {Information and Inference: A Journal of the IMA},
  shortjournal = {Information and Inference: A Journal of the IMA},
  volume = {10},
  number = {2},
  pages = {455--482},
  issn = {2049-8772},
  doi = {10.1093/imaiai/iaaa017},
  url = {https://doi.org/10.1093/imaiai/iaaa017},
  urldate = {2023-11-20},
  abstract = {We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not sufficient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work, we aim to explore the space in between these two and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting.},
  file = {/Users/jrudoler/Zotero/storage/4TTNSCFK/Foygel Barber et al. - 2021 - The limits of distribution-free conditional predic.pdf;/Users/jrudoler/Zotero/storage/9FNEDYBT/5896927.html}
}

@online{barberPredictiveInferenceJackknife2020,
  title = {Predictive Inference with the Jackknife+},
  author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  date = {2020-05-29},
  eprint = {1905.02928},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1905.02928},
  url = {http://arxiv.org/abs/1905.02928},
  urldate = {2023-11-02},
  abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk [2015] and we discuss connections.},
  pubstate = {preprint},
  keywords = {conformal,priority},
  file = {/Users/jrudoler/Zotero/storage/MPN6Z9A4/Barber et al. - 2020 - Predictive inference with the jackknife+.pdf;/Users/jrudoler/Zotero/storage/UV7GCRDF/1905.html}
}

@online{bartlettDeepLearningStatistical2021,
  title = {Deep Learning: A Statistical Viewpoint},
  shorttitle = {Deep Learning},
  author = {Bartlett, Peter L. and Montanari, Andrea and Rakhlin, Alexander},
  date = {2021-03-16},
  eprint = {2103.09177},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2103.09177},
  url = {http://arxiv.org/abs/2103.09177},
  urldate = {2022-11-13},
  abstract = {The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting. We survey recent theoretical progress that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behavior of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favorable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.},
  pubstate = {preprint},
  keywords = {machine-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/R5BCI6DK/Bartlett et al. - 2021 - Deep learning a statistical viewpoint.pdf;/Users/jrudoler/Zotero/storage/9KLVD3KY/2103.html}
}

@article{batesDistributionfreeRiskcontrollingPrediction2021,
  title = {Distribution-Free, {{Risk-controlling Prediction Sets}}},
  author = {Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael},
  date = {2021-09-30},
  journaltitle = {Journal of the ACM},
  shortjournal = {J. ACM},
  volume = {68},
  number = {6},
  pages = {43:1--43:34},
  issn = {0004-5411},
  doi = {10.1145/3478535},
  url = {https://dl.acm.org/doi/10.1145/3478535},
  urldate = {2023-12-10},
  abstract = {While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. To convey instance-wise uncertainty for prediction tasks, we show how to generate set-valued predictions from a black-box predictor that controls the expected loss on future test points at a user-specified level. Our approach provides explicit finite-sample guarantees for any dataset by using a holdout set to calibrate the size of the prediction sets. This framework enables simple, distribution-free, rigorous error control for many tasks, and we demonstrate it in five large-scale machine learning problems: (1) classification problems where some mistakes are more costly than others; (2) multi-label classification, where each observation has multiple associated labels; (3) classification problems where the labels have a hierarchical structure; (4) image segmentation, where we wish to predict a set of pixels containing an object of interest; and (5) protein structure prediction. Last, we discuss extensions to uncertainty quantification for ranking, metric learning, and distributionally robust learning.},
  file = {/Users/jrudoler/Zotero/storage/ZY2KNMD7/Bates et al. - 2021 - Distribution-free, Risk-controlling Prediction Set.pdf}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
  date = {2015-10-07},
  journaltitle = {Journal of Statistical Software},
  volume = {67},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  url = {https://doi.org/10.18637/jss.v067.i01},
  urldate = {2022-09-28},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/EI5DKSNZ/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@unpublished{batesParsimoniousMixedModels2018,
  title = {Parsimonious {{Mixed Models}}},
  author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
  date = {2018-05-26},
  eprint = {1506.04967},
  eprinttype = {arxiv},
  eprintclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.04967},
  url = {http://arxiv.org/abs/1506.04967},
  urldate = {2022-09-14},
  abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
  keywords = {stat-methods},
  file = {/Users/jrudoler/Zotero/storage/T248RBUU/Bates et al. - 2018 - Parsimonious Mixed Models.pdf;/Users/jrudoler/Zotero/storage/Y3Y5SR7Z/1506.html}
}

@article{baxterModelInductiveBias2000,
  title = {A {{Model}} of {{Inductive Bias Learning}}},
  author = {Baxter, J.},
  date = {2000-03-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {12},
  pages = {149--198},
  issn = {1076-9757},
  doi = {10.1613/jair.731},
  url = {https://jair.org/index.php/jair/article/view/10253},
  urldate = {2023-01-17},
  abstract = {A major problem in machine learning is that of inductive    bias: how to choose a learner's hypothesis space so that it is large    enough to contain a solution to the problem being learnt, yet small    enough to ensure reliable generalization from reasonably-sized    training sets.  Typically such bias is supplied by hand through the    skill and insights of experts. In this paper a model for automatically    learning bias is investigated. The central assumption of the model is    that the learner is embedded within an environment of related learning    tasks. Within such an environment the learner can sample from multiple    tasks, and hence it can search for a hypothesis space that contains    good solutions to many of the problems in the environment. Under    certain restrictions on the set of all hypothesis spaces available to    the learner, we show that a hypothesis space that performs well on a    sufficiently large number of training tasks will also perform well    when learning novel tasks in the same environment.  Explicit bounds    are also derived demonstrating that learning multiple tasks within an    environment of related tasks can potentially give much better    generalization than learning a single task.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/5T3NTKLS/Baxter - 2000 - A Model of Inductive Bias Learning.pdf}
}

@online{bayleCrossvalidationConfidenceIntervals2020,
  title = {Cross-Validation {{Confidence Intervals}} for {{Test Error}}},
  author = {Bayle, Pierre and Bayle, Alexandre and Janson, Lucas and Mackey, Lester},
  date = {2020-10-31},
  eprint = {2007.12671},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2007.12671},
  url = {http://arxiv.org/abs/2007.12671},
  urldate = {2022-11-24},
  abstract = {This work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for \$k\$-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller \$k\$-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/HRQRYFYW/Bayle et al. - 2020 - Cross-validation Confidence Intervals for Test Err.pdf;/Users/jrudoler/Zotero/storage/PY8RN9IW/2007.html}
}

@article{belkinReconcilingModernMachine2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  date = {2019-08-06},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {116},
  number = {32},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {15849--15854},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1903070116},
  url = {http://arxiv.org/abs/1812.11118},
  urldate = {2023-05-01},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
  keywords = {machine-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/6H8QBYYE/Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf;/Users/jrudoler/Zotero/storage/JCUCCLB7/1812.html}
}

@article{bennettNeuralCorrelatesInterspecies2009,
  title = {Neural Correlates of Interspecies Perspective Taking in the Post-Mortem {{Atlantic Salmon}}: An Argument for Multiple Comparisons Correction},
  shorttitle = {Neural Correlates of Interspecies Perspective Taking in the Post-Mortem {{Atlantic Salmon}}},
  author = {Bennett, Cm and Miller, Mb and Wolford, Gl},
  date = {2009-07},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {47},
  pages = {S125},
  issn = {10538119},
  doi = {10.1016/S1053-8119(09)71202-9},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811909712029},
  urldate = {2023-04-03},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/CV9SLVDK/Bennett et al. - 2009 - Neural correlates of interspecies perspective taki.pdf}
}

@article{bieverChatGPTBrokeTuring2023,
  title = {{{ChatGPT}} Broke the {{Turing}} Test — the Race Is on for New Ways to Assess {{AI}}},
  author = {Biever, Celeste},
  date = {2023-07-25},
  journaltitle = {Nature},
  volume = {619},
  number = {7971},
  pages = {686--689},
  doi = {10.1038/d41586-023-02361-7},
  url = {https://www.nature.com/articles/d41586-023-02361-7},
  urldate = {2023-07-26},
  abstract = {Large language models mimic human chatter, but scientists disagree on their ability to reason.},
  langid = {english},
  keywords = {AI},
  file = {/Users/jrudoler/Zotero/storage/A64E8D4N/Biever - 2023 - ChatGPT broke the Turing test — the race is on for new ways to assess AI.pdf}
}

@misc{billsLanguageModelsCan2023,
  title = {Language Models Can Explain Neurons in Language Models},
  author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  date = {2023},
  url = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  keywords = {LLM,STAT9910,unread}
}

@inproceedings{blundellWeightUncertaintyNeural2015,
  title = {Weight {{Uncertainty}} in {{Neural Network}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  date = {2015-06-01},
  pages = {1613--1622},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/blundell15.html},
  urldate = {2023-11-21},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/UBU39I5S/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf}
}

@article{bourlardAutoassociationMultilayerPerceptrons1988,
  title = {Auto-Association by Multilayer Perceptrons and Singular Value Decomposition},
  author = {Bourlard, H. and Kamp, Y.},
  date = {1988-09-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  volume = {59},
  number = {4},
  pages = {291--294},
  issn = {1432-0770},
  doi = {10.1007/BF00332918},
  url = {https://doi.org/10.1007/BF00332918},
  urldate = {2022-06-22},
  abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Loève transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the rôle of the different parameters.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/GBDM3FVH/Bourlard and Kamp - 1988 - Auto-association by multilayer perceptrons and sin.pdf}
}

@article{bowersDeepProblemsNeural2022,
  title = {Deep {{Problems}} with {{Neural Network Models}} of {{Human Vision}}},
  author = {Bowers, Jeffrey S. and Malhotra, Gaurav and Dujmović, Marin and Montero, Milton Llera and Tsvetkov, Christian and Biscione, Valerio and Puebla, Guillermo and Adolfi, Federico G. and Hummel, John and Heaton, Rachel Flood and Evans, Benjamin and Mitchell, Jeff and Blything, Ryan},
  date = {2022-04-13T09:25:23},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/5zf4s},
  url = {https://psyarxiv.com/5zf4s/},
  urldate = {2022-07-25},
  abstract = {Deep neural networks (DNNs) have had extraordinary successes in classifying photographic images of objects and are often described as the best models of biological vision.  This conclusion is largely based on three sets of findings: (1) DNNs are more accurate than any other model in classifying images taken from various datasets, (2) DNNs do the best job in predicting the pattern of human errors in classifying objects taken from various behavioral benchmark datasets, and (3) DNNs do the best job in predicting brain signals in response to images taken from various brain benchmark datasets (e.g., single cell responses or fMRI data).  However, most behavioral and brain benchmarks report the outcomes of observational experiments that do not manipulate any independent variables, and we show that the good prediction on these datasets may be mediated by DNNs that share little overlap with biological vision.  More problematically, we show that DNNs account for almost no results from psychological research.  This contradicts the common claim that DNNs are good, let alone the best, models of human object recognition.  We argue that theorists interested in developing biologically plausible models of human vision need to direct their attention to explaining psychological findings.  More generally, theorists need to build models that explain the results of experiments that manipulate independent variables designed to test hypotheses rather than compete on predicting observational data.  We conclude by briefly summarizing various promising modelling approaches that focus on psychological data.},
  keywords = {comp-neuro,deep-learning},
  file = {/Users/jrudoler/Zotero/storage/2PP9BHF5/Bowers et al. - 2022 - Deep Problems with Neural Network Models of Human Vision.pdf}
}

@online{bredenbergDesiderataNormativeModels2023,
  title = {Desiderata for Normative Models of Synaptic Plasticity},
  author = {Bredenberg, Colin and Savin, Cristina},
  date = {2023-08-09},
  eprint = {2308.04988},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  doi = {10.48550/arXiv.2308.04988},
  url = {http://arxiv.org/abs/2308.04988},
  urldate = {2023-08-10},
  abstract = {Normative models of synaptic plasticity use a combination of mathematics and computational simulations to arrive at predictions of behavioral and network-level adaptive phenomena. In recent years, there has been an explosion of theoretical work on these models, but experimental confirmation is relatively limited. In this review, we organize work on normative plasticity models in terms of a set of desiderata which, when satisfied, are designed to guarantee that a model has a clear link between plasticity and adaptive behavior, consistency with known biological evidence about neural plasticity, and specific testable predictions. We then discuss how new models have begun to improve on these criteria and suggest avenues for further development. As prototypes, we provide detailed analyses of two specific models -- REINFORCE and the Wake-Sleep algorithm. We provide a conceptual guide to help develop neural learning theories that are precise, powerful, and experimentally testable.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/8XDJR34Y/Bredenberg and Savin - 2023 - Desiderata for normative models of synaptic plasticity.pdf}
}

@article{brettePhilosophySpikeRateBased2015,
  title = {Philosophy of the {{Spike}}: {{Rate-Based}} vs. {{Spike-Based Theories}} of the {{Brain}}},
  shorttitle = {Philosophy of the {{Spike}}},
  author = {Brette, Romain},
  date = {2015},
  journaltitle = {Frontiers in Systems Neuroscience},
  volume = {9},
  issn = {1662-5137},
  url = {https://www.frontiersin.org/articles/10.3389/fnsys.2015.00151},
  urldate = {2023-10-23},
  abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
  file = {/Users/jrudoler/Zotero/storage/7VJUSU4N/Brette - 2015 - Philosophy of the Spike Rate-Based vs. Spike-Base.pdf}
}

@online{brillAnalyticsHaveHumility2023,
  title = {Analytics, Have Some Humility: A Statistical View of Fourth-down Decision Making},
  shorttitle = {Analytics, Have Some Humility},
  author = {Brill, Ryan S. and Wyner, Abraham J.},
  date = {2023-11-06},
  eprint = {2311.03490},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2311.03490},
  url = {http://arxiv.org/abs/2311.03490},
  urldate = {2023-11-28},
  abstract = {Expected points (EP) and win probability (WP) are value functions fundamental to strategic in-game decision making in American football, particularly for fourth down decision making. The EP and WP functions which are widely used today are statistical models fit from historical data. These models, however, are subject to serious statistical flaws: selection bias, overfitting, ignoring autocorrelation, and ignoring uncertainty quantification. We develop a machine learning framework that accounts for these issues and extracts our analysis into a decision-making inference. Along the way, we introduce a novel methodological approach to mitigate overfitting in machine learning models. Specifically, we extend the catalytic prior, initially developed in the context of linear models, to smooth our tree machine learning models. Our final product is a major advance in fourth-down strategic decision making: far fewer fourth-down decisions are as obvious as analysts claim.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/H8K6JSJI/Brill and Wyner - 2023 - Analytics, have some humility a statistical view .pdf;/Users/jrudoler/Zotero/storage/QJF94S44/2311.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-04-19},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/T8Y3HAY9/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/jrudoler/Zotero/storage/S4LAGCJS/2005.html}
}

@online{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  date = {2023-04-13},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.12712},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-09-06},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/VX5QJNHG/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf;/Users/jrudoler/Zotero/storage/KKJ6KNNR/2303.html}
}

@online{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  date = {2023-08-16},
  eprint = {2308.08708},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2308.08708},
  url = {http://arxiv.org/abs/2308.08708},
  urldate = {2023-08-21},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/HRHEA3D4/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf;/Users/jrudoler/Zotero/storage/JD3FEKBF/Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf;/Users/jrudoler/Zotero/storage/NJTRB9DF/2308.html}
}

@online{carliniAreAlignedNeural2023,
  title = {Are Aligned Neural Networks Adversarially Aligned?},
  author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A. and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and Schmidt, Ludwig},
  date = {2023-06-26},
  eprint = {2306.15447},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.15447},
  url = {http://arxiv.org/abs/2306.15447},
  urldate = {2023-09-11},
  abstract = {Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/UX5CWGDU/Carlini et al. - 2023 - Are aligned neural networks adversarially aligned.pdf;/Users/jrudoler/Zotero/storage/Z9HJPN9X/2306.html}
}

@article{carpenterStanProbabilisticProgramming2017,
  title = {Stan: {{A Probabilistic Programming Language}}},
  shorttitle = {Stan},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  date = {2017-01-11},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {https://doi.org/10.18637/jss.v076.i01},
  urldate = {2022-11-08},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  langid = {english},
  keywords = {bayes},
  file = {/Users/jrudoler/Zotero/storage/F75KG4SS/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf}
}

@online{casperOpenProblemsFundamental2023,
  title = {Open {{Problems}} and {{Fundamental Limitations}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Raphaël and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Bıyık, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
  date = {2023-07-27},
  eprint = {2307.15217},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.15217},
  url = {http://arxiv.org/abs/2307.15217},
  urldate = {2023-09-06},
  abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/HP4YCB83/Casper et al. - 2023 - Open Problems and Fundamental Limitations of Reinf.pdf;/Users/jrudoler/Zotero/storage/3B3GEXMA/2307.html}
}

@article{cauchoisKnowingWhatYou2021,
  title = {Knowing What {{You Know}}: Valid and Validated Confidence Sets in Multiclass and Multilabel Prediction},
  author = {Cauchois, Maxime and Gupta, Suyash and Duchi, John C},
  date = {2021-03-21},
  journaltitle = {Journal of Machine Learning Research},
  abstract = {We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods—which give marginal validity (coverage) guarantees—provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically consistent) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model—neural network, random forest, boosted tree—to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/YTZ3GEYR/Cauchois et al. - Knowing what You Know valid and validated conﬁden.pdf}
}

@article{chakrabortyFakeProfileDetection2022,
  title = {Fake {{Profile Detection Using Machine Learning Techniques}}},
  author = {Chakraborty, Partha and Shazan, Mahim Musharof and Nahid, Mahamudul and Ahmed, Md Kaysar and Talukder, Prince Chandra},
  date = {2022-10-13},
  journaltitle = {Journal of Computer and Communications},
  volume = {10},
  number = {10},
  pages = {74--87},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/jcc.2022.1010006},
  url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=120727},
  urldate = {2023-04-11},
  abstract = {Our lives are significantly impacted by social media platforms such as Facebook, Twitter, Instagram, LinkedIn, and others. People are actively participating in it the world over. However, it also has to deal with the issue of bogus profiles. False accounts are frequently created by humans, bots, or computers. They are used to disseminate rumors and engage in illicit activities like identity theft and phishing. So, in this project, the author’ll talk about a detection model that uses a variety of machine learning techniques to distinguish between fake and real Twitter profiles based on attributes like follower and friend counts, status updates, and more. The author used the dataset of Twitter profiles, separating real accounts into TFP and E13 and false accounts into INT, TWT, and FSF. Here, the author discusses LSTM, XG Boost, Random Forest, and Neural Networks. The key characteristics are chosen to assess a social media profile’s authenticity. Hyperparameters and the architecture are also covered. Finally, results are produced after training the models. The output is therefore 0 for genuine profiles and 1 for false profiles. When a phony profile is discovered, it can be disabled or destroyed so that cyber security problems can be prevented. Python and the necessary libraries, such as Sklearn, Numpy, and Pandas, are used for implementation. At the end of this study, the author will come to the conclusion that XG Boost is the best machine learning technique for finding fake profiles.},
  issue = {10},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/R2C99Y6S/Chakraborty et al. - 2022 - Fake Profile Detection Using Machine Learning Tech.pdf}
}

@online{chizatLazyTrainingDifferentiable2020,
  title = {On {{Lazy Training}} in {{Differentiable Programming}}},
  author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  date = {2020-01-07},
  eprint = {1812.07956},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1812.07956},
  url = {http://arxiv.org/abs/1812.07956},
  urldate = {2023-03-26},
  abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/63L4EQKM/Chizat et al. - 2020 - On Lazy Training in Differentiable Programming.pdf;/Users/jrudoler/Zotero/storage/WYX7SS2S/1812.html}
}

@online{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  date = {2014-12-11},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.3555},
  url = {http://arxiv.org/abs/1412.3555},
  urldate = {2022-11-06},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/G24MB44N/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf;/Users/jrudoler/Zotero/storage/YJT2QUAE/1412.html}
}

@article{chungNeuralPopulationGeometry2021,
  title = {Neural Population Geometry: {{An}} Approach for Understanding Biological and Artificial Neural Networks},
  shorttitle = {Neural Population Geometry},
  author = {Chung, SueYeon and Abbott, L. F.},
  date = {2021-10-01},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {70},
  pages = {137--144},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2021.10.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438821001227},
  urldate = {2023-01-22},
  abstract = {Advances in experimental neuroscience have transformed our ability to explore the structure and function of neural circuits. At the same time, advances in machine learning have unleashed the remarkable computational power of artificial neural networks (ANNs). While these two fields have different tools and applications, they present a similar challenge: namely, understanding how information is embedded and processed through high-dimensional representations to solve complex tasks. One approach to addressing this challenge is to utilize mathematical and computational tools to analyze the geometry of these high-dimensional representations, i.e., neural population geometry. We review examples of geometrical approaches providing insight into the function of biological and artificial neural networks: representation untangling in perception, a geometric theory of classification capacity, disentanglement, and abstraction in cognitive systems, topological representations underlying cognitive maps, dynamic untangling in motor systems, and a dynamical approach to cognition. Together, these findings illustrate an exciting trend at the intersection of machine learning, neuroscience, and geometry, in which neural population geometry provides a useful population-level mechanistic descriptor underlying task implementation. Importantly, geometric descriptions are applicable across sensory modalities, brain regions, network architectures, and timescales. Thus, neural population geometry has the potential to unify our understanding of structure and function in biological and artificial neural networks, bridging the gap between single neurons, population activities, and behavior.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/5Z9F3ZI9/Chung and Abbott - 2021 - Neural population geometry An approach for unders.pdf;/Users/jrudoler/Zotero/storage/YPXPY968/S0959438821001227.html}
}

@online{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  date = {2022-12-06},
  eprint = {2210.11416},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.11416},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2023-09-20},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/FKPBW6SP/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf;/Users/jrudoler/Zotero/storage/PBBAP32X/2210.html}
}

@article{coxRegressionAnalysisBinary1958,
  title = {The {{Regression Analysis}} of {{Binary Sequences}}},
  author = {Cox, D. R.},
  date = {1958},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {20},
  number = {2},
  eprint = {2983890},
  eprinttype = {jstor},
  pages = {215--242},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  url = {https://www.jstor.org/stable/2983890},
  urldate = {2023-04-30},
  abstract = {A sequence of 0's and 1's is observed and it is suspected that the chance that a particular trial is a 1 depends on the value of one or more independent variables. Tests and estimates for such situations are considered, dealing first with problems in which the independent variable is preassigned and then with independent variables that are functions of the sequence. There is a considerable amount of earlier work, which is reviewed.},
  file = {/Users/jrudoler/Zotero/storage/C4BSRF78/Cox - 1958 - The Regression Analysis of Binary Sequences.pdf}
}

@online{curthUturnDoubleDescent2023,
  title = {A {{U-turn}} on {{Double Descent}}: {{Rethinking Parameter Counting}} in {{Statistical Learning}}},
  shorttitle = {A {{U-turn}} on {{Double Descent}}},
  author = {Curth, Alicia and Jeffares, Alan and family=Schaar, given=Mihaela, prefix=van der, useprefix=true},
  date = {2023-10-29},
  eprint = {2310.18988},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.18988},
  url = {http://arxiv.org/abs/2310.18988},
  urldate = {2023-11-02},
  abstract = {Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to what is being plotted on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and only) when and where the transition between these underlying axes occurs, and that its location is thus not inherently tied to the interpolation threshold p=n. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as smoothers and propose a generalized measure for the effective number of parameters they use on unseen examples, using which we find that their apparent double descent curves indeed fold back into more traditional convex shapes - providing a resolution to tensions between double descent and statistical intuition.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/SJS9WZYG/Curth et al. - 2023 - A U-turn on Double Descent Rethinking Parameter Counting in Statistical Learning.pdf}
}

@online{dabagiaComparingHighdimensionalNeural2022,
  title = {Comparing High-Dimensional Neural Recordings by Aligning Their Low-Dimensional Latent Representations},
  author = {Dabagia, Max and Kording, Konrad P. and Dyer, Eva L.},
  date = {2022-05-17},
  eprint = {2205.08413},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  doi = {10.48550/arXiv.2205.08413},
  url = {http://arxiv.org/abs/2205.08413},
  urldate = {2022-11-11},
  abstract = {Many questions in neuroscience involve understanding of the responses of large populations of neurons. However, when dealing with large-scale neural activity, interpretation becomes difficult, and comparisons between two animals, or across different time points becomes challenging. One major challenge that we face in modern neuroscience is that of correspondence, e.g. we do not record the exact same neurons at the exact same times. Without some way to link two or more datasets, comparing different collections of neural activity patterns becomes impossible. Here, we describe approaches for leveraging shared latent structure across neural recordings to tackle this correspondence challenge. We review algorithms that map two datasets into a shared space where they can be directly compared, and argue that alignment is key for comparing high-dimensional neural activities across times, subsets of neurons, and individuals.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/IHA3JA7M/Dabagia et al. - 2022 - Comparing high-dimensional neural recordings by al.pdf;/Users/jrudoler/Zotero/storage/4KZPBDY6/2205.html}
}

@online{daumeiiiFrustratinglyEasyDomain2009,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  author = {Daumé III, Hal},
  date = {2009-07-10},
  eprint = {0907.1815},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.0907.1815},
  url = {http://arxiv.org/abs/0907.1815},
  urldate = {2022-08-12},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/JSC2BZTY/Daumé III - 2009 - Frustratingly Easy Domain Adaptation.pdf;/Users/jrudoler/Zotero/storage/73APHS9S/0907.html}
}

@article{dawidWellCalibratedBayesian1982,
  title = {The {{Well-Calibrated Bayesian}}},
  author = {Dawid, A. P.},
  date = {1982-09-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {77},
  number = {379},
  pages = {605--610},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1982.10477856},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477856},
  urldate = {2023-03-02},
  abstract = {Suppose that a forecaster sequentially assigns probabilities to events. He is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent. We prove a theorem to the effect that a coherent Bayesian expects to be well calibrated, and consider its destructive implications for the theory of coherence.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/IRY49NC6/Dawid - 1982 - The Well-Calibrated Bayesian.pdf}
}

@article{debettencourtClosedloopTrainingAttention2015,
  title = {Closed-Loop Training of Attention with Real-Time Brain Imaging | {{Nature Neuroscience}}},
  author = {{deBettencourt}, Megan T and Cohen, Jonathan D and Lee, Ray F and Norman, Kenneth A and Turk-Browne, Nicholas B},
  date = {2015-03},
  journaltitle = {Nature Neuroscience},
  volume = {18},
  number = {3},
  pages = {470--475},
  issn = {1546-1726},
  doi = {10.1038/nn.3940},
  url = {https://www.nature.com/articles/nn.3940?version=meter+at+null&module=meter-Links&pgtype=article&contentId=&mediaId=&referrer=&priority=true&action=click&contentCollection=meter-links-click},
  urldate = {2023-01-04},
  abstract = {Lapses of attention are commonplace, potentially because they are detected too late to be prevented. The authors use real-time fMRI to provide participants continuous access to their attentional state. Real-time feedback, particularly from frontoparietal cortex, improved sustained attention abilities and modified representations in visual cortex and basal ganglia.},
  file = {/Users/jrudoler/Zotero/storage/64HFV42N/deBettencourt et al. - 2015 - Closed-loop training of attention with real-time brain imaging  Nature Neuroscience.pdf}
}

@online{dengFIFAMakingFairness2022,
  title = {{{FIFA}}: {{Making Fairness More Generalizable}} in {{Classifiers Trained}} on {{Imbalanced Data}}},
  shorttitle = {{{FIFA}}},
  author = {Deng, Zhun and Zhang, Jiayao and Zhang, Linjun and Ye, Ting and Coley, Yates and Su, Weijie J. and Zou, James},
  date = {2022-06-06},
  eprint = {2206.02792},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2206.02792},
  url = {http://arxiv.org/abs/2206.02792},
  urldate = {2023-12-14},
  abstract = {Algorithmic fairness plays an important role in machine learning and imposing fairness constraints during learning is a common approach. However, many datasets are imbalanced in certain label classes (e.g. "healthy") and sensitive subgroups (e.g. "older patients"). Empirically, this imbalance leads to a lack of generalizability not only of classification, but also of fairness properties, especially in over-parameterized models. For example, fairness-aware training may ensure equalized odds (EO) on the training data, but EO is far from being satisfied on new users. In this paper, we propose a theoretically-principled, yet Flexible approach that is Imbalance-Fairness-Aware (FIFA). Specifically, FIFA encourages both classification and fairness generalization and can be flexibly combined with many existing fair learning methods with logits-based losses. While our main focus is on EO, FIFA can be directly applied to achieve equalized opportunity (EqOpt); and under certain conditions, it can also be applied to other fairness notions. We demonstrate the power of FIFA by combining it with a popular fair classification algorithm, and the resulting algorithm achieves significantly better fairness generalization on several real-world datasets.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/METXV73A/Deng et al. - 2022 - FIFA Making Fairness More Generalizable in Classi.pdf;/Users/jrudoler/Zotero/storage/7X8FXCGM/2206.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/jrudoler/Zotero/storage/BK4CS9XC/5206848.html}
}

@online{detommasoFortunaLibraryUncertainty2023,
  title = {Fortuna: {{A Library}} for {{Uncertainty Quantification}} in {{Deep Learning}}},
  shorttitle = {Fortuna},
  author = {Detommaso, Gianluca and Gasparin, Alberto and Donini, Michele and Seeger, Matthias and Wilson, Andrew Gordon and Archambeau, Cedric},
  date = {2023-02-08},
  eprint = {2302.04019},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2302.04019},
  url = {http://arxiv.org/abs/2302.04019},
  urldate = {2023-11-21},
  abstract = {We present Fortuna, an open-source library for uncertainty quantification in deep learning. Fortuna supports a range of calibration techniques, such as conformal prediction that can be applied to any trained neural network to generate reliable uncertainty estimates, and scalable Bayesian inference methods that can be applied to Flax-based deep neural networks trained from scratch for improved uncertainty quantification and accuracy. By providing a coherent framework for advanced uncertainty quantification methods, Fortuna simplifies the process of benchmarking and helps practitioners build robust AI systems.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/M8KHRWP7/Detommaso et al. - 2023 - Fortuna A Library for Uncertainty Quantification .pdf;/Users/jrudoler/Zotero/storage/Q27TV7WG/2302.html}
}

@online{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-05-01},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/4U2L4GCY/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/jrudoler/Zotero/storage/RLDYM6VE/1810.html}
}

@online{donohoDataScienceSingularity2023,
  title = {Data {{Science}} at the {{Singularity}}},
  author = {Donoho, David},
  date = {2023-10-01},
  eprint = {2310.00865},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2310.00865},
  url = {http://arxiv.org/abs/2310.00865},
  urldate = {2023-10-05},
  abstract = {A purported `AI Singularity' has been in the public eye recently. Mass media and US national political attention focused on `AI Doom' narratives hawked by social media influencers. The European Commission is announcing initiatives to forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong narrative for what's happening now; recent happenings signal something else entirely. Something fundamental to computation-based research really changed in the last ten years. In certain fields, progress is dramatically more rapid than previously, as the fields undergo a transition to frictionless reproducibility (FR). This transition markedly changes the rate of spread of ideas and practices, affects mindsets, and erases memories of much that came before. The emergence of frictionless reproducibility follows from the maturation of 3 data science principles in the last decade. Those principles involve data sharing, code sharing, and competitive challenges, however implemented in the particularly strong form of frictionless open services. Empirical Machine Learning (EML) is todays leading adherent field, and its consequent rapid changes are responsible for the AI progress we see. Still, other fields can and do benefit when they adhere to the same principles. Many rapid changes from this maturation are misidentified. The advent of FR in EML generates a steady flow of innovations; this flow stimulates outsider intuitions that there's an emergent superpower somewhere in AI. This opens the way for PR to push worrying narratives: not only `AI Extinction', but also the supposed monopoly of big tech on AI research. The helpful narrative observes that the superpower of EML is adherence to frictionless reproducibility practices; these practices are responsible for the striking progress in AI that we see everywhere.},
  pubstate = {preprint},
  keywords = {priority,unread},
  file = {/Users/jrudoler/Zotero/storage/MIWX9DZK/Donoho - 2023 - Data Science at the Singularity.pdf;/Users/jrudoler/Zotero/storage/UYJG25HH/2310.html}
}

@online{doughertyNeuralCorrelatesMemory2022,
  title = {Neural Correlates of Memory in an Immersive Spatiotemporal Context},
  author = {Dougherty, Matthew R. and Chang, Woohyeuk and Rudoler, Joseph H. and Katerman, Brandon S. and Halpern, David J. and Bruska, James P. and Diamond, Nicholas B. and Kahana, Michael J.},
  date = {2022-12-17},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2022.11.30.518606},
  doi = {10.1101/2022.11.30.518606},
  url = {https://www.biorxiv.org/content/10.1101/2022.11.30.518606v2},
  urldate = {2023-02-19},
  abstract = {We investigated memory encoding and retrieval during a quasi-naturalistic spatial-episodic memory task in which subjects delivered items to landmarks in a virtual environment and later recalled the delivered items. Recall transitions revealed spatial and temporal organization of studied items. Using scalp electroencephalography (EEG), we asked whether neural signatures of successful encoding and retrieval uncovered in traditional word list tasks also appear when learning occurs within a spatiotemporal context. We found that increased theta (T +) and decreased alpha/beta (A−) accompanied successful encoding events, with the addition of increased gamma (G+) for retrieval events. Logistic-regression classifiers trained on these spectral features reliably predicted encoding and retrieval success in hold-out sessions. Both univariate and multivariate analyses of EEG data revealed a similar spectral T +A−G+ of successful encoding and retrieval. These findings extend behavioral and neural signatures of recall to naturalistic and multidimensional encoding conditions.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/4G53ZADV/Dougherty et al. - 2022 - Neural correlates of memory in an immersive spatio.pdf}
}

@article{drainvilleElephantMotorbikesToo2022,
  title = {Elephant Motorbikes and Too Many Neckties: Epistemic Spatialization as a Framework for Investigating Patterns of Bias in Convolutional Neural Networks},
  shorttitle = {Elephant Motorbikes and Too Many Neckties},
  author = {Drainville, Raymond and Vis, Farida},
  date = {2022-08-10},
  journaltitle = {AI \& SOCIETY},
  shortjournal = {AI \& Soc},
  issn = {1435-5655},
  doi = {10.1007/s00146-022-01542-8},
  url = {https://doi.org/10.1007/s00146-022-01542-8},
  urldate = {2022-08-16},
  abstract = {This article presents Epistemic Spatialization as a new framework for investigating the interconnected patterns of biases when identifying objects with convolutional neural networks (convnets). It draws upon Foucault’s notion of spatialized knowledge to guide its method of enquiry. We argue that decisions involved in the creation of algorithms, alongside the labeling, ordering, presentation, and commercial prioritization of objects, together create a distorted “nomination of the visible”: they harden the visibility of some objects, make other objects excessively visible, and consign yet others to permanent or haphazard invisibility. Our approach differs from those who focus on high-stakes misidentifications, such as errors tied to structural racism. Examining the far more dominant series of low-stakes mistakes shows the scope of errors, destabilizing the goal of image content identification with considerable societal impact. We explore these issues by closely examining the demonstration video of a popular convnet. This examination reveals an interlocking series of biases undermining the content identification process. The picture we paint is crucial for a better understanding of the errors that result as these convnets become further embedded in everyday products.~The framework is valuable for critical work on computer vision, AI studies, and large-scale visual analysis.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/64AQLW5F/Drainville and Vis - 2022 - Elephant motorbikes and too many neckties epistem.pdf}
}

@article{driscollFlexibleMultitaskComputation2022,
  title = {Flexible Multitask Computation in Recurrent Networks Utilizes Shared Dynamical Motifs},
  author = {Driscoll, Laura and Shenoy, Krishna and Sussillo, David},
  date = {2022-08-15},
  pages = {2022.08.15.503870},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.15.503870},
  url = {https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1},
  urldate = {2022-08-18},
  abstract = {Flexible computation is a hallmark of intelligent behavior. Yet, little is known about how neural networks contextually reconfigure for different computations. Humans are able to perform a new task without extensive training, presumably through the composition of elementary processes that were previously learned. Cognitive scientists have long hypothesized the possibility of a compositional neural code, where complex neural computations are made up of constituent components; however, the neural substrate underlying this structure remains elusive in biological and artificial neural networks. Here we identified an algorithmic neural substrate for compositional computation through the study of multitasking artificial recurrent neural networks. Dynamical systems analyses of networks revealed learned computational strategies that mirrored the modular subtask structure of the task-set used for training. Dynamical motifs such as attractors, decision boundaries and rotations were reused across different task computations. For example, tasks that required memory of a continuous circular variable repurposed the same ring attractor. We show that dynamical motifs are implemented by clusters of units and are reused across different contexts, allowing for flexibility and generalization of previously learned computation. Lesioning these clusters resulted in modular effects on network performance: a lesion that destroyed one dynamical motif only minimally perturbed the structure of other dynamical motifs. Finally, modular dynamical motifs could be reconfigured for fast transfer learning. After slow initial learning of dynamical motifs, a subsequent faster stage of learning reconfigured motifs to perform novel tasks. This work contributes to a more fundamental understanding of compositional computation underlying flexible general intelligence in neural systems. We present a conceptual framework that establishes dynamical motifs as a fundamental unit of computation, intermediate between the neuron and the network. As more whole brain imaging studies record neural activity from multiple specialized systems simultaneously, the framework of dynamical motifs will guide questions about specialization and generalization across brain regions.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/QJCFLX2Y/Driscoll et al. - 2022 - Flexible multitask computation in recurrent networ.pdf;/Users/jrudoler/Zotero/storage/RN5MBULG/2022.08.15.html}
}

@article{duaneHybridMonteCarlo1987,
  title = {Hybrid {{Monte Carlo}}},
  author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan},
  date = {1987-09-03},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  volume = {195},
  number = {2},
  pages = {216--222},
  issn = {0370-2693},
  doi = {10.1016/0370-2693(87)91197-X},
  url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
  urldate = {2022-11-08},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  langid = {english},
  keywords = {bayes},
  file = {/Users/jrudoler/Zotero/storage/3HJNREYD/Duane et al. - 1987 - Hybrid Monte Carlo.pdf;/Users/jrudoler/Zotero/storage/3959F54J/037026938791197X.html}
}

@online{duttaExploringStickyMittens2022,
  title = {Exploring with {{Sticky Mittens}}: {{Reinforcement Learning}} with {{Expert Interventions}} via {{Option Templates}}},
  shorttitle = {Exploring with {{Sticky Mittens}}},
  author = {Dutta, Souradeep and Sridhar, Kaustubh and Bastani, Osbert and Dobriban, Edgar and Weimer, James and Lee, Insup and Parish-Morris, Julia},
  date = {2022-11-17},
  eprint = {2202.12967},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2202.12967},
  url = {http://arxiv.org/abs/2202.12967},
  urldate = {2023-03-02},
  abstract = {Long horizon robot learning tasks with sparse rewards pose a significant challenge for current reinforcement learning algorithms. A key feature enabling humans to learn challenging control tasks is that they often receive expert intervention that enables them to understand the high-level structure of the task before mastering low-level control actions. We propose a framework for leveraging expert intervention to solve long-horizon reinforcement learning tasks. We consider \textbackslash emph\{option templates\}, which are specifications encoding a potential option that can be trained using reinforcement learning. We formulate expert intervention as allowing the agent to execute option templates before learning an implementation. This enables them to use an option, before committing costly resources to learning it. We evaluate our approach on three challenging reinforcement learning problems, showing that it outperforms state-of-the-art approaches by two orders of magnitude. Videos of trained agents and our code can be found at: https://sites.google.com/view/stickymittens},
  pubstate = {preprint},
  keywords = {AI,machine-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/33WMG266/Dutta et al. - 2022 - Exploring with Sticky Mittens Reinforcement Learn.pdf;/Users/jrudoler/Zotero/storage/M892HIJ5/2202.html}
}

@book{efronIntroductionBootstrap1994,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  date = {1994-05-15},
  eprint = {gLlpIUxRntoC},
  eprinttype = {googlebooks},
  publisher = {{CRC Press}},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  isbn = {978-0-412-04231-7},
  langid = {english},
  pagetotal = {456}
}

@online{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  date = {2023-05-24},
  eprint = {2305.07759},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.07759},
  url = {http://arxiv.org/abs/2305.07759},
  urldate = {2023-09-13},
  abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/TA7C85EA/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf;/Users/jrudoler/Zotero/storage/49CLZKNC/2305.html}
}

@article{ernstPermutationMethodsBasis2004,
  title = {Permutation {{Methods}}: {{A Basis}} for {{Exact Inference}}},
  shorttitle = {Permutation {{Methods}}},
  author = {Ernst, Michael D.},
  date = {2004-11},
  journaltitle = {Statistical Science},
  volume = {19},
  number = {4},
  pages = {676--685},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342304000000396},
  url = {https://projecteuclid.org/journals/statistical-science/volume-19/issue-4/Permutation-Methods-A-Basis-for-Exact-Inference/10.1214/088342304000000396.full},
  urldate = {2023-04-11},
  abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difficulties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/Y2AZXXIC/Ernst - 2004 - Permutation Methods A Basis for Exact Inference.pdf}
}

@online{FairnessCriminalJustice,
  title = {Fairness in {{Criminal Justice Risk Assessments}}: {{The State}} of the {{Art}} - {{Richard Berk}}, {{Hoda Heidari}}, {{Shahin Jabbari}}, {{Michael Kearns}}, {{Aaron Roth}}, 2021},
  url = {https://journals-sagepub-com.proxy.library.upenn.edu/doi/full/10.1177/0049124118782533},
  urldate = {2023-12-14},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/WXV5LYL3/0049124118782533.html}
}

@article{fawcettIntroductionROCAnalysis2006,
  title = {An Introduction to {{ROC}} Analysis},
  author = {Fawcett, Tom},
  date = {2006-06-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  series = {{{ROC Analysis}} in {{Pattern Recognition}}},
  volume = {27},
  number = {8},
  pages = {861--874},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2005.10.010},
  url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
  urldate = {2023-04-11},
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/MR373HKU/Fawcett - 2006 - An introduction to ROC analysis.pdf;/Users/jrudoler/Zotero/storage/4SA4WYSC/S016786550500303X.html}
}

@article{feinmanLEARNINGTASKGENERALREPRESENTATIONS2021,
  title = {{{LEARNING TASK-GENERAL REPRESENTATIONS WITH GENERATIVE NEURO-SYMBOLIC MODELING}}},
  author = {Feinman, Reuben and Lake, Brenden M},
  date = {2021},
  pages = {20},
  abstract = {People can learn rich, general-purpose conceptual representations from only raw perceptual inputs. Current machine learning approaches fall well short of these human standards, although different modeling traditions often have complementary strengths. Symbolic models can capture the compositional and causal knowledge that enables flexible generalization, but they struggle to learn from raw inputs, relying on strong abstractions and simplifying assumptions. Neural network models can learn directly from raw data, but they struggle to capture compositional and causal structure and typically must retrain to tackle new tasks. We bring together these two traditions to learn generative models of concepts that capture rich compositional and causal structure, while learning from raw data. We develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. The distributions of parts (strokes), and correlations between parts, are modeled with neural network subroutines, allowing the model to learn directly from raw data and express nonparametric statistical relationships. We apply our model to the Omniglot challenge of human-level concept learning, using a background set of alphabets to learn an expressive prior distribution over character drawings. In a subsequent evaluation, our GNS model uses probabilistic inference to learn rich conceptual representations from a single training image that generalize to 4 unique tasks, succeeding where previous work has fallen short.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/FCSCLTYY/Feinman and Lake - 2021 - LEARNING TASK-GENERAL REPRESENTATIONS WITH GENERAT.pdf}
}

@online{feldmanImprovingConditionalCoverage2021,
  title = {Improving {{Conditional Coverage}} via {{Orthogonal Quantile Regression}}},
  author = {Feldman, Shai and Bates, Stephen and Romano, Yaniv},
  date = {2021-10-02},
  eprint = {2106.00394},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.00394},
  url = {http://arxiv.org/abs/2106.00394},
  urldate = {2023-12-14},
  abstract = {We develop a method to generate prediction intervals that have a user-specified coverage level across all regions of feature-space, a property called conditional coverage. A typical approach to this task is to estimate the conditional quantiles with quantile regression -- it is well-known that this leads to correct coverage in the large-sample limit, although it may not be accurate in finite samples. We find in experiments that traditional quantile regression can have poor conditional coverage. To remedy this, we modify the loss function to promote independence between the size of the intervals and the indicator of a miscoverage event. For the true conditional quantiles, these two quantities are independent (orthogonal), so the modified loss function continues to be valid. Moreover, we empirically show that the modified loss function leads to improved conditional coverage, as evaluated by several metrics. We also introduce two new metrics that check conditional coverage by looking at the strength of the dependence between the interval size and the indicator of miscoverage.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/L7SXJSI9/Feldman et al. - 2021 - Improving Conditional Coverage via Orthogonal Quan.pdf;/Users/jrudoler/Zotero/storage/QBT6Z4KG/2106.html}
}

@book{fisherDesignExperiments1935,
  title = {The {{Design}} of {{Experiments}}},
  author = {Fisher, R. A.},
  date = {1935},
  publisher = {{Hafner Press}},
  location = {{New York, NY}}
}

@online{flemingIntegratedInformationTheory2023,
  title = {The {{Integrated Information Theory}} of {{Consciousness}} as {{Pseudoscience}}},
  author = {Fleming, Stephen and Frith, Chris and Goodale, Mel and Lau, Hakwan and LeDoux, Joseph E. and Lee, Alan L. F. and Michel, Matthias and Owen, Adrian and Peters, Megan A. K. and Slagter, Heleen A.},
  date = {2023-09-16T08:14:22},
  doi = {10.31234/osf.io/zsr78},
  url = {https://psyarxiv.com/zsr78/},
  urldate = {2023-09-16},
  abstract = {The media, including news articles in both Nature and Science, have recently celebrated the Integrated Information Theory (IIT) as a ‘leading’ and empirically tested theory of consciousness. We are writing as researchers with some relevant expertise to express our concerns.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/NBNW4A3F/Fleming et al. - 2023 - The Integrated Information Theory of Consciousness as Pseudoscience.pdf}
}

@article{fodorConnectionismCognitiveArchitecture1988a,
  title = {Connectionism and Cognitive Architecture: {{A}} Critical Analysis},
  shorttitle = {Connectionism and Cognitive Architecture},
  author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
  date = {1988-03-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {28},
  number = {1},
  pages = {3--71},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(88)90031-5},
  url = {https://www.sciencedirect.com/science/article/pii/0010027788900315},
  urldate = {2023-11-02},
  abstract = {This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation. Résumé Cet articleétudie les différences entre modèles connectionistes et modèles classiques de la structure cognitive. Nous pensons que, bien que les deux types de modèles stipulent l'existence d'états mentaux représentationnels, la différence essentielle est que seuls les modèles classiques requièrent l'existence d'un niveau de représentation symbolique—un “langage de la pensée”—, c'est-à-dire d'états représentationnels possédant une structure syntaxique et sémantique. Nous examinons ensuite différents arguments qui militent en faveur de l'existence de représentations mentales ayant ces propriétés. Certains de ces arguments reposent sur la “systématicité” des représentations mentales, c'est-à-dire sur le fait que les capacités cognitives exhibent toujours certaines symétries, de sorte que la capacitéd'entretenir certaines pensées implique la capacitéd'entretenir d'autres pensées apparentées par leur contenu sémantique. Nous pensons que ces arguments montrent de manière convainquante que l'architecture de l'esprit/du cerveau n'est pas connectioniste au niveau cognitif. Nous nous demandons ensuite s'il est possible d'interpréter le connectionisme comme une analyse des structures neuronales (ou des structures neurologiques “abstraites”) dans lesquelles est réalisée l'architecture cognitive classique. Nous examinons plusieurs des arguments avancés habituellement en défense du connectionisme, et en concluons que ceux-ci n'ont de sens que dans cette interprétation.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/ABCGRIGM/Fodor and Pylyshyn - 1988 - Connectionism and cognitive architecture A critic.pdf;/Users/jrudoler/Zotero/storage/F393UEUY/0010027788900315.html}
}

@article{fortunatoScienceScience2018,
  title = {Science of Science},
  author = {Fortunato, Santo and Bergstrom, Carl T. and Börner, Katy and Evans, James A. and Helbing, Dirk and Milojević, Staša and Petersen, Alexander M. and Radicchi, Filippo and Sinatra, Roberta and Uzzi, Brian and Vespignani, Alessandro and Waltman, Ludo and Wang, Dashun and Barabási, Albert-László},
  date = {2018-03-02},
  journaltitle = {Science},
  volume = {359},
  number = {6379},
  pages = {eaao0185},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aao0185},
  url = {https://www.science.org/doi/10.1126/science.aao0185},
  urldate = {2023-10-04},
  abstract = {Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/B87HQ4TK/Fortunato et al. - 2018 - Science of science.pdf}
}

@article{frankBridgingDataGap2023,
  title = {Bridging the Data Gap between Children and Large Language Models},
  author = {Frank, Michael C.},
  date = {2023-08-31},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2023.08.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661323002036},
  urldate = {2023-09-03},
  abstract = {Large language models (LLMs) show intriguing emergent behaviors, yet they receive around four or five orders of magnitude more language data than human children. What accounts for this vast difference in sample efficiency? Candidate explanations include children’s pre-existing conceptual knowledge, their use of multimodal grounding, and the interactive, social nature of their input.},
  keywords = {unread}
}

@article{freemanSimulatedPowerSpectral2009,
  title = {Simulated Power Spectral Density ({{PSD}}) of Background Electrocorticogram ({{ECoG}})},
  author = {Freeman, Walter J. and Zhai, Jian},
  date = {2009-03-01},
  journaltitle = {Cognitive Neurodynamics},
  shortjournal = {Cogn Neurodyn},
  volume = {3},
  number = {1},
  pages = {97--103},
  issn = {1871-4099},
  doi = {10.1007/s11571-008-9064-y},
  url = {https://doi.org/10.1007/s11571-008-9064-y},
  urldate = {2022-08-23},
  abstract = {The ECoG background activity of cerebral cortex in states of rest and slow wave sleep resembles broadband noise. The power spectral density (PSD) then may often conform to a power-law distribution: a straight line in coordinates of log power vs. log frequency. The exponent, x, of the distribution, 1/fx, ranges between 2 and 4. These findings are explained with a model of the neural source of the background activity in mutual excitation among pyramidal cells. The dendritic response of a population of interactive excitatory neurons to an impulse input is a rapid exponential rise and a slow exponential decay, which can be fitted with the sum of two exponential terms. When that function is convolved as the kernel with pulses from a Poisson process and summed, the resulting “brown” or “black noise conforms to the ECoG time series and the PSD in rest and sleep. The PSD slope is dependent on the rate of rise. The variation in the observed slope is attributed to variation in the level of the background activity that is homeostatically regulated by the refractory periods of the excitatory neurons. Departures in behavior from rest and sleep to action are accompanied by local peaks in the PSD, which manifest emergent nonrandom structure in the ECoG, and which prevent reliable estimation of the 1/fx exponents in active states. We conclude that the resting ECoG truly is low-dimensional noise, and that the resting state is an optimal starting point for defining and measuring both artifactual and physiological structures emergent in the activated ECoG.},
  langid = {english},
  keywords = {eeg,unread},
  file = {/Users/jrudoler/Zotero/storage/6S4ZAUXS/Freeman and Zhai - 2009 - Simulated power spectral density (PSD) of backgrou.pdf}
}

@online{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2016-10-04},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1506.02142},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2023-12-13},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/ZLXACSXL/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf;/Users/jrudoler/Zotero/storage/EJ2ZJMRU/1506.html}
}

@article{ganinDomainAdversarialTrainingNeural2016,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {59},
  pages = {1--35},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v17/15-239.html},
  urldate = {2022-09-05},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
  file = {/Users/jrudoler/Zotero/storage/M7IT5E9U/Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf}
}

@inproceedings{ganinUnsupervisedDomainAdaptation2015,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  date = {2015-06-01},
  pages = {1180--1189},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/ganin15.html},
  urldate = {2022-08-12},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/MRB25DW2/Ganin and Lempitsky - 2015 - Unsupervised Domain Adaptation by Backpropagation.pdf}
}

@inproceedings{ganzPermutationTestsClassification2017,
  title = {Permutation Tests for Classification: Revisited},
  shorttitle = {Permutation Tests for Classification},
  booktitle = {2017 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}} ({{PRNI}})},
  author = {Ganz, Melanie and Konukoglu, Ender},
  date = {2017-06},
  pages = {1--4},
  doi = {10.1109/PRNI.2017.7981495},
  abstract = {In recent years, the focus on validating the statistical methods used in the field of neuroimaging has increased. While several papers have already highlighted the importance of non-parametric methods and especially permutation testing for general linear models (GLMs), it seems like the importance of validating classification results other than through cross-validation has taken a back seat. But classification, especially binary classification, is one of the most common tools in neuroimaging. Often permutations are not performed using the argument that they are too computationally expensive, especially for trainingintensive classifier as e.g. neural networks. In the following we want to re-visit the use of permutation tests for validating cross-validation results statistically and employ recent approximate permutation methods that reduce the number of permutations that need to be performed. We evaluate the feasibility of using full as well as approximate permutation methods in the extreme cases of small and unbalanced data sets. Our results indicate the applicability of a tail and Gamma approximation to perform permutation testing for binary classification tasks.},
  eventtitle = {2017 {{International Workshop}} on {{Pattern Recognition}} in {{Neuroimaging}} ({{PRNI}})},
  keywords = {neuroimaging,permutation},
  file = {/Users/jrudoler/Zotero/storage/MWVLCAJN/7981495.html}
}

@online{gargWhatCanTransformers2023,
  title = {What {{Can Transformers Learn In-Context}}? {{A Case Study}} of {{Simple Function Classes}}},
  shorttitle = {What {{Can Transformers Learn In-Context}}?},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  date = {2023-08-11},
  eprint = {2208.01066},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.01066},
  url = {http://arxiv.org/abs/2208.01066},
  urldate = {2023-09-11},
  abstract = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/LXRX6A7T/Garg et al. - 2023 - What Can Transformers Learn In-Context A Case Stu.pdf;/Users/jrudoler/Zotero/storage/NBCLCXQB/2208.html}
}

@article{gelmanGardenForkingPaths,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No “Fishing Expedition” or “p-Hacking” and the Research Hypothesis Was Posited Ahead of Time},
  author = {Gelman, Andrew and Loken, Eric},
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/764RZFU5/Gelman and Loken - The garden of forking paths Why multiple comparis.pdf}
}

@online{geRethinkingFairnessHumanAI2023,
  title = {Rethinking {{Fairness}} for {{Human-AI Collaboration}}},
  author = {Ge, Haosen and Bastani, Hamsa and Bastani, Osbert},
  date = {2023-10-05},
  eprint = {2310.03647},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.03647},
  url = {http://arxiv.org/abs/2310.03647},
  urldate = {2023-11-30},
  abstract = {Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are simultaneously fair in isolation, compliance-robustly fair, and more accurate than the human policy; thus, if our goal is to improve the equity and accuracy of human-AI collaboration, it may not be desirable to enforce traditional fairness constraints.},
  pubstate = {preprint},
  keywords = {fairness,unread},
  file = {/Users/jrudoler/Zotero/storage/IS9XYU2X/Ge et al. - 2023 - Rethinking Fairness for Human-AI Collaboration.pdf;/Users/jrudoler/Zotero/storage/M9EM2NBJ/2310.html}
}

@article{gershmanWhatHaveWe,
  title = {What Have We Learned about Artificial Intelligence from Studying the Brain?},
  author = {Gershman, Samuel J},
  abstract = {Neuroscience and artificial intelligence (AI) share a long, intertwined history. It has been argued that discoveries in neuroscience were (and continue to be) instrumental in driving the development of new AI technology. Scrutinizing these historical claims yields a more nuanced story, where AI researchers were loosely inspired by the brain, but ideas flowed mostly in the other direction.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/73FQ3S49/Gershman - What have we learned about artificial intelligence.pdf}
}

@online{gevaTransformerFeedForwardLayers2021,
  title = {Transformer {{Feed-Forward Layers Are Key-Value Memories}}},
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  date = {2021-09-05},
  eprint = {2012.14913},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.14913},
  url = {http://arxiv.org/abs/2012.14913},
  urldate = {2023-09-23},
  abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/H7Y5BFUM/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memories.pdf}
}

@inproceedings{gibbsAdaptiveConformalInference2021,
  title = {Adaptive {{Conformal Inference Under Distribution Shift}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gibbs, Isaac and Candes, Emmanuel},
  date = {2021},
  volume = {34},
  pages = {1660--1672},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/0d441de75945e5acbc865406fc9a2559-Abstract.html},
  urldate = {2023-11-17},
  abstract = {We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.},
  keywords = {priority},
  file = {/Users/jrudoler/Zotero/storage/8VS75MRI/Gibbs and Candes - 2021 - Adaptive Conformal Inference Under Distribution Sh.pdf}
}

@article{gokcenDisentanglingFlowSignals2022,
  title = {Disentangling the Flow of Signals between Populations of Neurons | {{Nature Computational Science}}},
  author = {Gokcen, Evren and Jasper, Anna I. and Semedo, João D. and Zandvakili, Amin and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
  date = {2022-08},
  journaltitle = {Nature Computational Science},
  volume = {2},
  number = {8},
  pages = {512--525},
  issn = {2662-8457},
  doi = {10.1038/s43588-022-00282-5},
  url = {https://www.nature.com/articles/s43588-022-00282-5},
  urldate = {2022-08-19},
  abstract = {Technological advances now allow us to record from large populations of neurons across multiple brain areas. These recordings may illuminate how communication between areas contributes to brain function, yet a substantial barrier remains: how do we disentangle the concurrent, bidirectional flow of signals between populations of neurons? We propose here a dimensionality reduction framework, delayed latents across groups (DLAG), that disentangles signals relayed in each direction, identifies how these signals are represented by each population and characterizes how they evolve within and across trials. We demonstrate that DLAG performs well on synthetic datasets similar in scale to current neurophysiological recordings. Then we study simultaneously recorded populations in primate visual areas V1 and V2, where DLAG reveals signatures of bidirectional yet selective communication. Our framework lays a foundation for dissecting the intricate flow of signals across populations of neurons, and how this signalling contributes to cortical computation.}
}

@online{goldblumNoFreeLunch2023,
  title = {The {{No Free Lunch Theorem}}, {{Kolmogorov Complexity}}, and the {{Role}} of {{Inductive Biases}} in {{Machine Learning}}},
  author = {Goldblum, Micah and Finzi, Marc and Rowan, Keefer and Wilson, Andrew Gordon},
  date = {2023-04-11},
  eprint = {2304.05366},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2304.05366},
  url = {http://arxiv.org/abs/2304.05366},
  urldate = {2023-08-17},
  abstract = {No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/9BEIFRYI/Goldblum et al. - 2023 - The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning.pdf}
}

@article{gollandPermutationTestsClassification2003,
  title = {Permutation Tests for Classification: Towards Statistical Significance in Image-Based Studies},
  shorttitle = {Permutation Tests for Classification},
  author = {Golland, Polina and Fischl, Bruce},
  date = {2003-07},
  journaltitle = {Information Processing in Medical Imaging: Proceedings of the ... Conference},
  shortjournal = {Inf Process Med Imaging},
  volume = {18},
  eprint = {15344469},
  eprinttype = {pmid},
  pages = {330--341},
  issn = {1011-2499},
  doi = {10.1007/978-3-540-45087-0_28},
  abstract = {Estimating statistical significance of detected differences between two groups of medical scans is a challenging problem due to the high dimensionality of the data and the relatively small number of training examples. In this paper, we demonstrate a non-parametric technique for estimation of statistical significance in the context of discriminative analysis (i.e., training a classifier function to label new examples into one of two groups). Our approach adopts permutation tests, first developed in classical statistics for hypothesis testing, to estimate how likely we are to obtain the observed classification performance, as measured by testing on a hold-out set or cross-validation, by chance. We demonstrate the method on examples of both structural and functional neuroimaging studies.},
  langid = {english},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/QMHFD25K/Golland and Fischl - 2003 - Permutation tests for classification towards stat.pdf}
}

@incollection{gollandPermutationTestsClassification2005,
  title = {Permutation {{Tests}} for {{Classification}}},
  booktitle = {Learning {{Theory}}},
  author = {Golland, Polina and Liang, Feng and Mukherjee, Sayan and Panchenko, Dmitry},
  editor = {Auer, Peter and Meir, Ron},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2005},
  volume = {3559},
  pages = {501--515},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11503415_34},
  url = {http://link.springer.com/10.1007/11503415_34},
  urldate = {2022-12-20},
  abstract = {Permutation tests have been proposed for a variety of problems going back to the early works of Fisher. We describe a permutation procedure used extensively in classification problems in computational biology and medical imaging. We empirically study the procedure on simulated data and real examples from neuroimaging studies and DNA microarray analysis. A theoretical analysis is suggested to assess the asymptotic behavior of the test. An interesting observation is that concentration of the permutation procedure is controlled by a Rademacher average which also controls the concentration of empirical errors to expected errors. A byproduct of the analysis is a uniform central limit theorem for a permutation procedure.},
  isbn = {978-3-540-26556-6 978-3-540-31892-7},
  langid = {english},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/K6WTU5UD/Golland et al. - 2005 - Permutation Tests for Classification.pdf}
}

@book{goodPermutationTestsPractical2013,
  title = {Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses},
  author = {Good, Phillip},
  date = {2013},
  publisher = {{Springer Science \& Business Media}},
  keywords = {permutation}
}

@article{goyalInductiveBiasesDeep2022,
  title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  author = {Goyal, Anirudh and Bengio, Yoshua},
  date = {2022-10-12},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {478},
  number = {2266},
  pages = {20210068},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2021.0068},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068},
  urldate = {2023-01-11},
  abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
  keywords = {causality,deep-learning,ood},
  file = {/Users/jrudoler/Zotero/storage/RAFHWH72/Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level.pdf}
}

@article{greenlandRandomizationStatisticsCausal1990,
  title = {Randomization, {{Statistics}}, and {{Causal Inference}}},
  author = {Greenland, Sander},
  date = {1990-11},
  journaltitle = {Epidemiology},
  volume = {1},
  number = {6},
  pages = {421},
  issn = {1044-3983},
  url = {https://journals.lww.com/epidem/Abstract/1990/11000/Randomization,_Statistics,_and_Causal_Inference.3.aspx?casa_token=xCTJXoOiIkkAAAAA:tabgyMrT7KEfLcG9GZnxwIo-ZEN-FgM3pFGpKjQlN3Nu9BOuVdjgCCVyaxPu7HzQ9kZf95lwR-K8v4fVRdO2fA},
  urldate = {2023-08-08},
  abstract = {This paper reviews the role of statistics in causal inference. Special attention is given to the need for randomization to justify causal inferences from conventional statistics, and the need for random sampling to justify descriptive inferences. In most epidemiologic studies, randomization and random sampling play little or no role in the assembly of study cohorts. I therefore conclude that probabilistic interpretations of conventional statistics are rarely justified, and that such interpretations may encourage misinter pretation of nonrandomized studies. Possible remedies for this problem include deemphasizing inferential statistics in favor of data descriptors, and adopting statistical techniques based on more realistic probability models than those in common use. (Epidemi},
  langid = {american},
  keywords = {causality},
  file = {/Users/jrudoler/Zotero/storage/EKMZYP9I/Greenland - 1990 - Randomization, Statistics, and Causal Inference.pdf}
}

@online{grosseStudyingLargeLanguage2023,
  title = {Studying {{Large Language Model Generalization}} with {{Influence Functions}}},
  author = {Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and Hubinger, Evan and Lukošiūtė, Kamilė and Nguyen, Karina and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R.},
  date = {2023-08-07},
  eprint = {2308.03296},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2308.03296},
  url = {http://arxiv.org/abs/2308.03296},
  urldate = {2023-09-23},
  abstract = {When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/8SMCN5SH/Grosse et al. - 2023 - Studying Large Language Model Generalization with Influence Functions.pdf}
}

@online{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and family=Rosa, given=Gustavo, prefix=de, useprefix=true and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  date = {2023-06-20},
  eprint = {2306.11644},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11644},
  url = {http://arxiv.org/abs/2306.11644},
  urldate = {2023-09-06},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/ZBZTQVW2/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf;/Users/jrudoler/Zotero/storage/47AVF8DJ/2306.html}
}

@inproceedings{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-07-17},
  pages = {1321--1330},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/guo17a.html},
  urldate = {2023-11-21},
  abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/IGAN5QMS/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf;/Users/jrudoler/Zotero/storage/TIA4LRSN/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@online{gurneeLanguageModelsRepresent2023,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  date = {2023-10-03},
  eprint = {2310.02207},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.02207},
  url = {http://arxiv.org/abs/2310.02207},
  urldate = {2023-10-04},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/ITXXKAA4/Gurnee and Tegmark - 2023 - Language Models Represent Space and Time.pdf;/Users/jrudoler/Zotero/storage/T34GCJR8/2310.html}
}

@article{guyonWhatSizeTest1998,
  title = {What {{Size Test Set Gives Good Error Rate Estimates}}?},
  author = {Guyon, Isabelle and Makhoul, John and Schwartz, Richard and Vapnik, Vladimir},
  date = {1998-01-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {01},
  pages = {52--64},
  publisher = {{IEEE Computer Society}},
  issn = {0162-8828},
  doi = {10.1109/34.655649},
  url = {https://www.computer.org/csdl/journal/tp/1998/01/i0052/13rRUyYjK3I},
  urldate = {2022-12-21},
  abstract = {Abstract—We address the problem of determining what size test set guarantees statistically significant results in a character recognition task, as a function of the expected error rate. We provide a statistical analysis showing that if, for example, the expected character error rate is around 1 percent, then, with a test set of at least 10,000 statistically independent handwritten characters (which could be obtained by taking 100 characters from each of 100 different writers), we guarantee, with 95 percent confidence, that: (1) The expected value of the character error rate is not worse than 1.25 E, where E is the empirical character error rate of the best recognizer, calculated on the test set; and (2) a difference of 0.3 E between the error rates of two recognizers is significant. We developed this framework with character recognition applications in mind, but it applies as well to speech recognition and to other pattern recognition problems.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/D34R3U8U/Guyon et al. - 1998 - What Size Test Set Gives Good Error Rate Estimates.pdf}
}

@online{haspelReverseEngineerEntire2023,
  title = {To Reverse Engineer an Entire Nervous System},
  author = {Haspel, Gal and Boyden, Edward S. and Brown, Jeffrey and Church, George and Cohen, Netta and Fang-Yen, Christopher and Flavell, Steven and Goodman, Miriam B. and Hart, Anne C. and Hobert, Oliver and Kagias, Konstantinos and Lockery, Shawn and Lu, Yangning and Marblestone, Adam and Matelsky, Jordan and Pfister, Hanspeter and Rotstein, Horacio G. and Scholz, Monika and Shlizerman, Eli and Simeon, Quilee and Venkatachalam, Vivek and Yang, Guangyu Robert and Yemini, Eviatar and Zimmer, Manuel and Kording, Konrad P.},
  date = {2023-08-12},
  eprint = {2308.06578},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  doi = {10.48550/arXiv.2308.06578},
  url = {http://arxiv.org/abs/2308.06578},
  urldate = {2023-08-16},
  abstract = {There are many theories of how behavior may be controlled by neurons. Testing and refining these theories would be greatly facilitated if we could correctly simulate an entire nervous system so we could replicate the brain dynamics in response to any stimuli or contexts. Besides, simulating a nervous system is in itself one of the big dreams in systems neuroscience. However, doing so requires us to identify how each neuron's output depends on its inputs, a process we call reverse engineering. Current efforts at this focus on the mammalian nervous system, but these brains are mind-bogglingly complex, allowing only recordings of tiny subsystems. Here we argue that the time is ripe for systems neuroscience to embark on a concerted effort to reverse engineer a smaller system and that Caenorhabditis elegans is the ideal candidate system as the established optophysiology techniques can capture and control each neuron's activity and scale to hundreds of thousands of experiments. Data across populations and behaviors can be combined because across individuals the nervous system is largely conserved in form and function. Modern machine-learning-based modeling should then enable a simulation of C. elegans' impressive breadth of brain states and behaviors. The ability to reverse engineer an entire nervous system will benefit the design of artificial intelligence systems and all of systems neuroscience, enabling fundamental insights as well as new approaches for investigations of progressively larger nervous systems.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/LYWEHDU5/Haspel et al. - 2023 - To reverse engineer an entire nervous system.pdf;/Users/jrudoler/Zotero/storage/FKYRU6YV/2308.html}
}

@online{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2012-07-03},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1207.0580},
  url = {http://arxiv.org/abs/1207.0580},
  urldate = {2023-03-23},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/EEXJNRDU/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf;/Users/jrudoler/Zotero/storage/8H8ZAKTH/1207.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  urldate = {2022-11-06},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/jrudoler/Zotero/storage/XGI78LXM/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf}
}

@article{hoeffdingLargeSamplePowerTests1952,
  title = {The {{Large-Sample Power}} of {{Tests Based}} on {{Permutations}} of {{Observations}}},
  author = {Hoeffding, Wassily},
  date = {1952-06},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {2},
  pages = {169--192},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729436},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-23/issue-2/The-Large-Sample-Power-of-Tests-Based-on-Permutations-of/10.1214/aoms/1177729436.full},
  urldate = {2023-03-20},
  abstract = {The paper investigates the power of a family of nonparametric tests which includes those known as tests based on permutations of observations. Under general conditions the tests are found to be asymptotically (as the sample size tends to \$\textbackslash infty\$) as powerful as certain related standard parametric tests. The results are based on a study of the convergence in probability of certain random distribution functions. A more detailed summary will be found at the end of the Introduction.},
  file = {/Users/jrudoler/Zotero/storage/S5STTHMZ/Hoeffding - 1952 - The Large-Sample Power of Tests Based on Permutati.pdf}
}

@online{hoffBayesoptimalPredictionFrequentist2021,
  title = {Bayes-Optimal Prediction with Frequentist Coverage Control},
  author = {Hoff, Peter},
  date = {2021-05-28},
  eprint = {2105.14045},
  eprinttype = {arxiv},
  eprintclass = {math, stat},
  doi = {10.48550/arXiv.2105.14045},
  url = {http://arxiv.org/abs/2105.14045},
  urldate = {2023-12-10},
  abstract = {This article illustrates how indirect or prior information can be optimally used to construct a prediction region that maintains a target frequentist coverage rate. If the indirect information is accurate, the volume of the prediction region is lower on average than that of other regions with the same coverage rate. Even if the indirect information is inaccurate, the resulting region still maintains the target coverage rate. Such a prediction region can be constructed for models that have a complete sufficient statistic, which includes many widely-used parametric and nonparametric models. Particular examples include a Bayes-optimal conformal prediction procedure that maintains a constant coverage rate across distributions in a nonparametric model, as well as a prediction procedure for the normal linear regression model that can utilize a regularizing prior distribution, yet maintain a frequentist coverage rate that is constant as a function of the model parameters and explanatory variables. No results in this article rely on asymptotic approximations.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/9IR3IPT9/Hoff - 2021 - Bayes-optimal prediction with frequentist coverage.pdf;/Users/jrudoler/Zotero/storage/JEY84FZB/2105.html}
}

@online{hoffmanNoUTurnSamplerAdaptively2011,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  date = {2011-11-17},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1111.4246},
  url = {http://arxiv.org/abs/1111.4246},
  urldate = {2022-11-08},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/WJ5EFBQM/Hoffman and Gelman - 2011 - The No-U-Turn Sampler Adaptively Setting Path Len.pdf;/Users/jrudoler/Zotero/storage/HCZ96LGE/1111.html}
}

@online{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and family=Driessche, given=George, prefix=van den, useprefix=false and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  date = {2022-03-29},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-09-06},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/QPALSIKW/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/Users/jrudoler/Zotero/storage/E7I2TN4G/2203.html}
}

@article{holdgrafIEEGBIDSExtendingBrain2019,
  title = {{{iEEG-BIDS}}, Extending the {{Brain Imaging Data Structure}} Specification to Human Intracranial Electrophysiology},
  author = {Holdgraf, Christopher and Appelhoff, Stefan and Bickel, Stephan and Bouchard, Kristofer and D’Ambrosio, Sasha and David, Olivier and Devinsky, Orrin and Dichter, Benjamin and Flinker, Adeen and Foster, Brett L. and Gorgolewski, Krzysztof J. and Groen, Iris and Groppe, David and Gunduz, Aysegul and Hamilton, Liberty and Honey, Christopher J. and Jas, Mainak and Knight, Robert and Lachaux, Jean-Philippe and Lau, Jonathan C. and Lee-Messer, Christopher and Lundstrom, Brian N. and Miller, Kai J. and Ojemann, Jeffrey G. and Oostenveld, Robert and Petridou, Natalia and Piantoni, Gio and Pigorini, Andrea and Pouratian, Nader and Ramsey, Nick F. and Stolk, Arjen and Swann, Nicole C. and Tadel, François and Voytek, Bradley and Wandell, Brian A. and Winawer, Jonathan and Whitaker, Kirstie and Zehl, Lyuba and Hermes, Dora},
  date = {2019-06-25},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {6},
  number = {1},
  pages = {102},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0105-7},
  url = {https://www.nature.com/articles/s41597-019-0105-7},
  urldate = {2022-07-11},
  abstract = {The Brain Imaging Data Structure (BIDS) is a community-driven specification for organizing neuroscience data and metadata with the aim to make datasets more transparent, reusable, and reproducible. Intracranial electroencephalography (iEEG) data offer a unique combination of high spatial and temporal resolution measurements of the living human brain. To improve internal (re)use and external sharing of these unique data, we present a specification for storing and sharing iEEG data: iEEG-BIDS.},
  issue = {1},
  langid = {english},
  keywords = {cog-neuro,eeg},
  file = {/Users/jrudoler/Zotero/storage/ZWA5PBSE/Holdgraf et al. - 2019 - iEEG-BIDS, extending the Brain Imaging Data Struct.pdf;/Users/jrudoler/Zotero/storage/8BLU8J4J/s41597-019-0105-7.html}
}

@article{holmesNonparametricAnalysisStatistic1996,
  title = {Nonparametric Analysis of Statistic Images from Functional Mapping Experiments},
  author = {Holmes, A. P. and Blair, R. C. and Watson, J. D. and Ford, I.},
  date = {1996-01},
  journaltitle = {Journal of Cerebral Blood Flow and Metabolism: Official Journal of the International Society of Cerebral Blood Flow and Metabolism},
  shortjournal = {J Cereb Blood Flow Metab},
  volume = {16},
  number = {1},
  eprint = {8530558},
  eprinttype = {pmid},
  pages = {7--22},
  issn = {0271-678X},
  doi = {10.1097/00004647-199601000-00002},
  abstract = {The analysis of functional mapping experiments in positron emission tomography involves the formation of images displaying the values of a suitable statistic, summarising the evidence in the data for a particular effect at each voxel. These statistic images must then be scrutinised to locate regions showing statistically significant effects. The methods most commonly used are parametric, assuming a particular form of probability distribution for the voxel values in the statistic image. Scientific hypotheses, formulated in terms of parameters describing these distributions, are then tested on the basis of the assumptions. Images of statistics are usually considered as lattice representations of continuous random fields. These are more amenable to statistical analysis. There are various shortcomings associated with these methods of analysis. The many assumptions and approximations involved may not be true. The low numbers of subjects and scans, in typical experiments, lead to noisy statistic images with low degrees of freedom, which are not well approximated by continuous random fields. Thus, the methods are only approximately valid at best and are most suspect in single-subject studies. In contrast to the existing methods, we present a nonparametric approach to significance testing for statistic images from activation studies. Formal assumptions are replaced by a computationally expensive approach. In a simple rest-activation study, if there is really no activation effect, the labelling of the scans as "active" or "rest" is artificial, and a statistic image formed with some other labelling is as likely as the observed one. Thus, considering all possible relabellings, a p value can be computed for any suitable statistic describing the statistic image. Consideration of the maximal statistic leads to a simple nonparametric single-threshold test. This randomisation test relies only on minimal assumptions about the design of the experiment, is (almost) exact, with Type I error (almost) exactly that specified, and hence is always valid. The absence of distributional assumptions permits the consideration of a wide range of test statistics, for instance, "pseudo" t statistic images formed with smoothed variance images. The approach presented extends easily to other paradigms, permitting nonparametric analysis of most functional mapping experiments. When the assumptions of the parametric methods are true, these new nonparametric methods, at worst, provide for their validation. When the assumptions of the parametric methods are dubious, the nonparametric methods provide the only analysis that can be guaranteed valid and exact.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/KHHMCZ55/Holmes et al. - 1996 - Nonparametric analysis of statistic images from fu.pdf}
}

@online{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-05-23},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1801.06146},
  url = {http://arxiv.org/abs/1801.06146},
  urldate = {2023-05-01},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/56AYW8WY/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf;/Users/jrudoler/Zotero/storage/YAPFJ38X/1801.html}
}

@article{hsingRelationPermutationTestValues2003,
  title = {Relation {{Between Permutation-Test P Values}} and {{Classifier Error Estimates}}},
  author = {Hsing, Tailen and Attoor, Sanju and Dougherty, Edward},
  date = {2003-07-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {52},
  number = {1},
  pages = {11--30},
  issn = {1573-0565},
  doi = {10.1023/A:1023985022691},
  url = {https://doi.org/10.1023/A:1023985022691},
  urldate = {2022-12-21},
  abstract = {Gene-expression-based classifiers suffer from the small number of microarrays usually available for classifier design. Hence, one is confronted with the dual problem of designing a classifier and estimating its error with only a small sample. Permutation testing has been recommended to assess the dependency of a designed classifier on the specific data set. This involves randomly permuting the labels of the data points, estimating the error of the designed classifiers for each permutation, and then finding the p value of the error for the actual labeling relative to the population of errors for the random labelings. This paper addresses the issue of whether or not this p value is informative. It provides both analytic and simulation results to show that the permutation p value is, up to very small deviation, a function of the error estimate. Moreover, even though the p value is a monotonically increasing function of the error estimate, in the range of the error where the majority of the p values lie, the function is very slowly increasing, so that inversion is problematic. Hence, the conclusion is that the p value is less informative than the error estimate. This result demonstrates that random labeling does not provide any further insight into the accuracy of the classifier or the precision of the error estimate. We have no knowledge beyond the error estimate itself and the various distribution-free, classifier-specific bounds developed for this estimate.},
  langid = {english},
  keywords = {permutation,unread},
  file = {/Users/jrudoler/Zotero/storage/IZPCIHF9/Hsing et al. - 2003 - Relation Between Permutation-Test P Values and Cla.pdf}
}

@online{huangInContextConvergenceTransformers2023,
  title = {In-{{Context Convergence}} of {{Transformers}}},
  author = {Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  date = {2023-10-08},
  eprint = {2310.05249},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2310.05249},
  url = {http://arxiv.org/abs/2310.05249},
  urldate = {2023-11-01},
  abstract = {Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/R4F3F8RV/Huang et al. - 2023 - In-Context Convergence of Transformers.pdf;/Users/jrudoler/Zotero/storage/39FQP9MK/2310.html}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-09-06},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/IB8UP47P/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/jrudoler/Zotero/storage/3S5QK6X9/2106.html}
}

@article{huRobustScalableUncertainty2022,
  title = {Robust and Scalable Uncertainty Estimation with Conformal Prediction for Machine-Learned Interatomic Potentials},
  author = {Hu, Yuge and Musielewicz, Joseph and Ulissi, Zachary W. and Medford, Andrew J.},
  date = {2022-12},
  journaltitle = {Machine Learning: Science and Technology},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  volume = {3},
  number = {4},
  pages = {045028},
  publisher = {{IOP Publishing}},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/aca7b1},
  url = {https://dx.doi.org/10.1088/2632-2153/aca7b1},
  urldate = {2023-11-21},
  abstract = {Uncertainty quantification (UQ) is important to machine learning (ML) force fields to assess the level of confidence during prediction, as ML models are not inherently physical and can therefore yield catastrophically incorrect predictions. Established a-posteriori UQ methods, including ensemble methods, the dropout method, the delta method, and various heuristic distance metrics, have limitations such as being computationally challenging for large models due to model re-training. In addition, the uncertainty estimates are often not rigorously calibrated. In this work, we propose combining the distribution-free UQ method, known as conformal prediction (CP), with the distances in the neural network’s latent space to estimate the uncertainty of energies predicted by neural network force fields. We evaluate this method (CP+latent) along with other UQ methods on two essential aspects, calibration, and sharpness, and find this method to be both calibrated and sharp under the assumption of independent and identically-distributed (i.i.d.) data. We show that the method is relatively insensitive to hyperparameters selected, and test the limitations of the method when the i.i.d. assumption is violated. Finally, we demonstrate that this method can be readily applied to trained neural network force fields with traditional and graph neural network architectures to obtain estimates of uncertainty with low computational costs on a training dataset of 1 million images to showcase its scalability and portability. Incorporating the CP method with latent distances offers a calibrated, sharp and efficient strategy to estimate the uncertainty of neural network force fields. In addition, the CP approach can also function as a promising strategy for calibrating uncertainty estimated by other approaches.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/3PYQ4ANQ/Hu et al. - 2022 - Robust and scalable uncertainty estimation with co.pdf}
}

@article{huStatisticsEEGUnipolar2019,
  title = {The {{Statistics}} of {{EEG~Unipolar References}}: {{Derivations}} and {{Properties}}},
  shorttitle = {The {{Statistics}} of {{EEG~Unipolar References}}},
  author = {Hu, Shiang and Yao, Dezhong and Bringas-Vega, Maria L. and Qin, Yun and Valdes-Sosa, Pedro A.},
  date = {2019-07-01},
  journaltitle = {Brain Topography},
  shortjournal = {Brain Topogr},
  volume = {32},
  number = {4},
  pages = {696--703},
  issn = {1573-6792},
  doi = {10.1007/s10548-019-00706-y},
  url = {https://doi.org/10.1007/s10548-019-00706-y},
  urldate = {2022-08-12},
  abstract = {In this brief communication, which complements the EEG reference review (Yao et al. in Brain Topogr, 2019), we provide the mathematical derivations that show: (1) any EEG reference admits the general form of a linear transformation of the ideal multichannel EEG potentials with reference to infinity; (2) the average reference (AR), the reference electrode standardization technique (REST), and its regularized version (rREST) are solving the linear inverse problems that can be derived from both the maximum likelihood estimate (MLE) and the Bayesian theory; however, REST is based on more informative prior/constraint of volume conduction than that of AR; (3) we show for the first time that REST is also a unipolar reference (UR), allowing us to define a general family of URs with unified notations; (4) some notable properties of URs are ‘no memory’, ‘rank deficient by 1’, and ‘orthogonal projector centering’; (5) we also point out here, for the first time, that rREST provides the optimal interpolating function that can be used when the reference channel is missing or the~‘bad’ channels are rejected. The derivations and properties imply that: (a) any two URs can transform to each other and referencing with URs multiple times will not accumulate artifacts; (b) whatever URs the EEG data was previously transformed with, the minimum norm solution to the reference problem will be REST and AR with and without modeling volume conduction, respectively; (c) the MLE and the Bayesian theory show the theoretical optimality of REST. The advantages and limitations of AR and REST are discussed to guide readers for their proper use.},
  langid = {english},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/23FSEY3C/Hu et al. - 2019 - The Statistics of EEG Unipolar References Derivat.pdf}
}

@online{ivanovaLinearRegressionMapping2022,
  title = {Beyond Linear Regression: Mapping Models in Cognitive Neuroscience Should Align with Research Goals},
  shorttitle = {Beyond Linear Regression},
  author = {Ivanova, Anna A. and Schrimpf, Martin and Anzellotti, Stefano and Zaslavsky, Noga and Fedorenko, Evelina and Isik, Leyla},
  date = {2022-08-22},
  eprint = {2208.10668},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  doi = {10.48550/arXiv.2208.10668},
  url = {http://arxiv.org/abs/2208.10668},
  urldate = {2022-10-23},
  abstract = {Many cognitive neuroscience studies use large feature sets to predict and interpret brain activity patterns. Feature sets take many forms, from human stimulus annotations to representations in deep neural networks. Of crucial importance in all these studies is the mapping model, which defines the space of possible relationships between features and neural data. Until recently, most encoding and decoding studies have used linear mapping models. Increasing availability of large datasets and computing resources has recently allowed some researchers to employ more flexible nonlinear mapping models instead; however, the question of whether nonlinear mapping models can yield meaningful scientific insights remains debated. Here, we discuss the choice of a mapping model in the context of three overarching desiderata: predictive accuracy, interpretability, and biological plausibility. We show that, contrary to popular intuition, these desiderata do not map cleanly onto the linear/nonlinear divide; instead, each desideratum can refer to multiple research goals, each of which imposes its own constraints on the mapping model. Moreover, we argue that, instead of categorically treating the mapping models as linear or nonlinear, we should instead aim to estimate the complexity of these models. We show that, in many cases, complexity provides a more accurate reflection of restrictions imposed by various research goals. Finally, we outline several complexity metrics that can be used to effectively evaluate mapping models.},
  pubstate = {preprint},
  keywords = {neural-coding},
  file = {/Users/jrudoler/Zotero/storage/435ZQYCG/Ivanova et al. - 2022 - Beyond linear regression mapping models in cognit.pdf;/Users/jrudoler/Zotero/storage/LS8D7FB6/2208.html}
}

@online{jaeglePerceiverIOGeneral2022,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Hénaff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joāo},
  date = {2022-03-15},
  eprint = {2107.14795},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2107.14795},
  url = {http://arxiv.org/abs/2107.14795},
  urldate = {2023-11-02},
  abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/S8QCQMKY/Jaegle et al. - 2022 - Perceiver IO A General Architecture for Structure.pdf;/Users/jrudoler/Zotero/storage/D4TVTNJZ/2107.html}
}

@article{jamalabadiClassificationBasedHypothesis2016,
  title = {Classification Based Hypothesis Testing in Neuroscience: {{Below-chance}} Level Classification Rates and Overlooked Statistical Properties of Linear Parametric Classifiers},
  shorttitle = {Classification Based Hypothesis Testing in Neuroscience},
  author = {Jamalabadi, Hamidreza and Alizadeh, Sarah and Schönauer, Monika and Leibold, Christian and Gais, Steffen},
  date = {2016},
  journaltitle = {Human Brain Mapping},
  volume = {37},
  number = {5},
  pages = {1842--1855},
  issn = {1097-0193},
  doi = {10.1002/hbm.23140},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.23140},
  urldate = {2022-10-24},
  abstract = {Multivariate pattern analysis (MVPA) has recently become a popular tool for data analysis. Often, classification accuracy as quantified by correct classification rate (CCR) is used to illustrate the size of the effect under investigation. However, we show that in low sample size (LSS), low effect size (LES) data, which is typical in neuroscience, the distribution of CCRs from cross-validation of linear MVPA is asymmetric and can show classification rates considerably below what would be expected from chance classification. Conversely, the mode of the distribution in these cases is above expected chance levels, leading to a spuriously high number of above chance CCRs. This unexpected distribution has strong implications when using MVPA for hypothesis testing. Our analyses warrant the conclusion that CCRs do not well reflect the size of the effect under investigation. Moreover, the skewness of the null-distribution precludes the use of many standard parametric tests to assess significance of CCRs. We propose that MVPA results should be reported in terms of P values, which are estimated using randomization tests. Also, our results show that cross-validation procedures using a low number of folds, e.g. twofold, are generally more sensitive, even though the average CCRs are often considerably lower than those obtained using a higher number of folds. Hum Brain Mapp 37:1842–1855, 2016. © 2016 Wiley Periodicals, Inc.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/XE3KK27P/Jamalabadi et al. - 2016 - Classification based hypothesis testing in neurosc.pdf;/Users/jrudoler/Zotero/storage/UUV8X5SS/hbm.html}
}

@inproceedings{jiangLowResourceTextClassification2023,
  title = {“{{Low-Resource}}” {{Text Classification}}: {{A Parameter-Free Classification Method}} with {{Compressors}}},
  shorttitle = {“{{Low-Resource}}” {{Text Classification}}},
  author = {Jiang, Zhiying and Yang, Matthew and Tsirlin, Mikhail and Tang, Raphael and Dai, Yiqin and Lin, Jimmy},
  date = {2023-07},
  pages = {6810--6828},
  publisher = {{Association for Computational Linguistics}},
  location = {{Toronto, Canada}},
  url = {https://aclanthology.org/2023.findings-acl.426},
  urldate = {2023-07-14},
  abstract = {Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that's easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.},
  eventtitle = {Findings 2023},
  file = {/Users/jrudoler/Zotero/storage/8QIY3UJQ/Jiang et al. - 2023 - “Low-Resource” Text Classification A Parameter-Free Classification Method with Compressors.pdf}
}

@article{johanssonHailImpossiblePvalues2011,
  title = {Hail the Impossible: P-Values, Evidence, and Likelihood},
  shorttitle = {Hail the Impossible},
  author = {Johansson, Tobias},
  date = {2011},
  journaltitle = {Scandinavian Journal of Psychology},
  volume = {52},
  number = {2},
  pages = {113--125},
  issn = {1467-9450},
  doi = {10.1111/j.1467-9450.2010.00852.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9450.2010.00852.x},
  urldate = {2022-09-28},
  abstract = {Johansson, T. (2011). Hail the impossible: p-values, evidence, and likelihood. Scandinavian Journal of Psychology 52, 113–125. Significance testing based on p-values is standard in psychological research and teaching. Typically, research articles and textbooks present and use p as a measure of statistical evidence against the null hypothesis (the Fisherian interpretation), although using concepts and tools based on a completely different usage of p as a tool for controlling long-term decision errors (the Neyman–Pearson interpretation). There are four major problems with using p as a measure of evidence and these problems are often overlooked in the domain of psychology. First, p is uniformly distributed under the null hypothesis and can therefore never indicate evidence for the null. Second, p is conditioned solely on the null hypothesis and is therefore unsuited to quantify evidence, because evidence is always relative in the sense of being evidence for or against a hypothesis relative to another hypothesis. Third, p designates probability of obtaining evidence (given the null), rather than strength of evidence. Fourth, p depends on unobserved data and subjective intentions and therefore implies, given the evidential interpretation, that the evidential strength of observed data depends on things that did not happen and subjective intentions. In sum, using p in the Fisherian sense as a measure of statistical evidence is deeply problematic, both statistically and conceptually, while the Neyman–Pearson interpretation is not about evidence at all. In contrast, the likelihood ratio escapes the above problems and is recommended as a tool for psychologists to represent the statistical evidence conveyed by obtained data relative to two hypotheses.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/HWER4ZZM/Johansson - 2011 - Hail the impossible p-values, evidence, and likel.pdf;/Users/jrudoler/Zotero/storage/ETPLKSMT/j.1467-9450.2010.00852.html}
}

@article{johnSignificanceLevelsConfidence1983,
  title = {Significance Levels and Confidence Intervals for Permutation Tests},
  author = {John, R.D. and Robinson, J.},
  date = {1983-01-01},
  journaltitle = {Journal of Statistical Computation and Simulation},
  volume = {16},
  number = {3-4},
  pages = {161--173},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949658308810619},
  url = {https://doi.org/10.1080/00949658308810619},
  urldate = {2022-12-20},
  abstract = {A computational algorithm is given which calculates exact significance levels of a wide class of permutation tests in the one and two sample problems. This class includes the permutation test based on the means, locally most powerful permutation tests and linear rank tests. When a shift model is assumed confidence intervals can also be obtained. Approximate methods, based on asymptotic expansions, are also presented.},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/9XQTCALX/John and Robinson - 1983 - Significance levels and confidence intervals for p.pdf}
}

@article{jonasCouldNeuroscientistUnderstand2017,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  date = {2017-01-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  number = {1},
  pages = {e1005268},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005268},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268},
  urldate = {2022-06-27},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  langid = {english},
  keywords = {comp-neuro},
  file = {/Users/jrudoler/Zotero/storage/N4LB9IKQ/Jonas and Kording - 2017 - Could a Neuroscientist Understand a Microprocessor.pdf;/Users/jrudoler/Zotero/storage/YZFPQQM4/article.html}
}

@online{jungImpossibleDistillationLowQuality2023,
  title = {Impossible {{Distillation}}: From {{Low-Quality Model}} to {{High-Quality Dataset}} \& {{Model}} for {{Summarization}} and {{Paraphrasing}}},
  shorttitle = {Impossible {{Distillation}}},
  author = {Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman, Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor and Choi, Yejin},
  date = {2023-05-26},
  eprint = {2305.16635},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16635},
  url = {http://arxiv.org/abs/2305.16635},
  urldate = {2023-09-13},
  abstract = {It is commonly perceived that the strongest language models (LMs) rely on a combination of massive scale, instruction data, and human feedback to perform specialized tasks -- e.g. summarization and paraphrasing, without supervision. In this paper, we propose that language models can learn to summarize and paraphrase sentences, with none of these 3 factors. We present Impossible Distillation, a framework that distills a task-specific dataset directly from an off-the-shelf LM, even when it is impossible for the LM itself to reliably solve the task. By training a student model on the generated dataset and amplifying its capability through self-distillation, our method yields a high-quality model and dataset from a low-quality teacher model, without the need for scale or supervision. Using Impossible Distillation, we are able to distill an order of magnitude smaller model (with only 770M parameters) that outperforms 175B parameter GPT-3, in both quality and controllability, as confirmed by automatic and human evaluations. Furthermore, as a useful byproduct of our approach, we obtain DIMSUM+, a high-quality dataset with 3.4M sentence summaries and paraphrases. Our analyses show that this dataset, as a purely LM-generated corpus, is more diverse and more effective for generalization to unseen domains than all human-authored datasets -- including Gigaword with 4M samples.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/MZD7E9YA/Jung et al. - 2023 - Impossible Distillation from Low-Quality Model to.pdf;/Users/jrudoler/Zotero/storage/XESMD4LM/2305.html}
}

@online{kadavathLanguageModelsMostly2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  date = {2022-11-21},
  eprint = {2207.05221},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.05221},
  url = {http://arxiv.org/abs/2207.05221},
  urldate = {2023-09-06},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/EYDU2K69/Kadavath et al. - 2022 - Language Models (Mostly) Know What They Know.pdf;/Users/jrudoler/Zotero/storage/5BV7J7Y7/2207.html}
}

@online{kaddourCausalMachineLearning2022,
  title = {Causal {{Machine Learning}}: {{A Survey}} and {{Open Problems}}},
  shorttitle = {Causal {{Machine Learning}}},
  author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
  date = {2022-07-21},
  eprint = {2206.15475},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2206.15475},
  url = {http://arxiv.org/abs/2206.15475},
  urldate = {2023-10-19},
  abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
  pubstate = {preprint},
  keywords = {priority,unread},
  file = {/Users/jrudoler/Zotero/storage/YW39QYTD/Kaddour et al. - 2022 - Causal Machine Learning A Survey and Open Problem.pdf;/Users/jrudoler/Zotero/storage/79PF88AA/2206.html}
}

@online{kahanaPennElectrophysiologyEncoding2022,
  title = {The {{Penn Electrophysiology}} of {{Encoding}} and {{Retrieval Study}}},
  author = {Kahana, Michael J. and Lohnas, Lynn J. and Healey, Karl and Aka, Ada and Broitman, Adam and Crutchley, Elizabeth and Crutchley, Patrick and Alm, Kylie H. and Katerman, Brandon S. and Miller, Nicole E. and Kuhn, Joel R. and Li, Yuxuan and Weidemann, Nicole M. Long Jonathan Miller Madison D. Paron Jesse K. Pazdera Isaac Pedisich Christoph T. and Paron, Madison D. and Pazdera, Jesse K. and Pedisich, Isaac and Rudoler, Joseph H. and Weidemann, Christoph T.},
  date = {2022-03-23},
  doi = {10.31234/osf.io/bu5x8},
  url = {https://psyarxiv.com/bu5x8/},
  abstract = {The Penn Electrophysiology of Encoding and Retrieval Study (PEERS) aimed to characterize the behavioral and electrophysiological (EEG) correlates of memory encoding and retrieval in highly practiced individuals. Across five PEERS experiments, 300+ subjects contributed more than 7,000 90 minute memory testing sessions with recorded EEG data. Here we tell the story of PEERS: it's genesis, evolution, major findings, and the lessons it taught us about taking a big science approach to the study of memory and the human brain.},
  pubstate = {preprint}
}

@dataset{kahanaPennElectrophysiologyEncoding2023,
  title = {Penn {{Electrophysiology}} of {{Encoding}} and {{Retrieval Study}} ({{PEERS}})},
  author = {Kahana, Michael J. and Rudoler, Joseph H. and Lohnas, Lynn J. and Healey, Karl and Aka, Ada and Broitman, Adam and Crutchley, Elizabeth and Crutchley, Patrick and Alm, Kylie H. and Katerman, Brandon S. and Miller, Nicole E. and Kuhn, Joel R. and Li, Yuxuan and Long, Nicole M. and Miller, Jonathan and Paron, Madison D. and Pazdera, Jesse K. and Pedisich, Isaac and Weidemann, Christoph T.},
  date = {2023},
  publisher = {{OpenNeuro}},
  doi = {doi:10.18112/openneuro.ds004395.v2.0.0}
}

@online{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  date = {2020-01-22},
  eprint = {2001.08361},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2001.08361},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2023-09-06},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/D946CT6H/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/jrudoler/Zotero/storage/83YMQPFP/2001.html}
}

@online{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  date = {2018-12-03},
  eprint = {1711.05144},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05144},
  url = {http://arxiv.org/abs/1711.05144},
  urldate = {2023-12-14},
  abstract = {The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent "fairness gerrymandering", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/R4WKI92V/Kearns et al. - 2018 - Preventing Fairness Gerrymandering Auditing and L.pdf;/Users/jrudoler/Zotero/storage/K6SGQ5PX/1711.html}
}

@article{kimMinimaxOptimalityPermutation2022,
  title = {Minimax Optimality of Permutation Tests},
  author = {Kim, Ilmun and Balakrishnan, Sivaraman and Wasserman, Larry},
  date = {2022-02},
  journaltitle = {The Annals of Statistics},
  volume = {50},
  number = {1},
  pages = {225--251},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/21-AOS2103},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-1/Minimax-optimality-of-permutation-tests/10.1214/21-AOS2103.full},
  urldate = {2023-03-20},
  abstract = {Permutation tests are widely used in statistics, providing a finite-sample guarantee on the type I error rate whenever the distribution of the samples under the null hypothesis is invariant to some rearrangement. Despite its increasing popularity and empirical success, theoretical properties of the permutation test, especially its power, have not been fully explored beyond simple cases. In this paper, we attempt to partly fill this gap by presenting a general nonasymptotic framework for analyzing the minimax power of the permutation test. The utility of our proposed framework is illustrated in the context of two-sample and independence testing under both discrete and continuous settings. In each setting, we introduce permutation tests based on U-statistics and study their minimax performance. We also develop exponential concentration bounds for permuted U-statistics based on a novel coupling idea, which may be of independent interest. Building on these exponential bounds, we introduce permutation tests, which are adaptive to unknown smoothness parameters without losing much power. The proposed framework is further illustrated using more sophisticated test statistics including weighted U-statistics for multinomial testing and Gaussian kernel-based statistics for density testing. Finally, we provide some simulation results that further justify the permutation approach.},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/GZT8E733/Kim et al. - 2022 - Minimax optimality of permutation tests.pdf}
}

@online{kirchenbauerWatermarkLargeLanguage2023,
  title = {A {{Watermark}} for {{Large Language Models}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  date = {2023-06-06},
  eprint = {2301.10226},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.10226},
  url = {http://arxiv.org/abs/2301.10226},
  urldate = {2023-09-06},
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/INXDLF7H/Kirchenbauer et al. - 2023 - A Watermark for Large Language Models.pdf;/Users/jrudoler/Zotero/storage/G9TQ3AQE/2301.html}
}

@online{kirschMetaLearningBackpropagation2022,
  title = {Meta {{Learning Backpropagation And Improving It}}},
  author = {Kirsch, Louis and Schmidhuber, Jürgen},
  date = {2022-03-13},
  eprint = {2012.14905},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2012.14905},
  url = {http://arxiv.org/abs/2012.14905},
  urldate = {2022-10-25},
  abstract = {Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to reprogram fast weights, Hebbian plasticity, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VSML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VSML where the weights of a neural network are replaced by tiny LSTMs allows for implementing the backpropagation LA solely by running in forward-mode. It can even meta learn new LAs that differ from online backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned LAs learn through fast association in a way that is qualitatively different from gradient descent.},
  pubstate = {preprint},
  keywords = {AI,machine-learning},
  file = {/Users/jrudoler/Zotero/storage/38DA2ZRD/Kirsch and Schmidhuber - 2022 - Meta Learning Backpropagation And Improving It.pdf;/Users/jrudoler/Zotero/storage/KM5PNPHU/2012.html}
}

@article{koenkerRegressionQuantiles1978,
  title = {Regression {{Quantiles}}},
  author = {Koenker, Roger and Bassett, Gilbert},
  date = {1978},
  journaltitle = {Econometrica},
  volume = {46},
  number = {1},
  eprint = {1913643},
  eprinttype = {jstor},
  pages = {33--50},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1913643},
  url = {https://www.jstor.org/stable/1913643},
  urldate = {2023-11-23},
  abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/CBBHRLAB/Koenker and Bassett - 1978 - Regression Quantiles.pdf}
}

@online{kornblithSimilarityNeuralNetwork2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  date = {2019-07-19},
  eprint = {1905.00414},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  doi = {10.48550/arXiv.1905.00414},
  url = {http://arxiv.org/abs/1905.00414},
  urldate = {2022-11-21},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  pubstate = {preprint},
  keywords = {machine-learning,similarity,unread},
  file = {/Users/jrudoler/Zotero/storage/D9HXTRP2/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf;/Users/jrudoler/Zotero/storage/SIWVGKBE/1905.html}
}

@article{kriegeskorteNeuralTuningRepresentational2021,
  title = {Neural Tuning and Representational Geometry},
  author = {Kriegeskorte, Nikolaus and Wei, Xue-Xin},
  date = {2021-11},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {22},
  number = {11},
  pages = {703--718},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-021-00502-3},
  url = {https://www.nature.com/articles/s41583-021-00502-3},
  urldate = {2023-03-21},
  abstract = {A central goal of neuroscience is to understand the representations formed by brain activity patterns and their connection to behaviour. The classic approach is to investigate how individual neurons encode stimuli and how their tuning determines the fidelity of the neural representation. Tuning analyses often use the Fisher information to characterize the sensitivity of neural responses to small changes of the stimulus. In recent decades, measurements of large populations of neurons have motivated a complementary approach, which focuses on the information available to linear decoders. The decodable information is captured by the geometry of the representational patterns in the multivariate response space. Here we review neural tuning and representational geometry with the goal of clarifying the relationship between them. The tuning induces the geometry, but different sets of tuned neurons can induce the same geometry. The geometry determines the Fisher information, the mutual information and the behavioural performance of an ideal observer in a range of psychophysical tasks. We argue that future studies can benefit from considering both tuning and geometry to understand neural codes and reveal the connections between stimuli, brain activity and behaviour.},
  issue = {11},
  langid = {english},
  keywords = {neural-coding},
  file = {/Users/jrudoler/Zotero/storage/JDISTMIN/Kriegeskorte and Wei - 2021 - Neural tuning and representational geometry.pdf}
}

@article{kriegeskortePeelingOnionBrain2019,
  title = {Peeling the {{Onion}} of {{Brain Representations}}},
  author = {Kriegeskorte, Nikolaus and Diedrichsen, Jörn},
  date = {2019-07-08},
  journaltitle = {Annual Review of Neuroscience},
  shortjournal = {Annu. Rev. Neurosci.},
  volume = {42},
  number = {1},
  pages = {407--432},
  publisher = {{Annual Reviews}},
  issn = {0147-006X},
  doi = {10.1146/annurev-neuro-080317-061906},
  url = {https://doi.org/10.1146/annurev-neuro-080317-061906},
  urldate = {2023-02-07},
  abstract = {The brain's function is to enable adaptive behavior in the world. To this end, the brain processes information about the world. The concept of representation links the information processed by the brain back to the world and enables us to understand what the brain does at a functional level. The appeal of making the connection between brain activity and what it represents has been irresistible to neuroscience, despite the fact that representational interpretations pose several challenges: We must define which aspects of brain activity matter, how the code works, and how it supports computations that contribute to adaptive behavior. It has been suggested that we might drop representational language altogether and seek to understand the brain, more simply, as a dynamical system. In this review, we argue that the concept of representation provides a useful link between dynamics and computational function and ask which aspects of brain activity should be analyzed to achieve a representational understanding. We peel the onion of brain representations in search of the layers (the aspects of brain activity) that matter to computation. The article provides an introduction to the motivation and mathematics of representational models, a critical discussion of their assumptions and limitations, and a preview of future directions in this area.},
  keywords = {neural-coding},
  file = {/Users/jrudoler/Zotero/storage/ZIB77EPA/Kriegeskorte and Diedrichsen - 2019 - Peeling the Onion of Brain Representations.pdf}
}

@article{kriegeskorteRelatingPopulationcodeRepresentations2009,
  title = {Relating Population-Code Representations between Man, Monkey, and Computational Models},
  author = {Kriegeskorte, Nikolaus},
  date = {2009},
  journaltitle = {Frontiers in Neuroscience},
  volume = {3},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/articles/10.3389/neuro.01.035.2009},
  urldate = {2022-12-05},
  keywords = {neural-coding,unread},
  file = {/Users/jrudoler/Zotero/storage/5X3PLC6B/Kriegeskorte - 2009 - Relating population-code representations between m.pdf}
}

@online{laiModelingLongShortTerm2018,
  title = {Modeling {{Long-}} and {{Short-Term Temporal Patterns}} with {{Deep Neural Networks}}},
  author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  date = {2018-04-18},
  eprint = {1703.07015},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.07015},
  url = {http://arxiv.org/abs/1703.07015},
  urldate = {2023-05-01},
  abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/8ZVSVYI3/Lai et al. - 2018 - Modeling Long- and Short-Term Temporal Patterns wi.pdf;/Users/jrudoler/Zotero/storage/3P7HLQY3/1703.html}
}

@article{lakeHumanlevelConceptLearning2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  date = {2015-12-11},
  journaltitle = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aab3050},
  url = {https://www.science.org/doi/full/10.1126/science.aab3050},
  urldate = {2022-06-30},
  file = {/Users/jrudoler/Zotero/storage/HCYMFT9I/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf}
}

@article{lakeHumanlikeSystematicGeneralization2023,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  date = {2023-10-25},
  journaltitle = {Nature},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06668-3},
  url = {https://www.nature.com/articles/s41586-023-06668-3},
  urldate = {2023-10-25},
  abstract = {The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  langid = {english},
  keywords = {priority},
  file = {/Users/jrudoler/Zotero/storage/XIM6HY6K/Lake and Baroni - 2023 - Human-like systematic generalization through a met.pdf}
}

@inproceedings{lakshminarayananSimpleScalablePredictive2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html},
  urldate = {2023-11-22},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem.  Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian)  NNs.  We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  keywords = {Ensemble learning to estimate uncertainty},
  file = {/Users/jrudoler/Zotero/storage/GADZ4FVI/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf}
}

@online{langeNeuralNetworksPaths2022,
  title = {Neural {{Networks}} as {{Paths}} through the {{Space}} of {{Representations}}},
  author = {Lange, Richard D. and Kwok, Devin and Matelsky, Jordan and Wang, Xinyue and Rolnick, David S. and Kording, Konrad P.},
  date = {2022-11-27},
  eprint = {2206.10999},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.10999},
  url = {http://arxiv.org/abs/2206.10999},
  urldate = {2022-12-05},
  abstract = {Deep neural networks implement a sequence of layer-by-layer operations that are each relatively easy to understand, but the resulting overall computation is generally difficult to understand. We consider a simple hypothesis for interpreting the layer-by-layer construction of useful representations: perhaps the role of each layer is to reformat information to reduce the "distance" to the desired outputs. With this framework, the layer-wise computation implemented by a deep neural network can be viewed as a path through a high-dimensional representation space. We formalize this intuitive idea of a "path" by leveraging recent advances in *metric* representational similarity. We extend existing representational distance methods by computing geodesics, angles, and projections of representations, going beyond mere layer distances. We then demonstrate these tools by visualizing and comparing the paths taken by ResNet and VGG architectures on CIFAR-10. We conclude by sketching additional ways that this kind of representational geometry can be used to understand and interpret network training, and to describe novel kinds of similarities between different models.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/I7KB3LH6/Lange et al. - 2022 - Neural Networks as Paths through the Space of Repr.pdf;/Users/jrudoler/Zotero/storage/U6DWB7MT/2206.html}
}

@online{lecomteIncidentalPolysemanticity2023,
  title = {Incidental {{Polysemanticity}}},
  author = {Lecomte, Victor and Thaman, Kushal and Chow, Trevor and Schaeffer, Rylan and Koyejo, Sanmi},
  date = {2023-12-05},
  eprint = {2312.03096},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03096},
  url = {http://arxiv.org/abs/2312.03096},
  urldate = {2023-12-07},
  abstract = {Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more "features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this \textbackslash textit\{incidental polysemanticity\}.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/PL6RK5VG/Lecomte et al. - 2023 - Incidental Polysemanticity.pdf;/Users/jrudoler/Zotero/storage/G8BY79TI/2312.html}
}

@inproceedings{leeDemystifyingDisagreementontheLineHigh2023,
  title = {Demystifying {{Disagreement-on-the-Line}} in {{High Dimensions}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Lee, Donghwan and Moniri, Behrad and Huang, Xinmeng and Dobriban, Edgar and Hassani, Hamed},
  date = {2023-07-03},
  pages = {19053--19093},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v202/lee23o.html},
  urldate = {2023-11-01},
  abstract = {Evaluating the performance of machine learning models under distribution shifts is challenging, especially when we only have unlabeled data from the shifted (target) domain, along with labeled data from the original (source) domain. Recent work suggests that the notion of disagreement, the degree to which two models trained with different randomness differ on the same input, is a key to tackling this problem. Experimentally, disagreement and prediction error have been shown to be strongly connected, which has been used to estimate model performance. Experiments have led to the discovery of the disagreement-on-the-line phenomenon, whereby the classification error under the target domain is often a linear function of the classification error under the source domain; and whenever this property holds, disagreement under the source and target domain follow the same linear relation. In this work, we develop a theoretical foundation for analyzing disagreement in high-dimensional random features regression; and study under what conditions the disagreement-on-the-line phenomenon occurs in our setting. Experiments on CIFAR-10-C, Tiny ImageNet-C, and Camelyon17 are consistent with our theory and support the universality of the theoretical findings.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/9RVHX3IC/Lee et al. - 2023 - Demystifying Disagreement-on-the-Line in High Dimensions.pdf}
}

@article{leeFastConstructionInterpretable2022,
  title = {Fast Construction of Interpretable Whole-Brain Decoders},
  author = {Lee, Sangil and Bradlow, Eric T. and Kable, Joseph W.},
  date = {2022-06-20},
  journaltitle = {Cell Reports Methods},
  shortjournal = {Cell Reports Methods},
  volume = {2},
  number = {6},
  pages = {100227},
  issn = {2667-2375},
  doi = {10.1016/j.crmeth.2022.100227},
  url = {https://www.sciencedirect.com/science/article/pii/S266723752200090X},
  urldate = {2022-10-23},
  abstract = {Researchers often seek to decode mental states from brain activity measured with functional MRI. Rigorous decoding requires the use of formal neural prediction models, which are likely to be the most accurate if they use the whole brain. However, the computational burden and lack of interpretability of off-the-shelf statistical methods can make whole-brain decoding challenging. Here, we propose a method to build whole-brain neural decoders that are both interpretable and computationally efficient. We extend the partial least squares algorithm to build a regularized model with variable selection that offers a unique “fit once, tune later” approach: users need to fit the model only once and can choose the best tuning parameters post hoc. We show in real data that our method scales well with increasing data size and yields interpretable predictors. The algorithm is publicly available in multiple languages in the hope that interpretable whole-brain predictors can be implemented more widely in neuroimaging research.},
  langid = {english},
  keywords = {fMRI,neural-coding},
  file = {/Users/jrudoler/Zotero/storage/G3WKMDZZ/Lee et al. - 2022 - Fast construction of interpretable whole-brain dec.pdf;/Users/jrudoler/Zotero/storage/4HH3GG6B/S266723752200090X.html}
}

@online{leeTCalOptimalTest2022,
  title = {T-{{Cal}}: {{An}} Optimal Test for the Calibration of Predictive Models},
  shorttitle = {T-{{Cal}}},
  author = {Lee, Donghwan and Huang, Xinmeng and Hassani, Hamed and Dobriban, Edgar},
  date = {2022-03-25},
  eprint = {2203.01850},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2203.01850},
  url = {http://arxiv.org/abs/2203.01850},
  urldate = {2023-03-02},
  abstract = {The prediction accuracy of machine learning methods is steadily increasing, but the calibration of their uncertainty predictions poses a significant challenge. Numerous works focus on obtaining well-calibrated predictive models, but less is known about reliably assessing model calibration. This limits our ability to know when algorithms for improving calibration have a real effect, and when their improvements are merely artifacts due to random noise in finite datasets. In this work, we consider detecting mis-calibration of predictive models using a finite validation dataset as a hypothesis testing problem. The null hypothesis is that the predictive model is calibrated, while the alternative hypothesis is that the deviation from calibration is sufficiently large. We find that detecting mis-calibration is only possible when the conditional probabilities of the classes are sufficiently smooth functions of the predictions. When the conditional class probabilities are H\textbackslash "older continuous, we propose T-Cal, a minimax optimal test for calibration based on a debiased plug-in estimator of the \$\textbackslash ell\_2\$-Expected Calibration Error (ECE). We further propose Adaptive T-Cal, a version that is adaptive to unknown smoothness. We verify our theoretical findings with a broad range of experiments, including with several popular deep neural net architectures and several standard post-hoc calibration methods. T-Cal is a practical general-purpose tool, which -- combined with classical tests for discrete-valued predictors -- can be used to test the calibration of virtually any probabilistic classification method.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/SNPMPDGZ/Lee et al. - 2022 - T-Cal An optimal test for the calibration of pred.pdf;/Users/jrudoler/Zotero/storage/WNV7BIVS/2203.html}
}

@article{leiDistributionFreePrediction2013,
  title = {Distribution {{Free Prediction Sets}}},
  author = {Lei, Jing and Robins, James and Wasserman, Larry},
  date = {2013},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J Am Stat Assoc},
  volume = {108},
  number = {501},
  eprint = {25237208},
  eprinttype = {pmid},
  pages = {278--287},
  issn = {0162-1459},
  doi = {10.1080/01621459.2012.751873},
  abstract = {This paper introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. We start from the general conformal prediction approach and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cut-off values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example.},
  langid = {english},
  pmcid = {PMC4164906},
  keywords = {conformal,priority},
  file = {/Users/jrudoler/Zotero/storage/8G6TW38Y/Lei et al. - 2013 - Distribution-Free Prediction Sets.pdf;/Users/jrudoler/Zotero/storage/LY9D7KP7/Lei et al. - 2013 - Distribution Free Prediction Sets.pdf}
}

@article{leiDistributionFreePredictiveInference2018,
  title = {Distribution-{{Free Predictive Inference}} for {{Regression}}},
  author = {Lei, Jing and G’Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan J. and Wasserman, Larry},
  date = {2018-07-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {113},
  number = {523},
  pages = {1094--1111},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1307116},
  url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1307116},
  urldate = {2023-12-10},
  abstract = {We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, in order to adapt to heteroskedascity in the data. Finally, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying this paper is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/Y97W3NSB/Lei et al. - 2018 - Distribution-Free Predictive Inference for Regress.pdf}
}

@online{leviathanFastInferenceTransformers2023,
  title = {Fast {{Inference}} from {{Transformers}} via {{Speculative Decoding}}},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  date = {2023-05-18},
  eprint = {2211.17192},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.17192},
  url = {http://arxiv.org/abs/2211.17192},
  urldate = {2023-10-30},
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/I3R5VBXL/Leviathan et al. - 2023 - Fast Inference from Transformers via Speculative Decoding.pdf}
}

@online{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019-10-29},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.13461},
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2023-12-12},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/RJWE4IYJ/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf;/Users/jrudoler/Zotero/storage/4BMH38HW/1910.html}
}

@online{liExperimentsRichRegime2021,
  title = {Experiments with {{Rich Regime Training}} for {{Deep Learning}}},
  author = {Li, Xinyan and Banerjee, Arindam},
  date = {2021-02-26},
  eprint = {2102.13522},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2102.13522},
  urldate = {2023-03-30},
  abstract = {In spite of advances in understanding lazy training, recent work attributes the practical success of deep learning to the rich regime with complex inductive bias. In this paper, we study rich regime training empirically with benchmark datasets, and find that while most parameters are lazy, there is always a small number of active parameters which change quite a bit during training. We show that re-initializing (resetting to their initial random values) the active parameters leads to worse generalization. Further, we show that most of the active parameters are in the bottom layers, close to the input, especially as the networks become wider. Based on such observations, we study static Layer-Wise Sparse (LWS) SGD, which only updates some subsets of layers. We find that only updating the top and bottom layers have good generalization and, as expected, only updating the top layers yields a fast algorithm. Inspired by this, we investigate probabilistic LWS-SGD, which mostly updates the top layers and occasionally updates the full network. We show that probabilistic LWS-SGD matches the generalization performance of vanilla SGD and the back-propagation time can be 2-5 times more efficient.},
  pubstate = {preprint},
  keywords = {machine-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/UUEWCMK5/Li and Banerjee - 2021 - Experiments with Rich Regime Training for Deep Lea.pdf;/Users/jrudoler/Zotero/storage/DPZY94TT/2102.html}
}

@online{liFasterNonAsymptoticConvergence2023,
  title = {Towards {{Faster Non-Asymptotic Convergence}} for {{Diffusion-Based Generative Models}}},
  author = {Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  date = {2023-10-01},
  eprint = {2306.09251},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2306.09251},
  url = {http://arxiv.org/abs/2306.09251},
  urldate = {2023-10-20},
  abstract = {Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to \$\textbackslash ell\_2\$-accurate estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to \$1/T\$ (with \$T\$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model), we derive a convergence rate proportional to \$1/\textbackslash sqrt\{T\}\$, matching the state-of-the-art theory. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how \$\textbackslash ell\_2\$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to \$1/T\^2\$ for the ODE-based sampler and \$1/T\$ for the DDPM-type sampler, which might be of independent theoretical and empirical interest.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/KWLV4638/Li et al. - 2023 - Towards Faster Non-Asymptotic Convergence for Diff.pdf;/Users/jrudoler/Zotero/storage/YI6KR8PC/2306.html}
}

@online{liLargeLanguageModels2022,
  title = {Large {{Language Models Can Be Strong Differentially Private Learners}}},
  author = {Li, Xuechen and Tramèr, Florian and Liang, Percy and Hashimoto, Tatsunori},
  date = {2022-11-10},
  eprint = {2110.05679},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.05679},
  url = {http://arxiv.org/abs/2110.05679},
  urldate = {2023-09-06},
  abstract = {Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/3NDH5ANF/Li et al. - 2022 - Large Language Models Can Be Strong Differentially.pdf;/Users/jrudoler/Zotero/storage/E9K46ZHQ/2110.html}
}

@article{lindsayAttentionPsychologyNeuroscience2020,
  title = {Attention in {{Psychology}}, {{Neuroscience}}, and {{Machine Learning}}},
  author = {Lindsay, Grace W.},
  date = {2020},
  journaltitle = {Frontiers in Computational Neuroscience},
  volume = {14},
  issn = {1662-5188},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2020.00029},
  urldate = {2022-08-24},
  abstract = {Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored.},
  file = {/Users/jrudoler/Zotero/storage/IS7FB2EE/Lindsay - 2020 - Attention in Psychology, Neuroscience, and Machine.pdf}
}

@online{lindsayDivergentRepresentationsEthological2022,
  title = {Divergent Representations of Ethological Visual Inputs Emerge from Supervised, Unsupervised, and Reinforcement Learning},
  author = {Lindsay, Grace W. and Merel, Josh and Mrsic-Flogel, Tom and Sahani, Maneesh},
  date = {2022-02-08},
  eprint = {2112.02027},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2112.02027},
  url = {http://arxiv.org/abs/2112.02027},
  urldate = {2022-11-21},
  abstract = {Artificial neural systems trained using reinforcement, supervised, and unsupervised learning all acquire internal representations of high dimensional input. To what extent these representations depend on the different learning objectives is largely unknown. Here we compare the representations learned by eight different convolutional neural networks, each with identical ResNet architectures and trained on the same family of egocentric images, but embedded within different learning systems. Specifically, the representations are trained to guide action in a compound reinforcement learning task; to predict one or a combination of three task-related targets with supervision; or using one of three different unsupervised objectives. Using representational similarity analysis, we find that the network trained with reinforcement learning differs most from the other networks. Using metrics inspired by the neuroscience literature, we find that the model trained with reinforcement learning has a sparse and high-dimensional representation wherein individual images are represented with very different patterns of neural activity. Further analysis suggests these representations may arise in order to guide long-term behavior and goal-seeking in the RL agent. Finally, we compare the representations learned by the RL agent to neural activity from mouse visual cortex and find it to perform as well or better than other models. Our results provide insights into how the properties of neural representations are influenced by objective functions and can inform transfer learning approaches.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/WLR889VF/Lindsay et al. - 2022 - Divergent representations of ethological visual in.pdf;/Users/jrudoler/Zotero/storage/UL3397ID/2112.html}
}

@article{lindsayFeaturebasedAttentionConvolutional2015,
  title = {Feature-Based {{Attention}} in {{Convolutional Neural Networks}}},
  author = {Lindsay, Grace},
  date = {2015-11-19},
  abstract = {Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance.},
  file = {/Users/jrudoler/Zotero/storage/6A8HI7M3/Lindsay - 2015 - Feature-based Attention in Convolutional Neural Networks.pdf}
}

@unpublished{lindsayTestingToolsSystems2022,
  title = {Testing the {{Tools}} of {{Systems Neuroscience}} on {{Artificial Neural Networks}}},
  author = {Lindsay, Grace W.},
  date = {2022-02-14},
  eprint = {2202.07035},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2202.07035},
  urldate = {2022-08-22},
  abstract = {Neuroscientists apply a range of common analysis tools to recorded neural activity in order to glean insights into how neural circuits implement computations. Despite the fact that these tools shape the progress of the field as a whole, we have little empirical evidence that they are effective at quickly identifying the phenomena of interest. Here I argue that these tools should be explicitly tested and that artificial neural networks (ANNs) are an appropriate testing grounds for them. The recent resurgence of the use of ANNs as models of everything from perception to memory to motor control stems from a rough similarity between artificial and biological neural networks and the ability to train these networks to perform complex high-dimensional tasks. These properties, combined with the ability to perfectly observe and manipulate these systems, makes them well-suited for vetting the tools of systems and cognitive neuroscience. I provide here both a roadmap for performing this testing and a list of tools that are suitable to be tested on ANNs. Using ANNs to reflect on the extent to which these tools provide a productive understanding of neural systems -- and on exactly what understanding should mean here -- has the potential to expedite progress in the study of the brain.},
  file = {/Users/jrudoler/Zotero/storage/FDY8J2HG/Lindsay - 2022 - Testing the Tools of Systems Neuroscience on Artificial Neural Networks.pdf}
}

@online{liptonDoctorJustWon2017,
  title = {The {{Doctor Just Won}}'t {{Accept That}}!},
  author = {Lipton, Zachary C.},
  date = {2017-11-24},
  eprint = {1711.08037},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1711.08037},
  url = {http://arxiv.org/abs/1711.08037},
  urldate = {2023-09-29},
  abstract = {Calls to arms to build interpretable models express a well-founded discomfort with machine learning. Should a software agent that does not even know what a loan is decide who qualifies for one? Indeed, we ought to be cautious about injecting machine learning (or anything else, for that matter) into applications where there may be a significant risk of causing social harm. However, claims that stakeholders "just won't accept that!" do not provide a sufficient foundation for a proposed field of study. For the field of interpretable machine learning to advance, we must ask the following questions: What precisely won't various stakeholders accept? What do they want? Are these desiderata reasonable? Are they feasible? In order to answer these questions, we'll have to give real-world problems and their respective stakeholders greater consideration.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/GFDZZXTF/Lipton - 2017 - The Doctor Just Won't Accept That!.pdf;/Users/jrudoler/Zotero/storage/VPZ7YJ8I/1711.html}
}

@online{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  date = {2023-08-14},
  eprint = {2308.06259},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.06259},
  url = {http://arxiv.org/abs/2308.06259},
  urldate = {2023-09-06},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/TU7388SQ/Li et al. - 2023 - Self-Alignment with Instruction Backtranslation.pdf;/Users/jrudoler/Zotero/storage/GLSL56QH/2308.html}
}

@inproceedings{liuDatadrivenExclusionCriteria2022,
  title = {Data-Driven Exclusion Criteria for Instrumental Variable Studies},
  booktitle = {Proceedings of the {{First Conference}} on {{Causal Learning}} and {{Reasoning}}},
  author = {Liu, Tony and Lawlor, Patrick and Ungar, Lyle and Kording, Konrad},
  date = {2022-06-28},
  pages = {485--508},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v177/liu22a.html},
  urldate = {2022-07-14},
  abstract = {When using instrumental variables for causal inference, it is common practice to apply specific exclusion criteria to the data prior to estimation. This exclusion, critical for study design, is often done in an ad hoc manner, informed by a priori hypotheses and domain knowledge. In this study, we frame exclusion as a data-driven estimation problem, and apply flexible machine learning methods to estimate the probability of a unit complying with the instrument. We demonstrate how excluding likely noncompliers can increase power while maintaining valid treatment effect estimates. We show the utility of our approach with a fuzzy regression discontinuity analysis of the effect of initial diabetes diagnosis on follow-up blood sugar levels. Data-driven exclusion criterion can help improve both power and external validity for various quasi-experimental settings.},
  eventtitle = {Conference on {{Causal Learning}} and {{Reasoning}}},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/WBU4TLBH/Liu et al. - 2022 - Data-driven exclusion criteria for instrumental va.pdf}
}

@online{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  date = {2023-07-31},
  eprint = {2307.03172},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.03172},
  urldate = {2023-09-06},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/867IBLZV/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long C.pdf;/Users/jrudoler/Zotero/storage/UCM4VRGW/2307.html}
}

@article{liuQuantifyingCausalityData2021,
  title = {Quantifying Causality in Data Science with Quasi-Experiments},
  author = {Liu, Tony and Ungar, Lyle and Kording, Konrad},
  date = {2021-01-01},
  journaltitle = {Nature Computational Science},
  shortjournal = {Nature Computational Science},
  volume = {1},
  number = {1},
  pages = {24--32},
  issn = {2662-8457},
  doi = {10.1038/s43588-020-00005-8},
  url = {https://doi.org/10.1038/s43588-020-00005-8},
  abstract = {Estimating causality from observational data is essential in many data science questions but can be a challenging task. Here we review approaches to causality that are popular in econometrics and that exploit (quasi) random variation in existing data, called quasi-experiments, and show how they can be combined with machine learning to answer causal questions within typical data science settings. We also highlight how data scientists can help advance these methods to bring causal estimation to high-dimensional data from medicine, industry and society.},
  keywords = {causality},
  file = {/Users/jrudoler/Zotero/storage/MX4B765U/Liu et al. - 2021 - Quantifying causality in data science with quasi-e.pdf}
}

@online{liuSeeingForestTree2022,
  title = {Seeing the Forest and the Tree: {{Building}} Representations of Both Individual and Collective Dynamics with Transformers},
  shorttitle = {Seeing the Forest and the Tree},
  author = {Liu, Ran and Azabou, Mehdi and Dabagia, Max and Xiao, Jingyun and Dyer, Eva L.},
  date = {2022-10-20},
  eprint = {2206.06131},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2206.06131},
  urldate = {2022-11-03},
  abstract = {Complex time-varying systems are often studied by abstracting away from the dynamics of individual components to build a model of the population-level dynamics from the start. However, when building a population-level description, it can be easy to lose sight of each individual and how they contribute to the larger picture. In this paper, we present a novel transformer architecture for learning from time-varying data that builds descriptions of both the individual as well as the collective population dynamics. Rather than combining all of our data into our model at the onset, we develop a separable architecture that operates on individual time-series first before passing them forward; this induces a permutation-invariance property and can be used to transfer across systems of different size and order. After demonstrating that our model can be applied to successfully recover complex interactions and dynamics in many-body systems, we apply our approach to populations of neurons in the nervous system. On neural activity datasets, we show that our model not only yields robust decoding performance, but also provides impressive performance in transfer across recordings of different animals without any neuron-level correspondence. By enabling flexible pre-training that can be transferred to neural recordings of different size and order, our work provides a first step towards creating a foundation model for neural decoding.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/CG2CZ7MN/Liu et al. - 2022 - Seeing the forest and the tree Building represent.pdf;/Users/jrudoler/Zotero/storage/Y3T2UB93/2206.html}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2023-04-30},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/DLYEY9FJ/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/jrudoler/Zotero/storage/HFLQLTB5/1711.html}
}

@online{lotfiBayesianModelSelection2022,
  title = {Bayesian {{Model Selection}}, the {{Marginal Likelihood}}, and {{Generalization}}},
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  date = {2022-06-02},
  eprint = {2202.11678},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2202.11678},
  url = {http://arxiv.org/abs/2202.11678},
  urldate = {2022-11-27},
  abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
  pubstate = {preprint},
  keywords = {bayes},
  file = {/Users/jrudoler/Zotero/storage/S3EVRVQ6/Lotfi et al. - 2022 - Bayesian Model Selection, the Marginal Likelihood,.pdf;/Users/jrudoler/Zotero/storage/IRE8CABW/2202.html}
}

@thesis{mackayBayesianMethodsAdaptive1992,
  type = {phdthesis},
  title = {Bayesian Methods for Adaptive Models},
  author = {MacKay, David J. C.},
  date = {1992},
  institution = {{California Institute of Technology}},
  doi = {10.7907/H3A1-WM07},
  url = {https://resolver.caltech.edu/CaltechETD:etd-01042007-131447},
  urldate = {2023-12-13},
  abstract = {The Bayesian framework for model comparison and regularisation is demonstrated by studying interpolation and classification problems modelled with both linear and non-linear models. This framework quantitatively embodies 'Occam's razor'. Over-complex and under-regularised models are automatically inferred to be less probable, even though their flexibility allows them to fit the data better. When applied to 'neural networks', the Bayesian framework makes possible (1) objective comparison of solutions using alternative network architectures; (2) objective stopping rules for network pruning or growing procedures; (3) objective choice of type of weight decay terms (or regularisers); (4) on-line techniques for optimising weight decay (or regularisation constant) magnitude; (5) a measure of the effective number of well-determined parameters in a model; (6) quantified estimates of the error bars on network parameters and on network output. In the case of classification models, it is shown that the careful incorporation of error bar information into a classifier's predictions yields improved performance. Comparisons of the inferences of the Bayesian Framework with more traditional cross-validation methods help detect poor underlying assumptions in learning models. The relationship of the Bayesian learning framework to 'active learning' is examined. Objective functions are discussed which measure the expected informativeness data measurements, in the context of both interpolation and classification problems. The concepts and methods described in this thesis are quite general and will be applicable to other data modelling problems whether they involve regression, classification or density estimation.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/J8LWTZLN/MacKay - 1992 - Bayesian methods for adaptive models.pdf;/Users/jrudoler/Zotero/storage/AT32UKZC/25.html}
}

@article{mackayProbableNetworksPlausible1995a,
  title = {Probable Networks and Plausible Predictions — a Review of Practical {{Bayesian}} Methods for Supervised Neural Networks},
  author = {Mackay, David J C},
  date = {1995-01},
  journaltitle = {Network: Computation in Neural Systems},
  shortjournal = {Network: Computation in Neural Systems},
  volume = {6},
  number = {3},
  pages = {469--505},
  issn = {0954-898X, 1361-6536},
  doi = {10.1088/0954-898X_6_3_011},
  url = {https://www.tandfonline.com/doi/full/10.1088/0954-898X_6_3_011},
  urldate = {2023-12-13},
  abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
  langid = {english},
  keywords = {unread}
}

@article{madoreReadinessRememberPredicting2022,
  title = {Readiness to Remember: Predicting Variability in Episodic Memory},
  shorttitle = {Readiness to Remember},
  author = {Madore, Kevin P. and Wagner, Anthony D.},
  date = {2022-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {8},
  pages = {707--723},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2022.05.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661322001127},
  urldate = {2023-01-04},
  abstract = {Learning and remembering are fundamental to our lives, so what causes us to forget? Answers often highlight preparatory processes that precede learning, as well as mnemonic processes during the act of encoding or retrieval. Importantly, evidence now indicates that preparatory processes that precede retrieval attempts also have powerful influences on memory success or failure. Here, we review recent work from neuroimaging, electroencephalography, pupillometry, and behavioral science to propose an integrative framework of retrieval-period dynamics that explains variance in remembering in the moment and across individuals as a function of interactions among preparatory attention, goal coding, and mnemonic processes. Extending this approach, we consider how a ‘readiness to remember’ (R2R) framework explains variance in high-level functions of memory and mnemonic disruptions in aging.},
  langid = {english},
  keywords = {eeg,unread},
  file = {/Users/jrudoler/Zotero/storage/XWY5H4CX/S1364661322001127.html}
}

@inproceedings{mahalanobisGeneralizedDistanceStatistics1936,
  title = {On the Generalized Distance in Statistics},
  author = {Mahalanobis, Prasanta Chandra},
  date = {1936},
  publisher = {{National Institute of Science of India}},
  file = {/Users/jrudoler/Zotero/storage/7DSEPKBV/Vol02_1936_1_Art05-pcm.pdf}
}

@inproceedings{malininPredictiveUncertaintyEstimation2018,
  title = {Predictive Uncertainty Estimation via Prior Networks},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Malinin, Andrey and Gales, Mark},
  date = {2018-12-03},
  series = {{{NIPS}}'18},
  pages = {7047--7058},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/MPXLWP5L/Malinin and Gales - 2018 - Predictive uncertainty estimation via prior networ.pdf}
}

@article{manningHumanLanguageUnderstanding2022,
  title = {Human {{Language Understanding}} \& {{Reasoning}}},
  author = {Manning, Christopher D.},
  date = {2022-05-01},
  journaltitle = {Daedalus},
  shortjournal = {Daedalus},
  volume = {151},
  number = {2},
  pages = {127--138},
  issn = {0011-5266},
  doi = {10.1162/daed_a_01905},
  url = {https://doi.org/10.1162/daed_a_01905},
  urldate = {2023-08-19},
  abstract = {The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/YHN29PG2/Manning - 2022 - Human Language Understanding & Reasoning.pdf}
}

@online{maPrinciplesParsimonySelfConsistency2022,
  title = {On the {{Principles}} of {{Parsimony}} and {{Self-Consistency}} for the {{Emergence}} of {{Intelligence}}},
  author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
  date = {2022-07-27},
  eprint = {2207.04630},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2207.04630},
  url = {http://arxiv.org/abs/2207.04630},
  urldate = {2022-11-07},
  abstract = {Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence: what to learn and how to learn, respectively. We believe the two principles are the cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.},
  pubstate = {preprint},
  keywords = {AI,info-theory,unread},
  file = {/Users/jrudoler/Zotero/storage/UV43AWSY/Ma et al. - 2022 - On the Principles of Parsimony and Self-Consistenc.pdf;/Users/jrudoler/Zotero/storage/3TMS4PSB/2207.html}
}

@article{markiewiczOpenNeuroResourceSharing2021,
  title = {The {{OpenNeuro}} Resource for Sharing of Neuroscience Data},
  author = {Markiewicz, Christopher J. and Gorgolewski, Krzysztof J. and Feingold, Franklin and Blair, Ross and Halchenko, Yaroslav O. and Miller, Eric and Hardcastle, Nell and Wexler, Joe and Esteban, Oscar and Goncavles, Mathias and Jwa, Anita and Poldrack, Russell},
  date = {2021-10-18},
  journaltitle = {eLife},
  shortjournal = {Elife},
  volume = {10},
  eprint = {34658334},
  eprinttype = {pmid},
  pages = {e71774},
  issn = {2050-084X},
  doi = {10.7554/eLife.71774},
  abstract = {The sharing of research data is essential to ensure reproducibility and maximize the impact of public investments in scientific research. Here, we describe OpenNeuro, a BRAIN Initiative data archive that provides the ability to openly share data from a broad range of brain imaging data types following the FAIR principles for data sharing. We highlight the importance of the Brain Imaging Data Structure standard for enabling effective curation, sharing, and reuse of data. The archive presently shares more than 600 datasets including data from more than 20,000 participants, comprising multiple species and measurement modalities and a broad range of phenotypes. The impact of the shared data is evident in a growing number of published reuses, currently totalling more than 150 publications. We conclude by describing plans for future development and integration with other ongoing open science efforts.},
  langid = {english},
  pmcid = {PMC8550750},
  keywords = {eeg,fMRI,neuroimaging,neuroscience,open-science},
  file = {/Users/jrudoler/Zotero/storage/56HGM4MU/Markiewicz et al. - 2021 - The OpenNeuro resource for sharing of neuroscience.pdf}
}

@article{matuschekBalancingTypeError2017,
  title = {Balancing {{Type I}} Error and Power in Linear Mixed Models},
  author = {Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald and Bates, Douglas},
  date = {2017-06-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {94},
  pages = {305--315},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X17300013},
  urldate = {2022-09-14},
  abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. Although LMMs have many advantages over ANOVA, like ANOVAs, setting them up for data analysis also requires some care. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr, Levy, Scheepers \& Tily, 2013), presumably to keep Type I error down to the nominal α in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data.},
  langid = {english},
  keywords = {error,power},
  file = {/Users/jrudoler/Zotero/storage/7SJLA84Q/Matuschek et al. - 2017 - Balancing Type I error and power in linear mixed m.pdf;/Users/jrudoler/Zotero/storage/VMECMD7T/S0749596X17300013.html}
}

@online{mccoyEmbersAutoregressionUnderstanding2023,
  title = {Embers of {{Autoregression}}: {{Understanding Large Language Models Through}} the {{Problem They}} Are {{Trained}} to {{Solve}}},
  shorttitle = {Embers of {{Autoregression}}},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
  date = {2023-09-24},
  eprint = {2309.13638},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.13638},
  url = {http://arxiv.org/abs/2309.13638},
  urldate = {2023-09-27},
  abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51\% when the output is a high-probability word sequence but only 13\% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/TAG2WQTF/McCoy et al. - 2023 - Embers of Autoregression Understanding Large Language Models Through the Problem They are Trained to Solve.pdf}
}

@article{measeEvidenceContraryStatistical,
  title = {Evidence {{Contrary}} to the {{Statistical View}} of {{Boosting}}},
  author = {Mease, David},
  abstract = {The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial flaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these flaws and their practical consequences.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/S3G8XRP8/Mease - Evidence Contrary to the Statistical View of Boost.pdf}
}

@article{meislerDoesDataCleaning2019,
  title = {Does Data Cleaning Improve Brain State Classification?},
  author = {Meisler, Steven L. and Kahana, Michael J. and Ezzyat, Youssef},
  date = {2019-12-01},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {Journal of Neuroscience Methods},
  volume = {328},
  pages = {108421},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2019.108421},
  url = {https://www.sciencedirect.com/science/article/pii/S016502701930278X},
  urldate = {2022-09-05},
  abstract = {Background Neuroscientists routinely seek to identify and remove noisy or artifactual observations from their data. They do so with the belief that removing such data improves power to detect relations between neural activity and behavior, which are often subtle and can be overwhelmed by noise. Whereas standard methods can exclude certain well-defined noise sources (e.g., 50/60\,Hz electrical noise), in many situations there is not a clear difference between noise and signals so it is not obvious how to separate the two. Here we ask whether methods routinely used to “clean” human electrophysiological recordings lead to greater power to detect brain–behavior relations. New method This, to the authors’ knowledge, is the first large-scale simultaneous evaluation of multiple commonly used methods for removing noise from intracranial EEG recordings. Results We find that several commonly used data cleaning methods (automated methods based on statistical signal properties and manual methods based on expert review) do not increase the power to detect univariate and multivariate electrophysiological biomarkers of successful episodic memory encoding, a well-characterized broadband pattern of neural activity observed across the brain. Comparison with existing methods Researchers may be more likely to increase statistical power to detect physiological phenomena of interest by allocating resources away from cleaning noisy data and toward collecting more within-patient observations. Conclusions These findings highlight the challenge of partitioning signal and noise in the analysis of brain-behavior relations, and suggest increasing sample size and numbers of observations, rather than data cleaning, as the best approach to improving statistical power.},
  langid = {english},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/MB9NL4HN/Meisler et al. - 2019 - Does data cleaning improve brain state classificat.pdf;/Users/jrudoler/Zotero/storage/UC5G57R7/S016502701930278X.html}
}

@article{mengStatisticalParadisesParadoxes2018,
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 {{US}} Presidential Election},
  shorttitle = {Statistical Paradises and Paradoxes in Big Data ({{I}})},
  author = {Meng, Xiao-Li},
  date = {2018-06},
  journaltitle = {The Annals of Applied Statistics},
  volume = {12},
  number = {2},
  pages = {685--726},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1161SF},
  url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-2/Statistical-paradises-and-paradoxes-in-big-data-I--Law/10.1214/18-AOAS1161SF.full},
  urldate = {2022-07-17},
  abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size \$n\$, probabilistic or not, the difference between the sample average \$\textbackslash overline\{X\}\_\{n\}\$ and the population average \$\textbackslash overline\{X\}\_\{N\}\$ is the product of three terms: (1) a data quality measure, \$\textbackslash rho\_\{\{R,X\}\}\$, the correlation between \$X\_\{j\}\$ and the response/recording indicator \$R\_\{j\}\$; (2) a data quantity measure, \$\textbackslash sqrt\{(N-n)/n\}\$, where \$N\$ is the population size; and (3) a problem difficulty measure, \$\textbackslash sigma\_\{X\}\$, the standard deviation of \$X\$. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling \$\textbackslash rho\_\{\{R,X\}\}\$ at the level of \$N\^\{-1/2\}\$; (II) When we lose this control, the impact of \$N\$ is no longer canceled by \$\textbackslash rho\_\{\{R,X\}\}\$, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate \$1/\textbackslash sqrt\{n\}\$, increases with \$\textbackslash sqrt\{N\}\$; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size \$f=n/N\$, not the absolute size \$n\$; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a \$\textbackslash rho\_\{\{R,X\}\}\textbackslash approx-0.005\$ for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from \$1\textbackslash\%\$ of the US eligible voters, that is, \$n\textbackslash approx2\textbackslash mbox\{,\}300\textbackslash mbox\{,\}000\$, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size \$n\textbackslash approx400\$, a \$99.98\textbackslash\%\$ reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual \$95\textbackslash\%\$ confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/9RDE9ZHG/Meng - 2018 - Statistical paradises and paradoxes in big data (I.pdf;/Users/jrudoler/Zotero/storage/4E4XCMIQ/18-AOAS1161SF.html}
}

@article{menshTenSimpleRules2017,
  title = {Ten Simple Rules for Structuring Papers},
  author = {Mensh, Brett and Kording, Konrad},
  date = {2017-09-28},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  number = {9},
  pages = {e1005619},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005619},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005619},
  urldate = {2023-03-28},
  langid = {english},
  keywords = {priority},
  file = {/Users/jrudoler/Zotero/storage/54DR9QEH/Mensh and Kording - 2017 - Ten simple rules for structuring papers.pdf}
}

@online{mialonAugmentedLanguageModels2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozière, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  date = {2023-02-15},
  eprint = {2302.07842},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.07842},
  url = {http://arxiv.org/abs/2302.07842},
  urldate = {2023-02-28},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/G9IPXKEP/Mialon et al. - 2023 - Augmented Language Models a Survey.pdf}
}

@online{michaudQuantizationModelNeural2023,
  title = {The {{Quantization Model}} of {{Neural Scaling}}},
  author = {Michaud, Eric J. and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  date = {2023-03-23},
  eprint = {2303.13506},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2303.13506},
  url = {http://arxiv.org/abs/2303.13506},
  urldate = {2023-09-06},
  abstract = {We propose the \$\textbackslash textit\{Quantization Model\}\$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the \$\textbackslash textit\{Quantization Hypothesis\}\$, where learned network capabilities are quantized into discrete chunks (\$\textbackslash textit\{quanta\}\$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/PG3V78QX/Michaud et al. - 2023 - The Quantization Model of Neural Scaling.pdf;/Users/jrudoler/Zotero/storage/VGMEB3ML/2303.html}
}

@article{molinaroPredictionErrorEstimation2005,
  title = {Prediction Error Estimation: A Comparison of Resampling Methods},
  shorttitle = {Prediction Error Estimation},
  author = {Molinaro, Annette M. and Simon, Richard and Pfeiffer, Ruth M.},
  date = {2005-08-01},
  journaltitle = {Bioinformatics (Oxford, England)},
  shortjournal = {Bioinformatics},
  volume = {21},
  number = {15},
  eprint = {15905277},
  eprinttype = {pmid},
  pages = {3301--3307},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bti499},
  abstract = {MOTIVATION: In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection. RESULTS: For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase. SUPPLEMENTARY INFORMATION: A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/7LZRPGDF/Molinaro et al. - 2005 - Prediction error estimation a comparison of resam.pdf}
}

@online{muennighoffScalingDataConstrainedLanguage2023,
  title = {Scaling {{Data-Constrained Language Models}}},
  author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Scao, Teven Le and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  date = {2023-06-15},
  eprint = {2305.16264},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16264},
  url = {http://arxiv.org/abs/2305.16264},
  urldate = {2023-09-06},
  abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/9DC6M6S7/Muennighoff et al. - 2023 - Scaling Data-Constrained Language Models.pdf;/Users/jrudoler/Zotero/storage/67445UG8/2305.html}
}

@online{nakkiranDeepDoubleDescent2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  date = {2019-12-04},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.02292},
  url = {http://arxiv.org/abs/1912.02292},
  urldate = {2023-05-01},
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/ZQYZJMFE/Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf;/Users/jrudoler/Zotero/storage/SCEZYGSG/1912.html}
}

@online{nayebiNeuralFoundationsMental2023,
  title = {Neural {{Foundations}} of {{Mental Simulation}}: {{Future Prediction}} of {{Latent Representations}} on {{Dynamic Scenes}}},
  shorttitle = {Neural {{Foundations}} of {{Mental Simulation}}},
  author = {Nayebi, Aran and Rajalingham, Rishi and Jazayeri, Mehrdad and Yang, Guangyu Robert},
  date = {2023-05-19},
  eprint = {2305.11772},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2305.11772},
  urldate = {2023-05-22},
  abstract = {Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-centric objectives, to models that future predict in the latent space of purely static image-based or dynamic video-based pretrained foundation models. We find strong differentiation across these model classes in their ability to predict neural and behavioral data both within and across diverse environments. In particular, we find that neural responses are currently best predicted by models trained to predict the future state of their environment in the latent space of pretrained foundation models optimized for dynamic scenes in a self-supervised manner. Notably, models that future predict in the latent space of video foundation models that are optimized to support a diverse range of sensorimotor tasks, reasonably match both human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test. Overall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation are thus far most consistent with being optimized to future predict on dynamic, reusable visual representations that are useful for embodied AI more generally.},
  pubstate = {preprint},
  keywords = {AI,unread},
  file = {/Users/jrudoler/Zotero/storage/P9UZP2G6/Nayebi et al. - 2023 - Neural Foundations of Mental Simulation Future Pr.pdf}
}

@article{NeuroconnectionistResearchProgramme,
  title = {The Neuroconnectionist Research Programme | {{Nature Reviews Neuroscience}}},
  url = {https://www.nature.com/articles/s41583-023-00705-w},
  urldate = {2023-06-09},
  abstract = {Artificial neural networks (ANNs) inspired by biology are beginning to be widely used to model behavioural and neural data, an approach we call ‘neuroconnectionism’. ANNs have been not only lauded as the current best models of information processing in the brain but also criticized for failing to account for basic cognitive functions. In this Perspective article, we propose that arguing about the successes and failures of a restricted set of current ANNs is the wrong approach to assess the promise of neuroconnectionism for brain science. Instead, we take inspiration from the philosophy of science, and in particular from Lakatos, who showed that the core of a\&nbsp;scientific research programme is often not directly falsifiable but should be assessed by its capacity to generate novel insights. Following this view, we present neuroconnectionism as a general research programme centred around ANNs as a computational language for expressing falsifiable theories about brain computation. We describe the core of the programme, the underlying computational framework and its tools for testing specific neuroscientific hypotheses and deriving novel understanding. Taking a longitudinal view, we review past and present neuroconnectionist projects and their responses to challenges and argue that the research programme is highly progressive, generating new and otherwise unreachable insights into the workings of the brain. Artificial neural networks are being widely used to model behavioural and neural data. In this Perspective article, Doerig et al. present neuroconnectionism as a Lakatosian research programme using artificial neural networks as a computational language for expressing falsifiable theories and hypotheses about the brain computations underlying cognition.}
}

@article{ngaiApplicationDataMining2011,
  title = {The Application of Data Mining Techniques in Financial Fraud Detection: {{A}} Classification Framework and an Academic Review of Literature},
  shorttitle = {The Application of Data Mining Techniques in Financial Fraud Detection},
  author = {Ngai, E. W. T. and Hu, Yong and Wong, Y. H. and Chen, Yijun and Sun, Xin},
  date = {2011-02-01},
  journaltitle = {Decision Support Systems},
  shortjournal = {Decision Support Systems},
  series = {On Quantitative Methods for Detection of Financial Fraud},
  volume = {50},
  number = {3},
  pages = {559--569},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2010.08.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167923610001302},
  urldate = {2023-04-11},
  abstract = {This paper presents a review of — and classification scheme for — the literature on the application of data mining techniques for the detection of financial fraud. Although financial fraud detection (FFD) is an emerging topic of great importance, a comprehensive literature review of the subject has yet to be carried out. This paper thus represents the first systematic, identifiable and comprehensive academic literature review of the data mining techniques that have been applied to FFD. 49 journal articles on the subject published between 1997 and 2008 was analyzed and classified into four categories of financial fraud (bank fraud, insurance fraud, securities and commodities fraud, and other related financial fraud) and six classes of data mining techniques (classification, regression, clustering, prediction, outlier detection, and visualization). The findings of this review clearly show that data mining techniques have been applied most extensively to the detection of insurance fraud, although corporate fraud and credit card fraud have also attracted a great deal of attention in recent years. In contrast, we find a distinct lack of research on mortgage fraud, money laundering, and securities and commodities fraud. The main data mining techniques used for FFD are logistic models, neural networks, the Bayesian belief network, and decision trees, all of which provide primary solutions to the problems inherent in the detection and classification of fraudulent data. This paper also addresses the gaps between FFD and the needs of the industry to encourage additional research on neglected topics, and concludes with several suggestions for further FFD research.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/5VL4DESR/Ngai et al. - 2011 - The application of data mining techniques in finan.pdf}
}

@article{nicholsNonparametricPermutationTests2002,
  title = {Nonparametric Permutation Tests for Functional Neuroimaging: A Primer with Examples},
  shorttitle = {Nonparametric Permutation Tests for Functional Neuroimaging},
  author = {Nichols, Thomas E. and Holmes, Andrew P.},
  date = {2002-01},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  volume = {15},
  number = {1},
  eprint = {11747097},
  eprinttype = {pmid},
  pages = {1--25},
  issn = {1065-9471},
  doi = {10.1002/hbm.1058},
  abstract = {Requiring only minimal assumptions for validity, nonparametric permutation testing provides a flexible and intuitive methodology for the statistical analysis of data from functional neuroimaging experiments, at some computational expense. Introduced into the functional neuroimaging literature by Holmes et al. ([1996]: J Cereb Blood Flow Metab 16:7-22), the permutation approach readily accounts for the multiple comparisons problem implicit in the standard voxel-by-voxel hypothesis testing framework. When the appropriate assumptions hold, the nonparametric permutation approach gives results similar to those obtained from a comparable Statistical Parametric Mapping approach using a general linear model with multiple comparisons corrections derived from random field theory. For analyses with low degrees of freedom, such as single subject PET/SPECT experiments or multi-subject PET/SPECT or fMRI designs assessed for population effects, the nonparametric approach employing a locally pooled (smoothed) variance estimate can outperform the comparable Statistical Parametric Mapping approach. Thus, these nonparametric techniques can be used to verify the validity of less computationally expensive parametric approaches. Although the theory and relative advantages of permutation approaches have been discussed by various authors, there has been no accessible explication of the method, and no freely distributed software implementing it. Consequently, there have been few practical applications of the technique. This article, and the accompanying MATLAB software, attempts to address these issues. The standard nonparametric randomization and permutation testing ideas are developed at an accessible level, using practical examples from functional neuroimaging, and the extensions for multiple comparisons described. Three worked examples from PET and fMRI are presented, with discussion, and comparisons with standard parametric approaches made where appropriate. Practical considerations are given throughout, and relevant statistical concepts are expounded in appendices.},
  langid = {english},
  pmcid = {PMC6871862},
  file = {/Users/jrudoler/Zotero/storage/KKJIUC9D/Nichols and Holmes - 2002 - Nonparametric permutation tests for functional neu.pdf}
}

@inproceedings{nixEstimatingMeanVariance1994,
  title = {Estimating the Mean and Variance of the Target Probability Distribution},
  booktitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  author = {Nix, D.A. and Weigend, A.S.},
  date = {1994-06},
  volume = {1},
  pages = {55-60 vol.1},
  doi = {10.1109/ICNN.1994.374138},
  url = {https://ieeexplore.ieee.org/document/374138},
  urldate = {2023-11-26},
  abstract = {Introduces a method that estimates the mean and the variance of the probability distribution of the target as a function of the input, given an assumed target error-distribution model. Through the activation of an auxiliary output unit, this method provides a measure of the uncertainty of the usual network output for each input pattern. The authors derive the cost function and weight-update equations for the example of a Gaussian target error distribution, and demonstrate the feasibility of the network on a synthetic problem where the true input-dependent noise level is known.{$<>$}},
  eventtitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  file = {/Users/jrudoler/Zotero/storage/QMW6K6TL/Nix and Weigend - 1994 - Estimating the mean and variance of the target pro.pdf;/Users/jrudoler/Zotero/storage/NZ7LAYH5/374138.html}
}

@inproceedings{ojalaPermutationTestsStudying2009,
  title = {Permutation {{Tests}} for {{Studying Classifier Performance}}},
  booktitle = {2009 {{Ninth IEEE International Conference}} on {{Data Mining}}},
  author = {Ojala, Markus and Garriga, Gemma C.},
  date = {2009-12},
  pages = {908--913},
  publisher = {{IEEE}},
  location = {{Miami Beach, FL, USA}},
  doi = {10.1109/ICDM.2009.108},
  url = {http://ieeexplore.ieee.org/document/5360332/},
  urldate = {2022-12-20},
  abstract = {We explore the framework of permutation-based p-values for assessing the performance of classifiers. In this paper we study two simple permutation tests. The first test assess whether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.},
  eventtitle = {2009 {{Ninth IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-1-4244-5242-2},
  langid = {english},
  keywords = {permutation},
  file = {/Users/jrudoler/Zotero/storage/62JKM7P5/Ojala and Garriga - 2009 - Permutation Tests for Studying Classifier Performa.pdf}
}

@article{olssonEstimatingDiagnosticUncertainty2022,
  title = {Estimating Diagnostic Uncertainty in Artificial Intelligence Assisted Pathology Using Conformal Prediction},
  author = {Olsson, Henrik and Kartasalo, Kimmo and Mulliqi, Nita and Capuccini, Marco and Ruusuvuori, Pekka and Samaratunga, Hemamali and Delahunt, Brett and Lindskog, Cecilia and Janssen, Emiel A. M. and Blilie, Anders and Egevad, Lars and Spjuth, Ola and Eklund, Martin},
  date = {2022-12-15},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {7761},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-34945-8},
  url = {https://www.nature.com/articles/s41467-022-34945-8},
  urldate = {2023-10-29},
  abstract = {Unreliable predictions can occur when an artificial intelligence (AI) system is presented with data it has not been exposed to during training. We demonstrate the use of conformal prediction to detect unreliable predictions, using histopathological diagnosis and grading of prostate biopsies as example. We digitized 7788 prostate biopsies from 1192 men in the STHLM3 diagnostic study, used for training, and 3059 biopsies from 676 men used for testing. With conformal prediction, 1 in 794 (0.1\%) predictions is incorrect for cancer diagnosis (compared to 14 errors [2\%] without conformal prediction) while 175 (22\%) of the predictions are flagged as unreliable when the AI-system is presented with new data from the same lab and scanner that it was trained on. Conformal prediction could with small samples (N\,=\,49 for external scanner, N\,=\,10 for external lab and scanner, and N\,=\,12 for external lab, scanner and pathology assessment) detect systematic differences in external data leading to worse predictive performance. The AI-system with conformal prediction commits 3 (2\%) errors for cancer detection in cases of atypical prostate tissue compared to 44 (25\%) without conformal prediction, while the system flags 143 (80\%) unreliable predictions. We conclude that conformal prediction can increase patient safety of AI-systems.},
  issue = {1},
  langid = {english},
  keywords = {conformal,unread},
  file = {/Users/jrudoler/Zotero/storage/82HARP2L/Olsson et al. - 2022 - Estimating diagnostic uncertainty in artificial in.pdf}
}

@online{ongieDeepLearningTechniques2020,
  title = {Deep {{Learning Techniques}} for {{Inverse Problems}} in {{Imaging}}},
  author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
  date = {2020-05-12},
  eprint = {2005.06001},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  doi = {10.48550/arXiv.2005.06001},
  url = {http://arxiv.org/abs/2005.06001},
  urldate = {2022-11-10},
  abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the trade-offs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/5K535C72/Ongie et al. - 2020 - Deep Learning Techniques for Inverse Problems in I.pdf;/Users/jrudoler/Zotero/storage/IN5NNABH/2005.html}
}

@online{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  date = {2023-03-27},
  eprint = {2303.08774},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08774},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2023-09-20},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/CEQAHR4F/OpenAI - 2023 - GPT-4 Technical Report.pdf;/Users/jrudoler/Zotero/storage/7KZAIDFW/2303.html}
}

@inproceedings{orhanSelfsupervisedLearningEyes2020,
  title = {Self-Supervised Learning through the Eyes of a Child},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Orhan, Emin and Gupta, Vaibhav and Lake, Brenden M},
  date = {2020},
  volume = {33},
  pages = {9960--9971},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/7183145a2a3e0ce2b68cd3735186b1d5-Abstract.html},
  urldate = {2022-06-30},
  abstract = {Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly defined domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/QT4KGAUD/Orhan et al. - 2020 - Self-supervised learning through the eyes of a chi.pdf}
}

@online{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-09-06},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  pubstate = {preprint},
  keywords = {LLM,RLHF,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/2HVI9X4C/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/Users/jrudoler/Zotero/storage/JMW9N8UJ/2203.html}
}

@article{owenGaussianProcessModel2020,
  title = {A {{Gaussian Process Model}} of {{Human Electrocorticographic Data}}},
  author = {Owen, Lucy L W and Muntianu, Tudor A and Heusser, Andrew C and Daly, Patrick M and Scangos, Katherine W and Manning, Jeremy R},
  date = {2020-09-03},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cerebral Cortex},
  volume = {30},
  number = {10},
  pages = {5333--5345},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa115},
  url = {https://doi.org/10.1093/cercor/bhaa115},
  urldate = {2022-06-07},
  abstract = {We present a model-based method for inferring full-brain neural activity at millimeter-scale spatial resolutions and millisecond-scale temporal resolutions using standard human intracranial recordings. Our approach makes the simplifying assumptions that different people’s brains exhibit similar correlational structure, and that activity and correlation patterns vary smoothly over space. One can then ask, for an arbitrary individual’s brain: given recordings from a limited set of locations in that individual’s brain, along with the observed spatial correlations learned from other people’s recordings, how much can be inferred about ongoing activity at other locations throughout that individual’s brain? We show that our approach generalizes across people and tasks, thereby providing a person- and task-general means of inferring high spatiotemporal resolution full-brain neural dynamics from standard low-density intracranial recordings.},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/MV9XK9DA/Owen et al. - 2020 - A Gaussian Process Model of Human Electrocorticogr.pdf}
}

@online{palGiraffeAdventuresExpanding2023,
  title = {Giraffe: {{Adventures}} in {{Expanding Context Lengths}} in {{LLMs}}},
  shorttitle = {Giraffe},
  author = {Pal, Arka and Karkhanis, Deep and Roberts, Manley and Dooley, Samuel and Sundararajan, Arvind and Naidu, Siddartha},
  date = {2023-08-21},
  eprint = {2308.10882},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.10882},
  url = {http://arxiv.org/abs/2308.10882},
  urldate = {2023-08-30},
  abstract = {Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding. We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/Z8T5WB8G/Pal et al. - 2023 - Giraffe Adventures in Expanding Context Lengths i.pdf;/Users/jrudoler/Zotero/storage/RYNC8CKB/2308.html}
}

@article{panDomainAdaptationTransfer2011,
  title = {Domain {{Adaptation}} via {{Transfer Component Analysis}}},
  author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
  date = {2011-02},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {22},
  number = {2},
  pages = {199--210},
  issn = {1941-0093},
  doi = {10.1109/TNN.2010.2091281},
  abstract = {Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {kernel,manifolds,optimization,transfer-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/IZCPXX74/figures.html}
}

@incollection{paninskiStatisticalModelsNeural2007,
  title = {Statistical Models for Neural Encoding, Decoding, and Optimal Stimulus Design},
  booktitle = {Progress in {{Brain Research}}},
  author = {Paninski, Liam and Pillow, Jonathan and Lewi, Jeremy},
  editor = {Cisek, Paul and Drew, Trevor and Kalaska, John F.},
  date = {2007-01-01},
  series = {Computational {{Neuroscience}}: {{Theoretical Insights}} into {{Brain Function}}},
  volume = {165},
  pages = {493--507},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(06)65031-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0079612306650310},
  urldate = {2023-02-07},
  abstract = {There are two basic problems in the statistical analysis of neural data. The “encoding” problem concerns how information is encoded in neural spike trains: can we predict the spike trains of a neuron (or population of neurons), given an arbitrary stimulus or observed motor response? Conversely, the “decoding” problem concerns how much information is in a spike train, in particular, how well can we estimate the stimulus that gave rise to the spike train? This chapter describes statistical model-based techniques that in some cases provide a unified solution to these two coding problems. These models can capture stimulus dependencies as well as spike history and interneuronal interaction effects in population spike trains, and are intimately related to biophysically based models of integrate-and-fire type. We describe flexible, powerful likelihood-based methods for fitting these encoding models and then for using the models to perform optimal decoding. Each of these (apparently quite difficult) tasks turn out to be highly computationally tractable, due to a key concavity property of the model likelihood. Finally, we return to the encoding problem to describe how to use these models to adaptively optimize the stimuli presented to the cell on a trial-by-trial basis, in order that we may infer the optimal model parameters as efficiently as possible.},
  langid = {english},
  keywords = {neural-coding},
  file = {/Users/jrudoler/Zotero/storage/UI4TRWVZ/Paninski et al. - 2007 - Statistical models for neural encoding, decoding, .pdf;/Users/jrudoler/Zotero/storage/DFACT4AX/S0079612306650310.html}
}

@article{panSurveyTransferLearning2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  date = {2010-10},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {transfer-learning},
  file = {/Users/jrudoler/Zotero/storage/SNNXUVWK/5288526.html}
}

@incollection{papadopoulosInductiveConformalPrediction2008,
  title = {Inductive {{Conformal Prediction}}: {{Theory}} and {{Application}} to {{Neural Networks}}},
  shorttitle = {Inductive {{Conformal Prediction}}},
  booktitle = {Tools in {{Artificial Intelligence}}},
  author = {Papadopoulos, Harris},
  editor = {Fritzsche, Paula},
  date = {2008-08-01},
  publisher = {{InTech}},
  doi = {10.5772/6078},
  url = {http://www.intechopen.com/books/tools_in_artificial_intelligence/inductive_conformal_prediction__theory_and_application_to_neural_networks},
  urldate = {2023-11-21},
  isbn = {978-953-7619-03-9},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/C3T32ZGZ/Papadopoulos - 2008 - Inductive Conformal Prediction Theory and Applica.pdf;/Users/jrudoler/Zotero/storage/IMD3C26W/Papadopoulos - 2008 - Inductive Conformal Prediction Theory and Applica.pdf;/Users/jrudoler/Zotero/storage/YLKPECMX/Papadopoulos - 2008 - Inductive Conformal Prediction Theory and Applica.pdf}
}

@online{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-08-05},
  eprint = {2304.03442},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03442},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2023-09-06},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/PY78KTCM/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf;/Users/jrudoler/Zotero/storage/F852UNNS/2304.html}
}

@article{pernetEEGBIDSExtensionBrain2019,
  title = {{{EEG-BIDS}}, an Extension to the Brain Imaging Data Structure for Electroencephalography},
  author = {Pernet, Cyril R. and Appelhoff, Stefan and Gorgolewski, Krzysztof J. and Flandin, Guillaume and Phillips, Christophe and Delorme, Arnaud and Oostenveld, Robert},
  date = {2019-06-25},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {6},
  number = {1},
  pages = {103},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0104-8},
  url = {https://www.nature.com/articles/s41597-019-0104-8},
  urldate = {2022-07-11},
  abstract = {The Brain Imaging Data Structure (BIDS) project is a rapidly evolving effort in the human brain imaging research community to create standards allowing researchers to readily organize and share study data within and between laboratories. Here we present an extension to BIDS for electroencephalography (EEG) data, EEG-BIDS, along with tools and references to a series of public EEG datasets organized using this new standard.},
  issue = {1},
  langid = {english},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/7QDIARGT/Pernet et al. - 2019 - EEG-BIDS, an extension to the brain imaging data s.pdf;/Users/jrudoler/Zotero/storage/XDVY9S33/s41597-019-0104-8.html}
}

@online{plautPrincipalSubspacesPrincipal2018,
  title = {From {{Principal Subspaces}} to {{Principal Components}} with {{Linear Autoencoders}}},
  author = {Plaut, Elad},
  date = {2018-12-28},
  eprint = {1804.10253},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1804.10253},
  url = {http://arxiv.org/abs/1804.10253},
  urldate = {2022-06-22},
  abstract = {The autoencoder is an effective unsupervised learning model which is widely used in deep learning. It is well known that an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function trains weights that span the same subspace as the one spanned by the principal component loading vectors, but that they are not identical to the loading vectors. In this paper, we show how to recover the loading vectors from the autoencoder weights.},
  pubstate = {preprint},
  keywords = {machine-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/M8ITKDFD/Plaut - 2018 - From Principal Subspaces to Principal Components w.pdf;/Users/jrudoler/Zotero/storage/9G4VV2VZ/1804.html}
}

@article{poldrackCostsReproducibility2019,
  title = {The {{Costs}} of {{Reproducibility}}},
  author = {Poldrack, Russell A.},
  date = {2019-01-02},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {101},
  number = {1},
  pages = {11--14},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.11.030},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627318310390},
  urldate = {2023-01-30},
  abstract = {Improving the reproducibility of neuroscience research is of great concern, especially to early-career researchers (ECRs). Here I outline the potential costs for ECRs in adopting practices to improve reproducibility. I highlight the ways in which ECRs can achieve their career goals while doing better science and the need for established researchers to support them in these efforts.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/TLZDE83R/Poldrack - 2019 - The Costs of Reproducibility.pdf;/Users/jrudoler/Zotero/storage/U3RMUL9B/S0896627318310390.html}
}

@article{poldrackMakingBigData2014,
  title = {Making Big Data Open: Data Sharing in Neuroimaging},
  shorttitle = {Making Big Data Open},
  author = {Poldrack, Russell A. and Gorgolewski, Krzysztof J.},
  date = {2014-11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {17},
  number = {11},
  eprint = {25349916},
  eprinttype = {pmid},
  pages = {1510--1517},
  issn = {1546-1726},
  doi = {10.1038/nn.3818},
  abstract = {In the last decade, major advances have been made in the availability of shared neuroimaging data, such that there are more than 8,000 shared MRI (magnetic resonance imaging) data sets available online. Here we outline the state of data sharing for task-based functional MRI (fMRI) data, with a focus on various forms of data and their relative utility for subsequent analyses. We also discuss challenges to the future success of data sharing and highlight the ethical argument that data sharing may be necessary to maximize the contribution of human subjects.},
  langid = {english},
  keywords = {neuroimaging}
}

@online{powerGrokkingGeneralizationOverfitting2022,
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  date = {2022-01-06},
  eprint = {2201.02177},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.02177},
  url = {http://arxiv.org/abs/2201.02177},
  urldate = {2023-03-21},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/S2US5XHK/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf;/Users/jrudoler/Zotero/storage/LP5ILNMS/2201.html}
}

@online{qiFinetuningAlignedLanguage2023,
  title = {Fine-Tuning {{Aligned Language Models Compromises Safety}}, {{Even When Users Do Not Intend To}}!},
  author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  date = {2023-10-05},
  eprint = {2310.03693},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03693},
  url = {http://arxiv.org/abs/2310.03693},
  urldate = {2023-10-25},
  abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/WHSXD7GW/Qi et al. - 2023 - Fine-tuning Aligned Language Models Compromises Sa.pdf;/Users/jrudoler/Zotero/storage/IPKPYFHR/2310.html}
}

@online{qiuShouldWeLearn2023,
  title = {Should {{We Learn Most Likely Functions}} or {{Parameters}}?},
  author = {Qiu, Shikai and Rudner, Tim G. J. and Kapoor, Sanyam and Wilson, Andrew Gordon},
  date = {2023-11-27},
  eprint = {2311.15990},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2311.15990},
  url = {http://arxiv.org/abs/2311.15990},
  urldate = {2023-12-08},
  abstract = {Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/BPYV8S5L/Qiu et al. - 2023 - Should We Learn Most Likely Functions or Parameter.pdf;/Users/jrudoler/Zotero/storage/TAJX568L/2311.html}
}

@online{quachConformalLanguageModeling2023,
  title = {Conformal {{Language Modeling}}},
  author = {Quach, Victor and Fisch, Adam and Schuster, Tal and Yala, Adam and Sohn, Jae Ho and Jaakkola, Tommi S. and Barzilay, Regina},
  date = {2023-06-16},
  eprint = {2306.10193},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.10193},
  url = {http://arxiv.org/abs/2306.10193},
  urldate = {2023-09-06},
  abstract = {We propose a novel approach to conformal prediction for generative language models (LMs). Standard conformal prediction produces prediction sets -- in place of single predictions -- that have rigorous, statistical performance guarantees. LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language. Translating this process to conformal prediction, we calibrate a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until we are confident that the output set is sufficient. Since some samples may be low-quality, we also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we prove that the sampled set returned by our procedure contains at least one acceptable answer with high probability, while still being empirically precise (i.e., small) on average. Furthermore, within this set of candidate responses, we show that we can also accurately identify subsets of individual components -- such as phrases or sentences -- that are each independently correct (e.g., that are not "hallucinations"), again with statistical guarantees. We demonstrate the promise of our approach on multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.},
  pubstate = {preprint},
  keywords = {conformal,LLM,priority,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/CYLLH9MF/Quach et al. - 2023 - Conformal Language Modeling.pdf;/Users/jrudoler/Zotero/storage/6IVC8V7S/2306.html}
}

@article{quilty-dunnBestGameTown2022,
  title = {The {{Best Game}} in {{Town}}: {{The Re-Emergence}} of the {{Language}} of {{Thought Hypothesis Across}} the {{Cognitive Sciences}}},
  shorttitle = {The {{Best Game}} in {{Town}}},
  author = {Quilty-Dunn, Jake and Porot, Nicolas and Mandelbaum, Eric},
  date = {2022-12-06},
  journaltitle = {Behavioral and Brain Sciences},
  pages = {1--55},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X22002849},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/best-game-in-town-the-reemergence-of-the-language-of-thought-hypothesis-across-the-cognitive-sciences/76F46784C6C07FF52FF45B934D6D3542?s=31},
  urldate = {2022-12-19},
  abstract = {Mental representations remain the central posits of psychology after many decades of scrutiny. However, there is no consensus about the representational format(s) of biological cognition. This paper provides a survey of evidence from computational cognitive psychology, perceptual psychology, developmental psychology, comparative psychology, and social psychology, and concludes that one type of format that routinely crops up is the language of thought (LoT). We outline six core properties of LoTs: (i) discrete constituents; (ii) role-filler independence; (iii) predicate-argument structure; (iv) logical operators; (v) inferential promiscuity; and (vi) abstract content. These properties cluster together throughout cognitive science. Bayesian computational modeling, compositional features of object perception, complex infant and animal reasoning, and automatic, intuitive cognition in adults all implicate LoT-like structures. Instead of regarding LoT as a relic of the previous century, researchers in cognitive science and philosophy of mind must take seriously the explanatory breadth of LoT-based architectures. We grant that the mind may harbor many formats and architectures, including iconic and associative structures as well as deep-neural-network-like architectures. However, as computational/representational approaches to the mind continue to advance, classical compositional symbolic structures—i.e., LoTs—only prove more flexible and well-supported over time.},
  langid = {english},
  keywords = {deep-learning},
  file = {/Users/jrudoler/Zotero/storage/JDERI5R5/Quilty-Dunn et al. - 2022 - The Best Game in Town The Re-Emergence of the Lan.pdf}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/ADCUSW5K/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@inproceedings{rahimiRandomFeaturesLargeScale2007,
  title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  date = {2007},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
  urldate = {2022-06-10},
  abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
  keywords = {kernel},
  file = {/Users/jrudoler/Zotero/storage/N9PEBBHB/Rahimi and Recht - 2007 - Random Features for Large-Scale Kernel Machines.pdf}
}

@article{rahimiWeightedSumsRandom,
  title = {Weighted {{Sums}} of {{Random Kitchen Sinks}}: {{Replacing}} Minimization with Randomization in Learning},
  author = {Rahimi, Ali and Recht, Benjamin},
  pages = {8},
  abstract = {Randomized neural networks are immortalized in this well-known AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.},
  langid = {english},
  keywords = {kernel},
  file = {/Users/jrudoler/Zotero/storage/BLB2HG9Z/Rahimi and Recht - Weighted Sums of Random Kitchen Sinks Replacing m.pdf}
}

@online{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.12092},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2022-07-07},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/R7FUPCA4/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;/Users/jrudoler/Zotero/storage/L5QBJGU8/2102.html}
}

@article{ramotClosedloopNeuromodulationStudying2022,
  title = {Closed-Loop Neuromodulation for Studying Spontaneous Activity and Causality},
  author = {Ramot, Michal and Martin, Alex},
  date = {2022-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {4},
  pages = {290--299},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2022.01.008},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661322000237},
  urldate = {2023-01-04},
  abstract = {Having established that spontaneous brain activity follows meaningful coactivation patterns and correlates with behavior, researchers have turned their attention to understanding its function and behavioral significance. We suggest closed-loop neuromodulation as a neural perturbation tool uniquely well suited for this task. Closed-loop neuromodulation has primarily been viewed as an interventionist tool to teach subjects to directly control their own brain activity. We examine an alternative operant conditioning model of closed-loop neuromodulation which, through implicit feedback, can manipulate spontaneous activity at the network level, without violating the spontaneous or endogenous nature of the signal, thereby providing a direct test of network causality.},
  langid = {english},
  keywords = {causality,eeg,unread},
  file = {/Users/jrudoler/Zotero/storage/LHFSFE2V/Ramot and Martin - 2022 - Closed-loop neuromodulation for studying spontaneo.pdf;/Users/jrudoler/Zotero/storage/K8UDFWDK/S1364661322000237.html}
}

@online{razeghiImpactPretrainingTerm2022,
  title = {Impact of {{Pretraining Term Frequencies}} on {{Few-Shot Reasoning}}},
  author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
  date = {2022-05-23},
  eprint = {2202.07206},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.07206},
  url = {http://arxiv.org/abs/2202.07206},
  urldate = {2023-09-27},
  abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70\textbackslash\%\$ (absolute) more accurate on the top 10\textbackslash\% frequent terms in comparison to the bottom 10\textbackslash\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
  pubstate = {preprint},
  keywords = {LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/TXXU58LE/Razeghi et al. - 2022 - Impact of Pretraining Term Frequencies on Few-Shot Reasoning.pdf}
}

@online{rechtImageNetClassifiersGeneralize2019,
  title = {Do {{ImageNet Classifiers Generalize}} to {{ImageNet}}?},
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  date = {2019-06-12},
  eprint = {1902.10811},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1902.10811},
  url = {http://arxiv.org/abs/1902.10811},
  urldate = {2023-12-06},
  abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% - 15\% on CIFAR-10 and 11\% - 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/LANEEHGM/Recht et al. - 2019 - Do ImageNet Classifiers Generalize to ImageNet.pdf;/Users/jrudoler/Zotero/storage/LB3JJPGJ/1902.html}
}

@thesis{renModelfreeMethodsMultiple2021,
  title = {Model-Free Methods for Multiple Testing and Predictive Inference},
  author = {Ren, Zhimei and Candès, Emmanuel J. and Owen, Art B. and Tibshirani, Robert},
  editora = {Stanford University},
  editoratype = {collaborator},
  date = {2021},
  institution = {{Stanford University}},
  location = {{Stanford, California}},
  abstract = {Recent advances in technology have allowed us to collect, store and process an enormous amount of data, bringing unprecedented challenges to interpretable data analysis: first, the structure of data is often complicated, while model assumptions are hard to justify in practice; second, the algorithms used to analyze the data can be extremely complex---think of the convolutional neural nets---making it difficult to develop validity guarantees for the results. Indeed, it has been noticed by researchers that many of the classical statistical methods fail when applied to the modern type of problems---we need a new set of tools to conduct statistical data analysis in the modern era. This dissertation contributes to the toolbox of statistical data analysis in the modern world by presenting several model-free methods for multiple testing and predictive inference. The methods proposed in this dissertation, building upon knockoffs and conformal inference, bypass the modelling of the data structure and the analysis of complex algorithms, and work as wrappers of other (potentially black-box) existing algorithms. Despite the flexibility of these methods, they are guaranteed to achieve statistical validity under the minimal set of assumptions. The validity and efficacy of these methods are evaluated in extensive numerical experiments. Applying these methods to real genetic and clinical data has led to new scientific insights},
  pagetotal = {1},
  keywords = {unread}
}

@article{richardsDeepLearningFramework2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and family=Berker, given=Archy, prefix=de, useprefix=true and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  date = {2019-11},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {22},
  number = {11},
  pages = {1761--1770},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0520-2},
  url = {https://www.nature.com/articles/s41593-019-0520-2},
  urldate = {2022-09-11},
  abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In artificial neural networks, the three components specified by design are the objective functions, the learning rules and the architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
  issue = {11},
  langid = {english},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/MZBPIKN2/Richards et al. - 2019 - A deep learning framework for neuroscience.pdf;/Users/jrudoler/Zotero/storage/UCUGCJ53/s41593-019-0520-2.html}
}

@article{rolnickTacklingClimateChange2022,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  date = {2022-02-08},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  number = {2},
  pages = {42:1--42:96},
  issn = {0360-0300},
  doi = {10.1145/3485128},
  url = {https://doi.org/10.1145/3485128},
  urldate = {2022-09-11},
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
  keywords = {AI,climate-change,machine-learning},
  file = {/Users/jrudoler/Zotero/storage/GSGKWJY4/Rolnick et al. - 2022 - Tackling Climate Change with Machine Learning.pdf}
}

@inproceedings{romanoClassificationValidAdaptive2020,
  title = {Classification with {{Valid}} and {{Adaptive Coverage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Romano, Yaniv and Sesia, Matteo and Candes, Emmanuel},
  date = {2020},
  volume = {33},
  pages = {3581--3591},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/244edd7e85dc81602b7615cd705545f5-Abstract.html},
  urldate = {2023-11-23},
  abstract = {Conformal inference, cross-validation+, and the jackknife+ are hold-out methods that can be combined with virtually any machine learning algorithm to construct prediction sets with guaranteed marginal coverage. In this paper, we develop specialized versions of these techniques for categorical and unordered response labels that, in addition to providing marginal coverage, are also fully adaptive to complex data distributions, in the sense that they perform favorably in terms of approximate conditional coverage compared to alternative methods. The heart of our contribution is a novel conformity score, which we explicitly demonstrate to be powerful and intuitive for classification problems, but whose underlying principle is potentially far more general. Experiments on synthetic and real data demonstrate the practical value of our theoretical guarantees, as well as the statistical advantages of the proposed methods over the existing alternatives.},
  file = {/Users/jrudoler/Zotero/storage/T7H4S6ZW/Romano et al. - 2020 - Classification with Valid and Adaptive Coverage.pdf}
}

@online{romanoConformalizedQuantileRegression2019,
  title = {Conformalized {{Quantile Regression}}},
  author = {Romano, Yaniv and Patterson, Evan and Candès, Emmanuel J.},
  date = {2019-05-08},
  eprint = {1905.03222},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1905.03222},
  url = {http://arxiv.org/abs/1905.03222},
  urldate = {2023-11-03},
  abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/WPPGVSDA/Romano et al. - 2019 - Conformalized Quantile Regression.pdf}
}

@article{rouderIntroductionBayesianHierarchical2005,
  title = {An Introduction to {{Bayesian}} Hierarchical Models with an Application in the Theory of Signal Detection},
  author = {Rouder, Jeffrey N. and Lu, Jun},
  date = {2005-08-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  number = {4},
  pages = {573--604},
  issn = {1531-5320},
  doi = {10.3758/BF03196750},
  url = {https://doi.org/10.3758/BF03196750},
  urldate = {2022-08-16},
  abstract = {Although many nonlinear models of cognition have been proposed in the past 50 years, there has been little consideration of corresponding statistical techniques for their analysis. In analyses with nonlinear models, unmodeled variability from the selection of items or participants may lead to asymptotically biased estimation. This asymptotic bias, in turn, renders inference problematic. We show, for example, that a signal detection analysis of recognition memory data leads to asymptotic underestimation of sensitivity. To eliminate asymptotic bias, we advocate hierarchical models in which participant variability, item variability, and measurement error are modeled simultaneously. By accounting for multiple sources of variability, hierarchical models yield consistent and accurate estimates of participant and item effects in recognition memory. This article is written in tutorial format; we provide an introduction to Bayesian statistics, hierarchical modeling, and Markov chain Monte Carlo computational techniques.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/6CWHYRMH/Rouder and Lu - 2005 - An introduction to Bayesian hierarchical models wi.pdf}
}

@unpublished{rudolerDecodingOptimizingEpisodic2022,
  type = {Poster},
  title = {Decoding and Optimizing Episodic Memory},
  author = {Rudoler, Joseph H.},
  editora = {Bruska, J. P. and Dougherty, M. R. and Chang, W. and Katerman, B. S. and Halpern, D. J. and Diamond, N. B. and Kahana, Michael J.},
  editoratype = {collaborator},
  date = {2022-07-24},
  eventtitle = {{{MathPsych}}},
  venue = {{Toronto, Canada}}
}

@unpublished{rudolerDecodingOptimizingEpisodic2022a,
  type = {Poster},
  title = {Decoding and Optimizing Episodic Memory},
  author = {Rudoler, Joseph H.},
  editora = {Bruska, J. P. and Dougherty, M. R. and Chang, W. and Katerman, B. S. and Halpern, D. J. and Diamond, N. B. and Kahana, Michael J.},
  editoratype = {collaborator},
  date = {2022-04-24},
  eventtitle = {Cognitive {{Neuroscience Society}}},
  venue = {{San Francisco, California, USA}}
}

@article{rudolerHippocampalThetaEpisodic2023,
  title = {Hippocampal {{Theta}} and {{Episodic Memory}}},
  author = {Rudoler, Joseph H. and Herweg, Nora A. and Kahana, Michael J.},
  date = {2023-01-25},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {43},
  number = {4},
  eprint = {36639900},
  eprinttype = {pmid},
  pages = {613--620},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1045-22.2022},
  url = {https://www-jneurosci-org.proxy.library.upenn.edu/content/43/4/613},
  urldate = {2023-02-19},
  abstract = {Computational models of rodent physiology implicate hippocampal theta as a key modulator of learning and memory (Buzsáki and Moser, 2013; Lisman and Jensen, 2013), yet human hippocampal recordings have shown divergent theta correlates of memory formation. Herweg et al. (2020) suggest that decreases in memory-related broadband power mask narrowband theta increases. Their survey also notes that the theta oscillations appear most prominently in contrasts that isolate memory retrieval processes and when aggregating signals across large brain regions. We evaluate these hypotheses by analyzing human hippocampal recordings captured as 162 neurosurgical patients (n = 86 female) performed a free recall task. Using the Irregular-Resampling Auto-Spectral Analysis (IRASA) to separate broad and narrowband components of the field potential, we show that (1) broadband and narrowband components of theta exhibit opposite effects, with broadband signals decreasing and narrowband theta increasing during successful encoding; (2) whereas low-frequency theta oscillations increase before successful recall, higher-frequency theta and alpha oscillations decrease, masking the positive effect of theta when aggregating across the full band; and (3) the effects of theta on memory encoding and retrieval do not differ between reference schemes that accentuate local signals (bipolar) and those that aggregate signals globally (whole-brain average). In line with computational models that ascribe a fundamental role for hippocampal theta in memory, our large-scale study of human hippocampal recordings shows that 3–4 Hz theta oscillations reliably increase during successful memory encoding and before spontaneous recall of previously studied items. SIGNIFICANCE STATEMENT Analyzing recordings from 162 participants, we resolve a long-standing question regarding the role of hippocampal theta oscillations in the formation and retrieval of episodic memories. We show that broadband spectral changes confound estimates of narrowband theta activity, thereby accounting for inconsistent results in the literature. After accounting for broadband effects, we find that increased theta activity marks successful encoding and retrieval of episodic memories, supporting rodent models that ascribe a key role for hippocampal theta in memory function.},
  langid = {english},
  keywords = {memory},
  file = {/Users/jrudoler/Zotero/storage/FTQ8I939/Rudoler et al. - 2023 - Hippocampal Theta and Episodic Memory.pdf}
}

@unpublished{rudolerOcillatoryFractalBiomarkers2022,
  type = {Poster},
  title = {Ocillatory and Fractal Biomarkers of Human Memory},
  author = {Rudoler, Joseph H.},
  date = {2022-03-17},
  eventtitle = {Computational and {{Systems Neuroscience}} ({{COSYNE}})},
  venue = {{Lisbon, Portugal}}
}

@online{rudolerOptimizingLearningRealtime2023,
  title = {Optimizing Learning via Real-Time Neural Decoding},
  author = {Rudoler, Joseph H. and Bruska, James P. and Chang, Woohyeuk and Dougherty, Matthew R. and Katerman, Brandon S. and Halpern, David J. and Diamond, Nicholas B. and Kahana, Michael J.},
  date = {2023-08-25},
  eprinttype = {bioRxiv},
  abstract = {Spectral features of human electroencephalographic (EEG) recordings predict variability in both memory encoding and retrieval.   Here we develop and test a non-invasive closed-loop (NICL) system for optimizing human learning in real time by capitalizing on these fluctuating neural features. Participants play a virtual navigation and memory game; recording multi-session data across days allowed us to build participant-specific classification models of recall success. In subsequent closed-loop sessions, our platform manipulated the timing of memory encoding, selectively presenting items during periods of predicted good or poor memory function based on EEG features decoded in real time. While overall this manipulation of stimulus timing did not have reliable effects on recall performance, we expected a priori that our ability to predict memory performance from EEG would dictate the success of the procedure. We confirm this by showing that the induced recall differences between presentation conditions were marginally correlated with a participant's out-of-sample classification performance.  These findings present a proof-of-concept for using non-invasive closed-loop technology to optimize human learning and memory and provide evidence that generalizing prediction to new recording sessions is a key hurdle to the success of these systems.},
  pubstate = {preprint}
}

@dataset{rudolerSpatialMemoryNoninvasive2023,
  title = {"{{Spatial}} Memory and Non-Invasive Closed-Loop Stimulus Timing"},
  author = {Rudoler, Joseph H. and Dougherty, Matthew R. and Katerman, Brandon S. and Bruska, James P. and Chang, Woohyeuk and Halpern, David J. and Diamond, Nicholas B. and Kahana, Michael J.},
  date = {2023},
  publisher = {{OpenNeuro}},
  doi = {doi:10.18112/openneuro.ds004706.v1.0.0}
}

@online{santurkarWhoseOpinionsLanguage2023,
  title = {Whose {{Opinions Do Language Models Reflect}}?},
  author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  date = {2023-03-30},
  eprint = {2303.17548},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.17548},
  url = {http://arxiv.org/abs/2303.17548},
  urldate = {2023-09-06},
  abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/YECJ6EDK/Santurkar et al. - 2023 - Whose Opinions Do Language Models Reflect.pdf;/Users/jrudoler/Zotero/storage/II6PMLRG/2303.html}
}

@online{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  date = {2023-04-28},
  eprint = {2304.15004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.15004},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-05-01},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  pubstate = {preprint},
  keywords = {AI,LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/MKT58HXP/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a Mirage.pdf}
}

@online{schaefferDoubleDescentDemystified2023,
  title = {Double {{Descent Demystified}}: {{Identifying}}, {{Interpreting}} \& {{Ablating}} the {{Sources}} of a {{Deep Learning Puzzle}}},
  shorttitle = {Double {{Descent Demystified}}},
  author = {Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila Rani and Koyejo, Oluwasanmi},
  date = {2023-03-24},
  eprint = {2303.14151},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2303.14151},
  url = {http://arxiv.org/abs/2303.14151},
  urldate = {2023-07-19},
  abstract = {Double descent is a surprising phenomenon in machine learning, in which as the number of model parameters grows relative to the number of data, test error drops as models grow ever larger into the highly overparameterized (data undersampled) regime. This drop in test error flies against classical learning theory on overfitting and has arguably underpinned the success of large models in machine learning. This non-monotonic behavior of test loss depends on the number of data, the dimensionality of the data and the number of model parameters. Here, we briefly describe double descent, then provide an explanation of why double descent occurs in an informal and approachable manner, requiring only familiarity with linear algebra and introductory probability. We provide visual intuition using polynomial regression, then mathematically analyze double descent with ordinary linear regression and identify three interpretable factors that, when simultaneously all present, together create double descent. We demonstrate that double descent occurs on real data when using ordinary linear regression, then demonstrate that double descent does not occur when any of the three factors are ablated. We use this understanding to shed light on recent observations in nonlinear models concerning superposition and double descent. Code is publicly available.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/AFY9FMRT/Schaeffer et al. - 2023 - Double Descent Demystified Identifying, Interpret.pdf;/Users/jrudoler/Zotero/storage/3QWPI3W8/2303.html}
}

@article{schneiderLearnableLatentEmbeddings2023,
  title = {Learnable Latent Embeddings for Joint Behavioural and Neural Analysis},
  author = {Schneider, Steffen and Lee, Jin Hwa and Mathis, Mackenzie Weygandt},
  date = {2023-05},
  journaltitle = {Nature},
  volume = {617},
  number = {7960},
  pages = {360--368},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06031-6},
  url = {https://www.nature.com/articles/s41586-023-06031-6},
  urldate = {2023-11-02},
  abstract = {Mapping behavioural actions to neural activity is a fundamental goal of neuroscience. As our ability to record large neural and behavioural data increases, there is growing interest in modelling neural dynamics during adaptive behaviours to probe neural representations1–3. In particular, although neural latent embeddings can reveal underlying correlates of behaviour, we lack nonlinear techniques that can explicitly and flexibly leverage joint behaviour and neural data to uncover neural dynamics3–5. Here, we fill this gap with a new encoding method, CEBRA, that jointly uses behavioural and neural data in a (supervised) hypothesis- or (self-supervised) discovery-driven manner to produce both consistent and high-performance latent spaces. We show that consistency can be used as a metric for uncovering meaningful differences, and the inferred latents can be used for decoding. We validate its accuracy and demonstrate our tool’s utility for both calcium and electrophysiology datasets, across sensory and motor tasks and in simple or complex behaviours across species. It allows leverage of single- and multi-session datasets for hypothesis testing or can be used label free. Lastly, we show that CEBRA can be used for the mapping of space, uncovering complex kinematic features, for the production of consistent latent spaces across two-photon and Neuropixels data, and can provide rapid, high-accuracy decoding of natural videos from visual cortex.},
  issue = {7960},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/LLJIWZ78/Schneider et al. - 2023 - Learnable latent embeddings for joint behavioural .pdf}
}

@inproceedings{seedatImprovingAdaptiveConformal2023,
  title = {Improving {{Adaptive Conformal Prediction Using Self-Supervised Learning}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Seedat, Nabeel and Jeffares, Alan and Imrie, Fergus and family=Schaar, given=Mihaela, prefix=van der, useprefix=false},
  date = {2023-04-11},
  pages = {10160--10177},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/seedat23a.html},
  urldate = {2023-12-13},
  abstract = {Conformal prediction is a powerful distribution-free tool for uncertainty quantification, establishing valid prediction intervals with finite-sample guarantees. To produce valid intervals which are also adaptive to the difficulty of each instance, a common approach is to compute normalized nonconformity scores on a separate calibration set. Self-supervised learning has been effectively utilized in many domains to learn general representations for downstream predictors. However, the use of self-supervision beyond model pretraining and representation learning has been largely unexplored. In this work, we investigate how self-supervised pretext tasks can improve the quality of the conformal regressors, specifically by improving the adaptability of conformal intervals. We train an auxiliary model with a self-supervised pretext task on top of an existing predictive model and use the self-supervised error as an additional feature to estimate nonconformity scores. We empirically demonstrate the benefit of the additional information using both synthetic and real data on the efficiency (width), deficit, and excess of conformal prediction intervals.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/V4CHU7QV/Seedat et al. - 2023 - Improving Adaptive Conformal Prediction Using Self.pdf}
}

@article{sextonReassessingHierarchicalCorrespondences2022,
  title = {Reassessing Hierarchical Correspondences between Brain and Deep Networks through Direct Interface},
  author = {Sexton, Nicholas J. and Love, Bradley C.},
  date = {2022-07-13},
  journaltitle = {Science Advances},
  volume = {8},
  number = {28},
  pages = {eabm2219},
  doi = {10.1126/sciadv.abm2219},
  url = {https://www.science.org/doi/10.1126/sciadv.abm2219},
  urldate = {2022-07-25},
  file = {/Users/jrudoler/Zotero/storage/5NSEX3BE/Sexton and Love - 2022 - Reassessing hierarchical correspondences between brain and deep networks through direct interface.pdf}
}

@online{shenHuggingGPTSolvingAI2023,
  title = {{{HuggingGPT}}: {{Solving AI Tasks}} with {{ChatGPT}} and Its {{Friends}} in {{Hugging Face}}},
  shorttitle = {{{HuggingGPT}}},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  date = {2023-05-25},
  eprint = {2303.17580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.17580},
  url = {http://arxiv.org/abs/2303.17580},
  urldate = {2023-09-06},
  abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/NEUY4PHD/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf;/Users/jrudoler/Zotero/storage/PWJIK5NC/2303.html}
}

@online{simonEigenlearningFrameworkConservation2022,
  title = {The {{Eigenlearning Framework}}: {{A Conservation Law Perspective}} on {{Kernel Regression}} and {{Wide Neural Networks}}},
  shorttitle = {The {{Eigenlearning Framework}}},
  author = {Simon, James B. and Dickens, Madeline and Karkada, Dhruva and DeWeese, Michael R.},
  date = {2022-10-12},
  eprint = {2110.03922},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2110.03922},
  url = {http://arxiv.org/abs/2110.03922},
  urldate = {2023-02-21},
  abstract = {We derive a simple unified framework giving closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the "deep bootstrap" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.},
  pubstate = {preprint},
  keywords = {machine-learning},
  file = {/Users/jrudoler/Zotero/storage/TDJJS525/Simon et al. - 2022 - The Eigenlearning Framework A Conservation Law Pe.pdf;/Users/jrudoler/Zotero/storage/HQXRR4BD/2110.html}
}

@online{singhalLargeLanguageModels2022,
  title = {Large {{Language Models Encode Clinical Knowledge}}},
  author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Scharli, Nathaneal and Chowdhery, Aakanksha and Mansfield, Philip and family=Arcas, given=Blaise Aguera, prefix=y, useprefix=false and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  date = {2022-12-26},
  eprint = {2212.13138},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.13138},
  url = {http://arxiv.org/abs/2212.13138},
  urldate = {2023-09-06},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6\% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17\%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/WM472DCL/Singhal et al. - 2022 - Large Language Models Encode Clinical Knowledge.pdf;/Users/jrudoler/Zotero/storage/J39LF2EZ/2212.html}
}

@online{songRewardCollapseAligning2023,
  title = {Reward {{Collapse}} in {{Aligning Large Language Models}}},
  author = {Song, Ziang and Cai, Tianle and Lee, Jason D. and Su, Weijie J.},
  date = {2023-05-27},
  eprint = {2305.17608},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2305.17608},
  url = {http://arxiv.org/abs/2305.17608},
  urldate = {2023-09-06},
  abstract = {The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. In this paper, we document the phenomenon of \textbackslash textit\{reward collapse\}, an empirical observation where the prevailing ranking-based approach results in an \textbackslash textit\{identical\} reward distribution \textbackslash textit\{regardless\} of the prompts during the terminal phase of training. This outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of New Zealand'' should generate either high or low rewards. Our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. This insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. To overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. Our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/HXRKZS9Y/Song et al. - 2023 - Reward Collapse in Aligning Large Language Models.pdf;/Users/jrudoler/Zotero/storage/M2ZZPU6X/2305.html}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2023-04-27},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/Users/jrudoler/Zotero/storage/Y64DWKUF/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@inproceedings{stantonBayesianOptimizationConformal2023,
  title = {Bayesian {{Optimization}} with {{Conformal Prediction Sets}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Stanton, Samuel and Maddox, Wesley and Wilson, Andrew Gordon},
  date = {2023-04-11},
  pages = {959--986},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/stanton23a.html},
  urldate = {2023-11-21},
  abstract = {Bayesian optimization is a coherent, ubiquitous approach to decision-making under uncertainty, with applications including multi-arm bandits, active learning, and black-box optimization. Bayesian optimization selects decisions (i.e. objective function queries) with maximal expected utility with respect to the posterior distribution of a Bayesian model, which quantifies reducible, epistemic uncertainty about query outcomes. In practice, subjectively implausible outcomes can occur regularly for two reasons: 1) model misspecification and 2) covariate shift. Conformal prediction is an uncertainty quantification method with coverage guarantees even for misspecified models and a simple mechanism to correct for covariate shift. We propose conformal Bayesian optimization, which directs queries towards regions of search space where the model predictions have guaranteed validity, and investigate its behavior on a suite of black-box optimization tasks and tabular ranking tasks. In many cases we find that query coverage can be significantly improved without harming sample-efficiency.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/GDCCLLBC/Stanton et al. - 2023 - Bayesian Optimization with Conformal Prediction Se.pdf}
}

@online{steinbergerConditionalPredictiveInference2022,
  title = {Conditional Predictive Inference for Stable Algorithms},
  author = {Steinberger, Lukas and Leeb, Hannes},
  date = {2022-05-12},
  eprint = {1809.01412},
  eprinttype = {arxiv},
  eprintclass = {math, stat},
  doi = {10.48550/arXiv.1809.01412},
  url = {http://arxiv.org/abs/1809.01412},
  urldate = {2023-12-12},
  abstract = {We investigate generically applicable and intuitively appealing prediction intervals based on \$k\$-fold cross validation. We focus on the conditional coverage probability of the proposed intervals, given the observations in the training sample (hence, training conditional validity), and show that it is close to the nominal level, in an appropriate sense, provided that the underlying algorithm used for computing point predictions is sufficiently stable when feature-response pairs are omitted. Our results are based on a finite sample analysis of the empirical distribution function of \$k\$-fold cross validation residuals and hold in non-parametric settings with only minimal assumptions on the error distribution. To illustrate our results, we also apply them to high-dimensional linear predictors, where we obtain uniform asymptotic training conditional validity as both sample size and dimension tend to infinity at the same rate and consistent parameter estimation typically fails. These results show that despite the serious problems of resampling procedures for inference on the unknown parameters (cf. Bickel and Freedman, 1983; El Karoui and Purdom, 2018; Mammen, 1996), cross validation methods can be successfully applied to obtain reliable predictive inference even in high dimensions and conditionally on the training data.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/Y2SJPVA2/Steinberger and Leeb - 2022 - Conditional predictive inference for stable algori.pdf;/Users/jrudoler/Zotero/storage/3RGF9834/1809.html}
}

@online{sunReturnFrustratinglyEasy2015,
  title = {Return of {{Frustratingly Easy Domain Adaptation}}},
  author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  date = {2015-12-09},
  eprint = {1511.05547},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.05547},
  url = {http://arxiv.org/abs/1511.05547},
  urldate = {2022-07-07},
  abstract = {Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.},
  pubstate = {preprint},
  keywords = {AI},
  file = {/Users/jrudoler/Zotero/storage/8CVACXL8/Sun et al. - 2015 - Return of Frustratingly Easy Domain Adaptation.pdf;/Users/jrudoler/Zotero/storage/XKEAUBCX/1511.html}
}

@online{suRoFormerEnhancedTransformer2022,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  date = {2022-08-08},
  eprint = {2104.09864},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.09864},
  url = {http://arxiv.org/abs/2104.09864},
  urldate = {2023-11-02},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \textbackslash url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  pubstate = {preprint},
  keywords = {priority,unread},
  file = {/Users/jrudoler/Zotero/storage/DP7RV7ES/Su et al. - 2022 - RoFormer Enhanced Transformer with Rotary Positio.pdf;/Users/jrudoler/Zotero/storage/E9HVKWFZ/2104.html}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
  urldate = {2023-12-07},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/4A4GEBR8/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@online{suttonBitterLesson2019,
  title = {The Bitter Lesson},
  author = {Sutton, Richard},
  date = {2019-03-13},
  organization = {{Incomplete Ideas}},
  file = {/Users/jrudoler/Zotero/storage/75KKSEFJ/bitter_lesson.pdf}
}

@online{tarzanaghTransformersSupportVector2023,
  title = {Transformers as {{Support Vector Machines}}},
  author = {Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  date = {2023-09-07},
  eprint = {2308.16898},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2308.16898},
  url = {http://arxiv.org/abs/2308.16898},
  urldate = {2023-11-13},
  abstract = {Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens \$X\$ and makes them interact through pairwise similarities computed as softmax\$(XQK\^\textbackslash top X\^\textbackslash top)\$, where \$(K,Q)\$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by \$(K,Q)\$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter \$W=KQ\^\textbackslash top\$. Instead, directly parameterizing by \$W\$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias with nonlinear heads. Our findings are applicable to arbitrary datasets and their validity is verified via experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens.},
  pubstate = {preprint},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/VPP7JR2Q/Tarzanagh et al. - 2023 - Transformers as Support Vector Machines.pdf;/Users/jrudoler/Zotero/storage/FWJAGG57/2308.html}
}

@unpublished{thomasSelfSupervisedLearningBrain2022,
  title = {Self-{{Supervised Learning Of Brain Dynamics From Broad Neuroimaging Data}}},
  author = {Thomas, Armin W. and Ré, Christopher and Poldrack, Russell A.},
  date = {2022-06-22},
  eprint = {2206.11417},
  eprinttype = {arxiv},
  eprintclass = {q-bio},
  url = {http://arxiv.org/abs/2206.11417},
  urldate = {2022-09-17},
  abstract = {Self-supervised learning techniques are celebrating immense success in natural language processing (NLP) by enabling models to learn from broad language data at unprecedented scales. Here, we aim to leverage the success of these techniques for mental state decoding, where researchers aim to identify specific mental states (such as an individual's experience of anger or happiness) from brain activity. To this end, we devise a set of novel self-supervised learning frameworks for neuroimaging data based on prominent learning frameworks in NLP. At their core, these frameworks learn the dynamics of brain activity by modeling sequences of activity akin to how NLP models sequences of text. We evaluate the performance of the proposed frameworks by pre-training models on a broad neuroimaging dataset spanning functional Magnetic Resonance Imaging (fMRI) data from 11,980 experimental runs of 1,726 individuals across 34 datasets and subsequently adapting the pre-trained models to two benchmark mental state decoding datasets. We show that the pre-trained models transfer well, outperforming baseline models when adapted to the data of only a few individuals, while models pre-trained in a learning framework based on causal language modeling clearly outperform the others.},
  file = {/Users/jrudoler/Zotero/storage/LFGK53Z2/Thomas et al. - 2022 - Self-Supervised Learning Of Brain Dynamics From Broad Neuroimaging Data.pdf}
}

@online{tibshiraniConformalPredictionCovariate2020,
  title = {Conformal {{Prediction Under Covariate Shift}}},
  author = {Tibshirani, Ryan J. and Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya},
  date = {2020-07-06},
  eprint = {1904.06019},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1904.06019},
  url = {http://arxiv.org/abs/1904.06019},
  urldate = {2023-11-21},
  abstract = {We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between these two distributions is known---or, in practice, can be estimated accurately with access to a large set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more generally, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.},
  pubstate = {preprint},
  keywords = {priority,unread},
  file = {/Users/jrudoler/Zotero/storage/B9SBRY4C/Tibshirani et al. - 2020 - Conformal Prediction Under Covariate Shift.pdf;/Users/jrudoler/Zotero/storage/ZZGEUYKK/1904.html}
}

@article{tohmeReliableNeuralNetworks2023,
  title = {Reliable Neural Networks for Regression Uncertainty Estimation},
  author = {Tohme, Tony and Vanslette, Kevin and Youcef-Toumi, Kamal},
  date = {2023-01-01},
  journaltitle = {Reliability Engineering \& System Safety},
  shortjournal = {Reliability Engineering \& System Safety},
  volume = {229},
  pages = {108811},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2022.108811},
  url = {https://www.sciencedirect.com/science/article/pii/S0951832022004306},
  urldate = {2023-11-21},
  abstract = {While deep neural networks are highly performant and successful in a wide range of real-world problems, estimating their predictive uncertainty remains a challenging task. To address this challenge, we propose and implement a loss function for regression uncertainty estimation based on the Bayesian Validation Metric (BVM) framework while using ensemble learning. The proposed loss reproduces maximum likelihood estimation in the limiting case. A series of experiments on in-distribution data show that the proposed method is competitive with existing state-of-the-art methods. Experiments on out-of-distribution data show that the proposed method is robust to statistical change and exhibits superior predictive capability.},
  file = {/Users/jrudoler/Zotero/storage/L3LNXFME/Tohme et al. - 2023 - Reliable neural networks for regression uncertaint.pdf;/Users/jrudoler/Zotero/storage/VKI6PYPE/S0951832022004306.html}
}

@article{tomovNeuralArchitectureTheorybased2022,
  title = {The {{Neural Architecture}} of {{Theory-based Reinforcement Learning}}},
  author = {Tomov, Momchil S. and Tsividis, Pedro A. and Pouncy, Thomas and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  date = {2022-06-16},
  pages = {2022.06.14.496001},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.06.14.496001},
  url = {https://www.biorxiv.org/content/10.1101/2022.06.14.496001v1},
  urldate = {2022-09-15},
  abstract = {Humans learn internal models of the environment that support efficient planning and flexible generalization in complex, real-world domains. Yet it remains unclear how such internal models are represented and learned in the brain. We approach this question within the framework of theory-based reinforcement learning, a strong form of model-based reinforcement learning in which the model is an intuitive theory – a rich, abstract, causal model of the environment built on a natural ontology of physical objects, intentional agents, relations, and goals. We used a theory-based reinforcement learning model to analyze brain data from human participants learning to play different Atari-style video games while undergoing functional MRI. Theories inferred by the theory-based model explained the signal in inferior frontal gyrus and other prefrontal areas better than several alternative models. Brain activity increased in response to theory update events in inferior frontal gyrus, occipital cortex, and fusiform gyrus, with separate learning signals for different theory components. This corresponded with a transient strengthening of theory representations in those regions. Finally, the effective connectivity pattern during theory updating suggests that information flows top-down from theory-coding regions in the prefrontal cortex to theory updating regions in occipital and temporal cortex. These results are consistent with a neural architecture in which top-down theory representations originating in prefrontal regions shape sensory predictions in visual areas, where factorized theory prediction errors are computed and in turn trigger bottom-up updates of the theory. This initial sketch provides a foundation for understanding of the neural representations and computations that support efficient theory-based reinforcement learning in complex, naturalistic environments.},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/76PGR8KT/Tomov et al. - 2022 - The Neural Architecture of Theory-based Reinforcem.pdf;/Users/jrudoler/Zotero/storage/4AX82FSJ/2022.06.14.html}
}

@online{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  date = {2023-02-27},
  eprint = {2302.13971},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2023-09-20},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/5M8R5AID/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;/Users/jrudoler/Zotero/storage/SWPKDKKQ/2302.html}
}

@online{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023-07-19},
  eprint = {2307.09288},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.09288},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2023-09-20},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  pubstate = {preprint},
  keywords = {LLM},
  file = {/Users/jrudoler/Zotero/storage/6Q5LKY66/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf;/Users/jrudoler/Zotero/storage/RBMD6UA9/2307.html}
}

@article{turcuSparseRNNsCan2022,
  title = {Sparse {{RNNs}} Can Support High-Capacity Classification},
  author = {Turcu, Denis and Abbott, L. F.},
  date = {2022-12-14},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {18},
  number = {12},
  pages = {e1010759},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010759},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010759},
  urldate = {2023-01-22},
  abstract = {Feedforward network models performing classification tasks rely on highly convergent output units that collect the information passed on by preceding layers. Although convergent output-unit like neurons may exist in some biological neural circuits, notably the cerebellar cortex, neocortical circuits do not exhibit any obvious candidates for this role; instead they are highly recurrent. We investigate whether a sparsely connected recurrent neural network (RNN) can perform classification in a distributed manner without ever bringing all of the relevant information to a single convergence site. Our model is based on a sparse RNN that performs classification dynamically. Specifically, the interconnections of the RNN are trained to resonantly amplify the magnitude of responses to some external inputs but not others. The amplified and non-amplified responses then form the basis for binary classification. Furthermore, the network acts as an evidence accumulator and maintains its decision even after the input is turned off. Despite highly sparse connectivity, learned recurrent connections allow input information to flow to every neuron of the RNN, providing the basis for distributed computation. In this arrangement, the minimum number of synapses per neuron required to reach maximum memory capacity scales only logarithmically with network size. The model is robust to various types of noise, works with different activation and loss functions and with both backpropagation- and Hebbian-based learning rules. The RNN can also be constructed with a split excitation-inhibition architecture with little reduction in performance.},
  langid = {english},
  keywords = {neural-networks},
  file = {/Users/jrudoler/Zotero/storage/QKQFBZAZ/Turcu and Abbott - 2022 - Sparse RNNs can support high-capacity classificati.pdf}
}

@online{turnerIntroductionTransformers2023,
  title = {An {{Introduction}} to {{Transformers}}},
  author = {Turner, Richard E.},
  date = {2023-10-19},
  eprint = {2304.10557},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.10557},
  url = {http://arxiv.org/abs/2304.10557},
  urldate = {2023-11-07},
  abstract = {The transformer is a neural network component that can be used to learn useful representations of sequences or sets of data-points. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture. We will not discuss training as this is rather standard. We assume that the reader is familiar with fundamental topics in machine learning including multi-layer perceptrons, linear transformations, softmax functions and basic probability.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/WP3WXJYN/Turner - 2023 - An Introduction to Transformers.pdf}
}

@online{tuWhatShouldData2023,
  title = {What {{Should Data Science Education Do}} with {{Large Language Models}}?},
  author = {Tu, Xinming and Zou, James and Su, Weijie J. and Zhang, Linjun},
  date = {2023-07-07},
  eprint = {2307.02792},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02792},
  url = {http://arxiv.org/abs/2307.02792},
  urldate = {2023-08-30},
  abstract = {The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and learning tools, contributing to personalized education. This paper discusses the opportunities, resources and open challenges for each of these directions. As with any transformative technology, integrating LLMs into education calls for careful consideration. While LLMs can perform repetitive tasks efficiently, it's crucial to remember that their role is to supplement human intelligence and creativity, not to replace it. Therefore, the new era of data science education should balance the benefits of LLMs while fostering complementary human expertise and innovations. In conclusion, the rise of LLMs heralds a transformative period for data science and its education. This paper seeks to shed light on the emerging trends, potential opportunities, and challenges accompanying this paradigm shift, hoping to spark further discourse and investigation into this exciting, uncharted territory.},
  pubstate = {preprint},
  keywords = {education,LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/G72PUJK9/Tu et al. - 2023 - What Should Data Science Education Do with Large Language Models.pdf}
}

@article{tyulmankovMetalearningSynapticPlasticity2022,
  title = {Meta-Learning Synaptic Plasticity and Memory Addressing for Continual Familiarity Detection},
  author = {Tyulmankov, Danil and Yang, Guangyu Robert and Abbott, L. F.},
  date = {2022-02-02},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {110},
  number = {3},
  pages = {544-557.e8},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2021.11.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0896627321009478},
  urldate = {2023-01-22},
  abstract = {Over the course of a lifetime, we process a continual stream of information. Extracted from this stream, memories must be efficiently encoded and stored in an addressable manner for retrieval. To explore potential mechanisms, we consider a familiarity detection task in which a subject reports whether an image has been previously encountered. We design a feedforward network endowed with synaptic plasticity and an addressing matrix, meta-learned to optimize familiarity detection over long intervals. We find that anti-Hebbian plasticity leads to better performance than Hebbian plasticity and replicates experimental results such as repetition suppression. A combinatorial addressing function emerges, selecting a unique neuron as an index into the synaptic memory matrix for storage or retrieval. Unlike previous models, this network operates continuously and generalizes to intervals it has not been trained on. Our work suggests a biologically plausible mechanism for continual learning and demonstrates an effective application of machine learning for neuroscience discovery.},
  langid = {english},
  keywords = {deep-learning,memory,meta-learning,unread},
  file = {/Users/jrudoler/Zotero/storage/DSM77PVZ/Tyulmankov et al. - 2022 - Meta-learning synaptic plasticity and memory addre.pdf;/Users/jrudoler/Zotero/storage/7JE6YSBZ/S0896627321009478.html}
}

@article{vapnikUniformConvergenceRelative1971,
  title = {On the {{Uniform Convergence}} of {{Relative Frequencies}} of {{Events}} to {{Their Probabilities}}},
  author = {Vapnik, V. N. and family=Chervonenkis, given=A. Ya., given-i=A{{Ya}}},
  date = {1971-01},
  journaltitle = {Theory of Probability \& Its Applications},
  shortjournal = {Theory Probab. Appl.},
  volume = {16},
  number = {2},
  pages = {264--280},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/1116025},
  url = {https://epubs.siam.org/doi/10.1137/1116025},
  urldate = {2023-04-04},
  file = {/Users/jrudoler/Zotero/storage/49UKPGJ6/Vapnik and Chervonenkis - 1971 - On the Uniform Convergence of Relative Frequencies.pdf}
}

@article{varmaBiasErrorEstimation2006,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  author = {Varma, Sudhir and Simon, Richard},
  date = {2006-02-23},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {7},
  number = {1},
  pages = {91},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-7-91},
  url = {https://doi.org/10.1186/1471-2105-7-91},
  urldate = {2022-12-21},
  abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
  file = {/Users/jrudoler/Zotero/storage/8NK5DJWL/Varma and Simon - 2006 - Bias in error estimation when using cross-validati.pdf;/Users/jrudoler/Zotero/storage/ZBDJM2BT/1471-2105-7-91.html}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-11-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/UY6LFKW2/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/jrudoler/Zotero/storage/V8FW4BD6/1706.html}
}

@book{vovkAlgorithmicLearningRandom2022,
  title = {Algorithmic {{Learning}} in a {{Random World}}},
  author = {Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  date = {2022},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-06649-8},
  url = {https://link.springer.com/10.1007/978-3-031-06649-8},
  urldate = {2023-11-30},
  isbn = {978-3-031-06648-1 978-3-031-06649-8},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/Z7SAP7VV/Vovk et al. - 2022 - Algorithmic Learning in a Random World.pdf}
}

@incollection{vovkChapterBasicConformal2014,
  title = {Chapter 2 - {{Beyond}} the {{Basic Conformal Prediction Framework}}},
  booktitle = {Conformal {{Prediction}} for {{Reliable Machine Learning}}},
  author = {Vovk, Vladimir},
  editor = {Balasubramanian, Vineeth N. and Ho, Shen-Shyang and Vovk, Vladimir},
  date = {2014-01-01},
  pages = {21--46},
  publisher = {{Morgan Kaufmann}},
  location = {{Boston}},
  doi = {10.1016/B978-0-12-398537-8.00002-X},
  url = {https://www.sciencedirect.com/science/article/pii/B978012398537800002X},
  urldate = {2023-11-21},
  abstract = {An appealing property of conformal predictors is their automatic validity under the exchangeability assumption: they make an error with probability not exceeding the prespecified significance level. A major focus of this chapter is on conditional versions of the notion of validity. Other extensions that we consider are a computationally efficient version of conformal prediction and probabilistic prediction. We also discuss classical tolerance regions, which can be regarded as a special case of conformal predictors and a generalization of inductive conformal predictors; their importance in the context of this chapter lies in the possibility of extending to them certain properties of conditional validity enjoyed by inductive conformal predictors.},
  isbn = {978-0-12-398537-8},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/CCZSHKED/B978012398537800002X.html}
}

@inproceedings{vovkConditionalValidityInductive2012,
  title = {Conditional {{Validity}} of {{Inductive Conformal Predictors}}},
  booktitle = {Proceedings of the {{Asian Conference}} on {{Machine Learning}}},
  author = {Vovk, Vladimir},
  date = {2012-11-17},
  pages = {475--490},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v25/vovk12.html},
  urldate = {2023-12-12},
  abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given confidence level. Inductive conformal predictors are a computationally efficient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modifications.},
  eventtitle = {Asian {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/jrudoler/Zotero/storage/36A7BFEQ/Vovk - 2012 - Conditional Validity of Inductive Conformal Predic.pdf}
}

@inproceedings{vovkMachineLearningApplicationsAlgorithmic1999,
  title = {Machine-{{Learning Applications}} of {{Algorithmic Randomness}}},
  booktitle = {Proceedings of the {{Sixteenth International Conference}} on {{Machine Learning}}},
  author = {Vovk, Volodya and Gammerman, Alexander and Saunders, Craig},
  date = {1999-06-27},
  series = {{{ICML}} '99},
  pages = {444--453},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  isbn = {978-1-55860-612-8},
  file = {/Users/jrudoler/Zotero/storage/2JECEUA2/Vovk et al. - 1999 - Machine-Learning Applications of Algorithmic Rando.pdf}
}

@online{vyasProvableCopyrightProtection2023,
  title = {On {{Provable Copyright Protection}} for {{Generative Models}}},
  author = {Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
  date = {2023-07-21},
  eprint = {2302.10870},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2302.10870},
  url = {http://arxiv.org/abs/2302.10870},
  urldate = {2023-09-06},
  abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data \$C\$ that was in their training set. We give a formal definition of \$\textbackslash textit\{near access-freeness (NAF)\}\$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to \$C\$, even if \$C\$ is included in its training set. Roughly speaking, a generative model \$p\$ is \$\textbackslash textit\{\$k\$-NAF\}\$ if for every potentially copyrighted data \$C\$, the output of \$p\$ diverges by at most \$k\$-bits from the output of a model \$q\$ that \$\textbackslash textit\{did not access \$C\$ at all\}\$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/CK25XP44/Vyas et al. - 2023 - On Provable Copyright Protection for Generative Mo.pdf;/Users/jrudoler/Zotero/storage/TDGM46SJ/2302.html}
}

@article{wardHonestyBestPolicy,
  title = {Honesty {{Is}} the {{Best Policy}}: {{Defining}} and {{Mitigating AI Deception}}},
  author = {Ward, Francis Rhys and Everitt, Tom and Belardinelli, Francesco and Toni, Francesca},
  abstract = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being judged as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.},
  langid = {english},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/ERHW33WX/Ward et al. - Honesty Is the Best Policy Defining and Mitigatin.pdf}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-09-11},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/N8RNJ3NS/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf;/Users/jrudoler/Zotero/storage/M7HY6G5E/2201.html}
}

@online{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-09-11},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/SF95EYBJ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/Users/jrudoler/Zotero/storage/TL9ABMZR/2206.html}
}

@article{wenSeparatingFractalOscillatory2016,
  title = {Separating {{Fractal}} and {{Oscillatory Components}} in the {{Power Spectrum}} of {{Neurophysiological Signal}}},
  author = {Wen, Haiguang and Liu, Zhongming},
  date = {2016-01-01},
  journaltitle = {Brain Topography},
  shortjournal = {Brain Topogr},
  volume = {29},
  number = {1},
  pages = {13--26},
  issn = {1573-6792},
  doi = {10.1007/s10548-015-0448-0},
  url = {https://doi.org/10.1007/s10548-015-0448-0},
  urldate = {2022-08-23},
  abstract = {Neurophysiological field-potential signals consist of both arrhythmic and rhythmic patterns indicative of the fractal and oscillatory dynamics arising from likely distinct mechanisms. Here, we present a new method, namely the irregular-resampling auto-spectral analysis (IRASA), to separate fractal and oscillatory components in the power spectrum of neurophysiological signal according to their distinct temporal and spectral characteristics. In this method, we irregularly resampled the neural signal by a set of non-integer factors, and statistically summarized the auto-power spectra of the resampled signals to separate the fractal component from the oscillatory component in the frequency domain. We tested this method on simulated data and demonstrated that IRASA could robustly separate the fractal component from the oscillatory component. In addition, applications of IRASA to macaque electrocorticography and human magnetoencephalography data revealed a greater power-law exponent of fractal dynamics during sleep compared to wakefulness. The temporal fluctuation in the broadband power of the fractal component revealed characteristic dynamics within and across the eyes-closed, eyes-open and sleep states. These results demonstrate the efficacy and potential applications of this method in analyzing electrophysiological signatures of large-scale neural circuit activity. We expect that the proposed method or its future variations would potentially allow for more specific characterization of the differential contributions of oscillatory and fractal dynamics to distributed neural processes underlying various brain functions.},
  langid = {english},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/5Z6SXD69/Wen and Liu - 2016 - Separating Fractal and Oscillatory Components in t.pdf}
}

@online{wenTransformersTimeSeries2023,
  title = {Transformers in {{Time Series}}: {{A Survey}}},
  shorttitle = {Transformers in {{Time Series}}},
  author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  date = {2023-02-10},
  eprint = {2202.07125},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  doi = {10.48550/arXiv.2202.07125},
  url = {http://arxiv.org/abs/2202.07125},
  urldate = {2023-05-01},
  abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. A corresponding resource that has been continuously updated can be found in the GitHub repository. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
  pubstate = {preprint},
  keywords = {AI,machine-learning},
  file = {/Users/jrudoler/Zotero/storage/TJSNKEMI/Wen et al. - 2023 - Transformers in Time Series A Survey.pdf;/Users/jrudoler/Zotero/storage/JURNQEBP/2202.html}
}

@online{whitewayPartitioningVariabilityAnimal2021,
  title = {Partitioning Variability in Animal Behavioral Videos Using Semi-Supervised Variational Autoencoders},
  author = {Whiteway, Matthew R. and Biderman, Dan and Friedman, Yoni and Dipoppa, Mario and Buchanan, E. Kelly and Wu, Anqi and Zhou, John and Noel, Jean-Paul and Laboratory, The International Brain and Cunningham, John and Paninski, Liam},
  date = {2021-02-23},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2021.02.22.432309},
  doi = {10.1101/2021.02.22.432309},
  url = {https://www.biorxiv.org/content/10.1101/2021.02.22.432309v1},
  urldate = {2023-01-31},
  abstract = {Recent neuroscience studies in awake and behaving animals demonstrate that a deeper understanding of brain function requires a deeper understanding of behavior. Detailed behavioral measurements are now often collected using video cameras, resulting in an increased need for computer vision algorithms that extract useful information from this video data. In this work we introduce a new semi-supervised framework that combines the output of supervised pose estimation algorithms (e.g. DeepLabCut) with unsupervised dimensionality reduction methods to produce interpretable, low-dimensional representations of behavioral videos that extract more information than pose estimates alone. We demonstrate this method, the Partitioned Subspace Variational Autoencoder (PS-VAE), on head-fixed mouse behavioral videos. In a close up video of a mouse face, where we track pupil location and size, our method extracts unsupervised outputs that correspond to the eyelid and whisker pad positions, with no additional user annotations required. We use this resulting interpretable behavioral representation to construct saccade and whisking detectors, and quantify the accuracy with which these signals can be decoded from neural activity in visual cortex. In a two-camera mouse video we show how our method separates movements of experimental equipment from animal behavior, and extracts unsupervised features like chest position, again with no additional user annotation needed. This allows us to construct paw and body movement detectors, and decode individual features of behavior from widefield calcium imaging data. Our results demonstrate how the interpretable partitioning of behavioral videos provided by the PS-VAE can facilitate downstream behavioral and neural analyses.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/I3A3995N/Whiteway et al. - 2021 - Partitioning variability in animal behavioral vide.pdf}
}

@online{williamsGeneralizedShapeMetrics2022,
  title = {Generalized {{Shape Metrics}} on {{Neural Representations}}},
  author = {Williams, Alex H. and Kunz, Erin and Kornblith, Simon and Linderman, Scott W.},
  date = {2022-01-12},
  eprint = {2110.14739},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2110.14739},
  url = {http://arxiv.org/abs/2110.14739},
  urldate = {2022-11-21},
  abstract = {Understanding the operation of biological and artificial networks remains a difficult and important challenge. To identify general principles, researchers are increasingly interested in surveying large collections of networks that are trained on, or biologically adapted to, similar tasks. A standardized set of analysis tools is now needed to identify how network-level covariates -- such as architecture, anatomical brain region, and model organism -- impact neural representations (hidden layer activations). Here, we provide a rigorous foundation for these analyses by defining a broad family of metric spaces that quantify representational dissimilarity. Using this framework we modify existing representational similarity measures based on canonical correlation analysis to satisfy the triangle inequality, formulate a novel metric that respects the inductive biases in convolutional layers, and identify approximate Euclidean embeddings that enable network representations to be incorporated into essentially any off-the-shelf machine learning method. We demonstrate these methods on large-scale datasets from biology (Allen Institute Brain Observatory) and deep learning (NAS-Bench-101). In doing so, we identify relationships between neural representations that are interpretable in terms of anatomical features and model performance.},
  pubstate = {preprint},
  keywords = {machine-learning,similarity},
  file = {/Users/jrudoler/Zotero/storage/JX6IQDXB/Williams et al. - 2022 - Generalized Shape Metrics on Neural Representation.pdf;/Users/jrudoler/Zotero/storage/5GPHZZ63/2110.html}
}

@inproceedings{wilsonBayesianDeepLearning2020,
  title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  date = {2020-12-06},
  series = {{{NIPS}}'20},
  pages = {4697--4708},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility.},
  isbn = {978-1-71382-954-6},
  file = {/Users/jrudoler/Zotero/storage/L4H2DN4U/Wilson and Izmailov - 2020 - Bayesian deep learning and a probabilistic perspec.pdf}
}

@online{wolfHuggingFaceTransformersStateoftheart2019,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  date = {2019-10-09},
  url = {https://arxiv.org/abs/1910.03771v5},
  urldate = {2023-12-12},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textbackslash textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textbackslash textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \textbackslash url\{https://github.com/huggingface/transformers\}.},
  langid = {english},
  organization = {{arXiv.org}},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/P4KPBDKW/Wolf et al. - 2019 - HuggingFace's Transformers State-of-the-art Natur.pdf}
}

@article{wolpertNoFreeLunch1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  date = {1997-04},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1941-0026},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {info-theory},
  file = {/Users/jrudoler/Zotero/storage/V2YMIIY3/Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf;/Users/jrudoler/Zotero/storage/PXEQYS36/585893.html}
}

@online{wuReasoningRecitingExploring2023,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  date = {2023-08-01},
  eprint = {2307.02477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02477},
  url = {http://arxiv.org/abs/2307.02477},
  urldate = {2023-09-27},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  pubstate = {preprint},
  keywords = {causality,LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/6CW4FFCP/Wu et al. - 2023 - Reasoning or Reciting Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks.pdf}
}

@article{wynerExplainingSuccessAdaBoost,
  title = {Explaining the {{Success}} of {{AdaBoost}} and {{Random Forests}} as {{Interpolating Classiﬁers}}},
  author = {Wyner, Abraham J and Olson, Matthew and Bleich, Justin and Mease, David},
  abstract = {There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting’s interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a selfaveraging, interpolating algorithm which creates what we denote as a “spiked-smooth” classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping.},
  langid = {english},
  keywords = {unread},
  file = {/Users/jrudoler/Zotero/storage/QGU38ZRL/Wyner et al. - Explaining the Success of AdaBoost and Random Fore.pdf}
}

@article{yaminsPerformanceoptimizedHierarchicalModels2014,
  title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
  author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  date = {2014-06-10},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {23},
  pages = {8619--8624},
  doi = {10.1073/pnas.1403112111},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1403112111},
  urldate = {2022-08-16},
  file = {/Users/jrudoler/Zotero/storage/JEQNWKL8/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf}
}

@article{yaminsUsingGoaldrivenDeep2016,
  title = {Using Goal-Driven Deep Learning Models to Understand Sensory Cortex},
  author = {Yamins, Daniel L. K. and DiCarlo, James J.},
  date = {2016-03},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {19},
  number = {3},
  pages = {356--365},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4244},
  url = {https://www.nature.com/articles/nn.4244},
  urldate = {2022-08-16},
  abstract = {Recent computational neuroscience developments have used deep neural networks to model neural responses in higher visual areas. This Perspective describes key algorithmic underpinnings in computer vision and artificial intelligence that have contributed to this progress and outlines how deep networks could drive future improvements in understanding sensory cortical processing.},
  issue = {3},
  langid = {english},
  keywords = {comp-neuro},
  file = {/Users/jrudoler/Zotero/storage/BGM448NI/Yamins and DiCarlo - 2016 - Using goal-driven deep learning models to understa.pdf;/Users/jrudoler/Zotero/storage/8WFNRI6Y/nn.html}
}

@online{yangTensorProgramsTuning2022,
  title = {Tensor {{Programs V}}: {{Tuning Large Neural Networks}} via {{Zero-Shot Hyperparameter Transfer}}},
  shorttitle = {Tensor {{Programs V}}},
  author = {Yang, Greg and Hu, Edward J. and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  date = {2022-03-28},
  eprint = {2203.03466},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2203.03466},
  url = {http://arxiv.org/abs/2203.03466},
  urldate = {2023-09-06},
  abstract = {Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7\% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/DWT9KCMJ/Yang et al. - 2022 - Tensor Programs V Tuning Large Neural Networks vi.pdf;/Users/jrudoler/Zotero/storage/VXWV96ZB/2203.html}
}

@article{yaoWhichReferenceShould2019,
  title = {Which {{Reference Should We Use}} for {{EEG}} and {{ERP}} Practice?},
  author = {Yao, Dezhong and Qin, Yun and Hu, Shiang and Dong, Li and Bringas Vega, Maria L. and Valdés Sosa, Pedro A.},
  date = {2019-07-01},
  journaltitle = {Brain Topography},
  shortjournal = {Brain Topogr},
  volume = {32},
  number = {4},
  pages = {530--549},
  issn = {1573-6792},
  doi = {10.1007/s10548-019-00707-x},
  url = {https://doi.org/10.1007/s10548-019-00707-x},
  urldate = {2022-08-12},
  abstract = {Which reference is appropriate for the scalp ERP and EEG studies? This unsettled problem still inspires unceasing debate. The ideal reference should be the one with zero or constant potential but unfortunately it is well known that no point on the body fulfills this condition. Consequently, more than ten references are used in the present EEG-ERP studies. This diversity seriously undermines the reproducibility and comparability of results across laboratories. A comprehensive review accompanied by a brief communication with rigorous derivations and notable properties (Hu et al. Brain Topogr, 2019. https://doi.org/10.1007/s10548-019-00706-y) is thus necessary to provide application-oriented principled recommendations. In this paper current popular references are classified into two categories: (1) unipolar references that construct a neutral reference, including both online unipolar references and offline re-references. Examples of unipolar references are the reference electrode standardization technique (REST), average reference (AR), and linked-mastoids/ears reference (LM); (2) non-unipolar references that include the bipolar reference and the Laplacian reference. We show that each reference is derived with a different assumption and serves different aims. We also note from (Hu et al. 2019) that there is a general form for the reference problem, the ‘no memory’ property of the unipolar references, and a unified estimator for the potentials at infinity termed as the regularized REST (rREST) which has more advantageous statistical evidence than AR. A thorough discussion of the advantages and limitations of references is provided with recommendations in the hope to clarify the role of each reference in the ERP and EEG practice.},
  langid = {english},
  keywords = {eeg},
  file = {/Users/jrudoler/Zotero/storage/42KZH4YZ/Yao et al. - 2019 - Which Reference Should We Use for EEG and ERP prac.pdf}
}

@online{yinBenchmarkingZeroshotText2019,
  title = {Benchmarking {{Zero-shot Text Classification}}: {{Datasets}}, {{Evaluation}} and {{Entailment Approach}}},
  shorttitle = {Benchmarking {{Zero-shot Text Classification}}},
  author = {Yin, Wenpeng and Hay, Jamaal and Roth, Dan},
  date = {2019-08-31},
  eprint = {1909.00161},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.00161},
  url = {http://arxiv.org/abs/1909.00161},
  urldate = {2023-12-11},
  abstract = {Zero-shot text classification (0Shot-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0Shot-TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0Shot-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0Shot-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0Shot-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0Shot-TC relative to conceptually different and diverse aspects: the ``topic'' aspect includes ``sports'' and ``politics'' as labels; the ``emotion'' aspect includes ``joy'' and ``anger''; the ``situation'' aspect includes ``medical assistance'' and ``water shortage''. ii) We extend the existing evaluation setup (label-partially-unseen) -- given a dataset, train on some labels, test on all labels -- to include a more challenging yet realistic evaluation label-fully-unseen 0Shot-TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0Shot-TC of diverse aspects within a textual entailment formulation and study it this way. Code \& Data: https://github.com/yinwenpeng/BenchmarkingZeroShot},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/BQBGXJHS/Yin et al. - 2019 - Benchmarking Zero-shot Text Classification Datase.pdf;/Users/jrudoler/Zotero/storage/GKW4E3E9/1909.html}
}

@online{yosinskiHowTransferableAre2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date = {2014-11-06},
  eprint = {1411.1792},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1411.1792},
  url = {http://arxiv.org/abs/1411.1792},
  urldate = {2023-05-01},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  pubstate = {preprint},
  file = {/Users/jrudoler/Zotero/storage/IZYTFCKX/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf;/Users/jrudoler/Zotero/storage/UQCQW463/1411.html}
}

@online{zadorNextGenerationArtificialIntelligence2022,
  title = {Toward {{Next-Generation Artificial Intelligence}}: {{Catalyzing}} the {{NeuroAI Revolution}}},
  shorttitle = {Toward {{Next-Generation Artificial Intelligence}}},
  author = {Zador, Anthony and Richards, Blake and Ölveczky, Bence and Escola, Sean and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Koerding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris},
  date = {2022-10-20},
  eprint = {2210.08340},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2210.08340},
  url = {http://arxiv.org/abs/2210.08340},
  urldate = {2022-10-24},
  abstract = {Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI.},
  pubstate = {preprint},
  keywords = {AI},
  file = {/Users/jrudoler/Zotero/storage/PL3R2VHR/Zador et al. - 2022 - Toward Next-Generation Artificial Intelligence Ca.pdf;/Users/jrudoler/Zotero/storage/STGW2CWD/2210.html}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2021-02-22},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782},
  doi = {10.1145/3446776},
  url = {https://doi.org/10.1145/3446776},
  urldate = {2023-01-13},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
  file = {/Users/jrudoler/Zotero/storage/NVDI79QL/Zhang et al. - 2021 - Understanding deep learning (still) requires rethi.pdf}
}

@online{zhongDescribingDifferencesText2022,
  title = {Describing {{Differences}} between {{Text Distributions}} with {{Natural Language}}},
  author = {Zhong, Ruiqi and Snell, Charlie and Klein, Dan and Steinhardt, Jacob},
  date = {2022-05-18},
  eprint = {2201.12323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.12323},
  url = {http://arxiv.org/abs/2201.12323},
  urldate = {2023-09-06},
  abstract = {How do two distributions of texts differ? Humans are slow at answering this, since discovering patterns might require tediously reading through hundreds of samples. We propose to automatically summarize the differences by "learning a natural language hypothesis": given two distributions \$D\_\{0\}\$ and \$D\_\{1\}\$, we search for a description that is more often true for \$D\_\{1\}\$, e.g., "is military-related." To tackle this problem, we fine-tune GPT-3 to propose descriptions with the prompt: "[samples of \$D\_\{0\}\$] + [samples of \$D\_\{1\}\$] + the difference between them is\_\_\_\_\_." We then re-rank the descriptions by checking how often they hold on a larger set of samples with a learned verifier. On a benchmark of 54 real-world binary classification tasks, while GPT-3 Curie (13B) only generates a description similar to human annotation 7\% of the time, the performance reaches 61\% with fine-tuning and re-ranking, and our best system using GPT-3 Davinci (175B) reaches 76\%. We apply our system to describe distribution shifts, debug dataset shortcuts, summarize unknown tasks, and label text clusters, and present analyses based on automatically generated descriptions.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910,unread},
  file = {/Users/jrudoler/Zotero/storage/X2XHPYSP/Zhong et al. - 2022 - Describing Differences between Text Distributions .pdf;/Users/jrudoler/Zotero/storage/6DQDNTA9/2201.html}
}

@online{zhouLIMALessMore2023,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  date = {2023-05-18},
  eprint = {2305.11206},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.11206},
  url = {http://arxiv.org/abs/2305.11206},
  urldate = {2023-09-06},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/AQMIUKPD/Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf;/Users/jrudoler/Zotero/storage/MIEGXSUC/2305.html}
}

@online{ziemsCanLargeLanguage2023,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  date = {2023-04-12},
  eprint = {2305.03514},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.03514},
  url = {http://arxiv.org/abs/2305.03514},
  urldate = {2023-09-06},
  abstract = {Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the hidden meaning behind text). In summary, LLMs can significantly reduce costs and increase efficiency of social science analysis in partnership with humans.},
  pubstate = {preprint},
  keywords = {LLM,STAT9910},
  file = {/Users/jrudoler/Zotero/storage/7NVV8WH5/Ziems et al. - 2023 - Can Large Language Models Transform Computational .pdf;/Users/jrudoler/Zotero/storage/VRB572SK/2305.html}
}

@online{zouUniversalTransferableAdversarial2023,
  title = {Universal and {{Transferable Adversarial Attacks}} on {{Aligned Language Models}}},
  author = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
  date = {2023-07-27},
  eprint = {2307.15043},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.15043},
  url = {http://arxiv.org/abs/2307.15043},
  urldate = {2023-08-21},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  pubstate = {preprint},
  keywords = {alignment,LLM,unread},
  file = {/Users/jrudoler/Zotero/storage/N6SYECQI/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on .pdf;/Users/jrudoler/Zotero/storage/BHSKJD2X/2307.html}
}
